,Unnamed: 0,Unnamed: 0.1,Unnamed: 0.1.1,RecordName,Download Link,SizeGB,Version,Format,License,Classification,UpdateFrequency,LifecycleStage,Description,Note,industry,keywords ,Type,P-ID,category1,category2,category3,category4,source code License,hash,error,uploaded
0,0,0,0,Mapillar Mapillary Vistas Dataset,https://s3-eu-west-1.amazonaws.com/static.mapillary.com/MVD_research_samples.zip,0.015,v1,zip,proprietary,public,static,initial,A diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world,,,,,,,,,Research Edition,sample,2e887967c99846119ae843c95f14b2ee2499bf266356603f79c28606a5db279d,,True
1,1,1,1,Mapillar Mapillary Vistas Dataset,https://s3-eu-west-1.amazonaws.com/static.mapillary.com/MVD_commercial_samples.zip,0.015,v1,zip,proprietary,public,static,initial,A diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world,,,,,,,,,Commercial Edition,sample,6ae01ee6b76c40af3aad85701777358b7fa7b351a0fe8164af2cd5f4c107faf1,,True
2,2,2,2,GloVe,https://nlp.stanford.edu/software/GloVe-1.2.zip,0.000745,10/1/2015,zip,Apache License 2.0,public,,,"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.",source code,communications,"linguistics, word, vector",text,P3000,,,,,,9ac2836bfbce93fbf693b18212569d02f9f091909c03ff583d037dae754c19d3,,True
3,3,3,3,Wikipedia + Gigaword 5,http://nlp.stanford.edu/data/glove.6B.zip,0.82,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"6B tokens, 400k vocab, uncased, 50d, 100d, & 300d vectors",Electronics,Machine Learning,text,P3000,,,,,,3a0cee816e07886cf0935d2c07cb65d64dc5ea2bcd756f3924ec70e8f9ef52e9,,False
4,4,4,4,Common Crawl,http://nlp.stanford.edu/data/glove.42B.300d.zip,1.75,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"42B tokens, 1.9M vocab, uncased, 300d vectors",Electronics,Machine Learning,text,P3000,,,,,,7595599f507aa49427feb9501d52d6edd864e7e850da9f0783859422dfb78c63,,False
5,5,5,5,Common Crawl,http://nlp.stanford.edu/data/glove.840B.300d.zip,2.03,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"840B tokens, 2.2M vocab, cased, 300d vectors",Electronics,Machine Learning,text,P3000,,,,,,f55c11b2111ae729b324a8cd346aff5de47d3c3b285f3296584075df2fd75737,,False
6,6,6,6,Twitter,http://nlp.stanford.edu/data/glove.twitter.27B.zip,1.42,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors",Electronics,Machine Learning,text,P3000,,,,,,46e470daa060326b7d547ddf68187c041dbb8be2748e447d12a68f7386468cce,,False
7,7,7,7,U.S. Department of Agriculture's Nutrient Database,https://www.ars.usda.gov/ARSUserFiles/80400525/Data/SR/SR28/dnload/sr28asc.zip,0.006,5/1/2016,zip,,public,static,,"Food descriptions, nutrient information, weights, and other measurements.",,Life Sciences,"vitamins, food, health",text,P3001,Agriculture,,,,,19b72d047b9fe19e462f3d1999859dcf76b1b8e3faa110bc3fb996ec19305bc5,,True
8,8,8,8,E-MTAB-6854,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6854/E-MTAB-6854.processed.1.zip,0.324,6/25/2018,zip,,public,static,,Single-cell RNA-seq data of mammary gland epithelial cells from different gestational stages to detect and remove barcode swapping,,Life Sciences,"mus musculus, RNA, cells",text,p3003,Biology,,,,,099138a4643526f432d2e7374b72c802f733507b5305aa4366de042cf6e6b9a2,,False
9,9,9,9,E-MTAB-6051,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6051/E-MTAB-6051.processed.1.zip,0.056,6/25/2018,zip,,public,static,,Single-cell RNA sequencing of OT-I CD8+ T cells after stimulation with different affinity ligands,,Life Sciences,"mus musculus, RNA",text,p3004,Biology,,,,,06c0a13b59e0830462f9a5f9d0cb032719d3b04b6279354b25a736ad1133041c,,True
10,10,10,10,E-MTAB-6131,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6131/E-MTAB-6131.processed.1.zip,1.15,6/24/2018,zip,,public,static,,Methylation array for Multi-omics molecular profiling of primary prostate adenocarcinoma,,Life Sciences,"Homo sapiens, Methylation",text,p3005,Biology,,,,,d4fbcce5274a5496601d087a24454531ce547853ba892d60982a77a481307c4b,,False
11,11,11,11,E-MTAB-6888,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6888/E-MTAB-6888.processed.1.zip,0.005,6/22/2018,zip,,public,static,,Genome-wide maps of nuclear lamina interactions in HeLa cells identified using MethylAdenine IDentification (MadID),,Life Sciences,Homo sapiens,text,p3006,Biology,,,,,c5930d6ad8d7d14d3c2fdcbbbbaccd89598c92830a7430460b74649430c7ee92,,True
12,12,12,12,E-MTAB-6855,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6855/E-MTAB-6855.processed.1.zip,0.012,6/22/2018,zip,,public,static,,"Gene expression profiles of human breast cancer cell lines MCF-7, T47D and MDA-MB-231 co-cultured with 3T3-L1 adipocytes",,Life Sciences,"Homo sapiens, profiling",text,p3007,Biology,,,,,6d0babbe8d51543a531d9db26dda4a19dd4d2ea0206ffba721ff647541390841,,True
13,13,13,13,E-MTAB-6843,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6843/E-MTAB-6843.processed.1.zip,0.075,6/21/2018,zip,,public,static,,Single cell RNA-sequencing of T cells to examine barcode swapping,,Life Sciences,"Mus musculus, RNA, cells",text,p3008,Biology,,,,,1a75d365a457422c93b959d593f7d9c8c042badb145493c74952231ed01542fa,,False
14,14,14,14,E-MTAB-6063,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6063/E-MTAB-6063.processed.1.zip,0.000266,6/20/2018,zip,,public,static,,microRNA array on plasma from two diffuse large B-cell lymphoma patients and one healthy donor,,Life Sciences,"Homo sapiens, microRNA",text,p3009,Biology,,,,,0ca3aa2353055fab934696a3b8f0e974ae4b2744997a1dcc7afb61b8d77f1653,,True
15,15,15,15,E-MTAB-6503,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6503/E-MTAB-6503.processed.1.zip,0.0001,6/17/2018,zip,,public,static,,ChIP-seq of Mycobacterium smegmatis PafBC to identify in vivo binding sites,,Life Sciences,"Mycobacterium, ChIP-seq",text,p3010,Biology,,,,,cf72949d084f8c6c1dfc5a4f421f8b1acc53462c4fb5e13a3c6d653069c4e295,,True
16,16,16,16,E-MTAB-6497,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6497/E-MTAB-6497.processed.1.zip,0.006,6/17/2018,zip,,public,static,,RNA-seq of Mycobacterium smegmatis mc2-155 SMR5 wild-type vs. a pafBC deletion strain under different growth/stress conditions,,Life Sciences,"Mycobacterium, RNA-seq",text,p3011,Biology,,,,,7e41476d67403d2b12bf0326790c9e0a3197312d1ad16c20844c8d6a3e2b22cb,,True
17,17,17,17,E-MTAB-6315,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6315/E-MTAB-6315.processed.1.zip,0.0015,6/8/2018,zip,,public,static,,Methylation arrays (MethylationEPIC) reveal specific DNA hypermethylation of T cells during human hematopoietic differentiation,,Life Sciences,"Homo sapiens, methylation",text,p3012,Biology,,,,,b931215c40f569f035db71182a42553e5776641932c388ff1cff7b067bc171f8,,True
18,18,18,18,E-MTAB-6311,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6311/E-MTAB-6311.processed.1.zip,0.0017,6/8/2018,zip,,public,static,,Microarray analysis of transcription changes in zebrafish embryos of different developmental ages after exposure to cyanide,,Life Sciences,"Danio rerio, transcription profiling",text,p3013,Biology,,,,,2b25bc60ec0ced3e0aac9fba23eb67d8e015a0bc13c2946050a740f960fb0381,,True
19,19,19,19,Leishmania major,http://geneontology.org/gene-associations/gene_association.GeneDB_Lmajor.gz,0.000197,v1,gzip,,public,static,,"L. major, is a protozoan parasite that causes cutaneous leishmaniasis. It is a representative genome for all the species of Leishmania. Leishmania, possesse a two-unit genome, a nuclear genome and a mitochondrial kinetoplast. The nuclear genome is divided into 36 chromosomes with a combined haploid size of 33.6Mb.  ",,Life Sciences,,text,p3014,Biology,,,,,7efe029a59ea2e04806e9965038e793cd3fd26769842617abbe26b2221739d12,,True
20,20,20,20,Plasmodium falciparum,http://geneontology.org/gene-associations/gene_association.GeneDB_Pfalciparum.gz,0.00018,v1,gzip,,public,static,,Plasmodium falciparum gene information,,Life Sciences,,text,p3015,Biology,,,,,fa423d72016682cf94193df824f64ed09dc93b5fdda62608d8d46c12519d7fbe,,True
21,21,21,21,Trypanosoma brucei,http://geneontology.org/gene-associations/gene_association.GeneDB_Tbrucei.gz,0.0005,v1,gzip,,public,static,,"T. brucei, the protozoan responsible for African Sleeping Sickness, possesses a two-unit genome, a nuclear genome and a mitochondrial (kinetoplast) genome with a total estimated size of 35Mb/haploid genome. The nuclear genome is split into three classes of chromosomes according to their size on pulse filed gel electrophoresis, 11 megabase chromosomes (0.9-5.7 Mb), intermediate (300-900 kb) and minichromosomes (50-100 kb). ",,Life Sciences,"genome, chromosomes",text,p3016,Biology,,,,,3b6e1edb6978b4b6c251be3d55c17ef644075955d676a5544d46df843748ff84,,True
22,22,22,22,Agrobacterium tumefaciens str. C58,http://geneontology.org/gene-associations/gene_association.PAMGO_Atumefaciens.gz,5e-06,v1,gzip,,public,static,,This dataset contains the annotation of Agrobacterium tumefaciens C58 using GeneOntology (GO) terms.,,Life Sciences,"genome, evidence, annotation",text,p3017,Biology,,,,,66a754d483f8ff1bff93e30acaa63c3c89e949ac096a765cc1bad489b483b219,,True
23,23,23,23,Dickeya dadantii,http://geneontology.org/gene-associations/gene_association.PAMGO_Ddadantii.gz,6e-06,v1,gzip,,public,static,,These annotations are the result of a directed literature search aimed at annotating known and putative virulence factors of Dickeya dadantii.  Annotations were done by Dr. Brad Anderson in the laboratory of Dr. Nicole Perna at the University of Wisconsin.,,Life Sciences,"genome, annotations",text,p3018,Biology,,,,,4d7b126ce2c88463c51bdd9d0d10cfee6153830ecd78b97caf8a96f224d9bb31,,True
24,24,24,24,Magnaporthe grisea,http://geneontology.org/gene-associations/gene_association.PAMGO_Mgrisea.gz,0.000583,v1,gzip,,public,static,,These annotations are based on both experimental data from scientific literature and orthologs identified by stringent computational approaches. Annotations were done by the Magnaporthe grisea genome annotation team at North Carolina State University.,,Life Sciences,"genome, annotations",text,p3019,Biology,,,,,8bcd83c1bd61bf282feb754ffceb721054c863346169f82ceed18123df444ab5,,True
25,25,25,25,Oomycetes,http://geneontology.org/gene-associations/gene_association.PAMGO_Oomycetes.gz,1e-06,v1,gzip,,public,static,,These annotations are based solely on experimental data from scientific literature. Annotations are made by the Oomycete Genome Annotation team at the Virginia Bioinformatics Institute.,,Life Sciences,"genome, annotations",text,p3020,Biology,,,,,f1473bb72ad17e42a33f385ddcbc67749da081f4ad68089c79c3d8f1751abf3a,,True
26,26,26,26,Aspergillus nidulans,http://geneontology.org/gene-associations/gene_association.aspgd.gz,0.006,v1,gzip,,public,static,,Contains all GO annotations for Aspergillus nidulans genes (protein and RNA),,Life Sciences,"genome, genes",text,p3021,Biology,,,,,28fe715a561d1af9dc0beeea6425edec7dda92ee59e26bc36294c0038f196a8f,,True
27,27,27,27,Candida albicans,http://geneontology.org/gene-associations/gene_association.cgd.gz,0.004,v1,gzip,,public,static,,Contains all GO annotations for C. albicans genes (protein and RNA),,Life Sciences,genome,text,p3022,Biology,,,,,f081e044dda27c302e3fcbf7f48c944d44118175df5bfcbbfd13df38a4268d5a,,True
28,28,28,28,Dictyostelium discoideum,http://geneontology.org/gene-associations/gene_association.dictyBase.gz,0.002,v1,gzip,,public,static,,This file contains all GO annotations for Dictyostelium discoideum gene products (protein and RNA) in dictyBase.,,Life Sciences,"ontology, genes",text,p3023,Biology,,,,,2e1b1993118b0e6d105908016aa3d7220dea875f864d2e6e4e7f256dce6e6fe4,,True
29,29,29,29,Escherichia coli,http://geneontology.org/gene-associations/gene_association.ecocyc.gz,0.000641,v1,gzip,,public,static,,Contains all GO annotations for Escherichia coli K-12.,,Life Sciences,"E-coli, genes, ontology",text,p3024,Biology,,,,,d617aa6bd9cc13a61f7f87615bae7e99425820f0836b2242256373d0f6219e29,,True
30,30,30,30,Drosophila melanogaster,http://geneontology.org/gene-associations/gene_association.fb.gz,0.004,v1,gzip,,public,static,,Currently all GO annotations in FlyBase are attributed to genes. The GO terms describe the attributes of the products (both RNA and protein) encoded by these Drosophila genes.,,Life Sciences,"genes, annotations",text,p3025,Biology,,,,,6088771a0bbe714a9a43c2d4d1d9f2738700569457b382b4a173284f49e1465b,,True
31,31,31,31,Gallus gallus,http://geneontology.org/gene-associations/goa_chicken.gaf.gz,0.004,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"chicken, genes, annotations",text,p3026,Biology,,,,,8192948d402b29fb4f1496b9f60b5cf2aa41bba6da6db48098de9de9a5cac8fb,,True
32,32,32,32,Gallus gallus (complex),http://geneontology.org/gene-associations/goa_chicken_complex.gaf.gz,1.6e-05,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"chicken, genes, annotations",text,p3027,Biology,,,,,d2ddb067f10723b79b59b8b095f994dd64bac64514db9037e6488397ae858cff,,True
33,33,33,33,Gallus gallus (rna),http://geneontology.org/gene-associations/goa_chicken_rna.gaf.gz,3.2e-05,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"chicken, genes, annotations",text,p3028,Biology,,,,,247aa3e57d23d01d64b9b5d163bdad1df573a3817bb955ee2430e2c6f81b24fb,,True
34,34,34,34,Bos taurus,http://geneontology.org/gene-associations/goa_cow.gaf.gz,0.002,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"cow, genes, annotations",text,p3029,Biology,,,,,77f967b325289f4df67bc444ce8784674b040a29d970f46f6b96dddf900cfa8d,,True
35,35,35,35,Bos taurus (complex),http://geneontology.org/gene-associations/goa_cow_complex.gaf.gz,4e-06,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"cow, genes, annotations",text,p3030,Biology,,,,,a2974bc936f4a4115635395f4e919a1c8abc777fed3aa3ac46a61fcb1fffd94a,,True
36,36,36,36,Bos taurus (rna),http://geneontology.org/gene-associations/goa_cow_rna.gaf.gz,0.000362,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"cow, genes, annotations",text,p3031,Biology,,,,,a09535ae822bc8f0353faed57354a50579e79da7a283177f678168c6b5c92c35,,True
37,37,37,37,Canis lupus familiaris,http://geneontology.org/gene-associations/goa_dog.gaf.gz,0.002,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"dog, genes, annotations",text,p3032,Biology,,,,,3514b42191cf3dec35b827e7a0c950663fe732053337c4073bf1784696c7b4ff,,True
38,38,38,38,Canis lupus familiaris (complex),http://geneontology.org/gene-associations/goa_dog_complex.gaf.gz,0.00031600000000000004,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"dog, genes, annotations",text,p3033,Biology,,,,,5f3a2c53653527fef44d82bd79843eb2ad51e7255f254aae01565e586b2d11f2,,True
39,39,39,39,Canis lupus familiaris (rna),http://geneontology.org/gene-associations/goa_dog_rna.gaf.gz,0.00018,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"dog, genes, annotations",text,p3034,Biology,,,,,b8e90f7916e99b79415fe27e9959ab0b643169486b23b5ef22a153d802778043,,True
40,40,40,40,Homo sapiens,http://geneontology.org/gene-associations/goa_human.gaf.gz,0.009000000000000001,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"human, genes, annotations",text,p3035,Biology,,,,,82a11b1973f76eaf56e10abd0f89cbc1cfcd4c2774cb69ba2a2d0e29f9d3fd06,,True
41,41,41,41,Homo sapiens (complex),http://geneontology.org/gene-associations/goa_human_complex.gaf.gz,3.3e-05,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"human, genes, annotations",text,p3036,Biology,,,,,245f4f8c678c4e41be74d3a767d5e7d489007b032817fcb337c9c418a3248167,,True
42,42,42,42,Homo sapiens (rna),http://geneontology.org/gene-associations/goa_human_rna.gaf.gz,0.000378,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"human, genes, annotations",text,p3037,Biology,,,,,6fef32a2ec955193df86fd2c7e92f546d8c0b19539f6c4bdd171414000f5f5e0,,True
43,43,43,43,Sus scrofa,http://geneontology.org/gene-associations/goa_pig.gaf.gz,0.002,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"pig, genes, annotations",text,p3038,Biology,,,,,77e6166d17635e656115db1717c97a7acf97195ff0c538badce0f5819e55c5c0,,True
44,44,44,44,Sus scrofa (complex),http://geneontology.org/gene-associations/goa_pig_complex.gaf.gz,8e-06,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"Pig, genes, annotations",text,p3039,Biology,,,,,9684ca73c65fcc38de8bd37ed8305afd6c822497c5bcf732c63de162bc41c693,,True
45,45,45,45,Sus scrofa (rna),http://geneontology.org/gene-associations/goa_pig_rna.gaf.gz,0.000114,v1,gzip,,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"Pig, genes, annotations",text,p3040,Biology,,,,,44498d08602cc4bc7d70c5305ea6d789bc525043b3c6291fc3fd57a0e5832af5,,True
46,46,46,46,Human Genome Diversity Project,http://www.hagsc.org/hgdp/data/hgdp.zip,2.08,v1,zip,,public,static,,"The genotypes were generated on Illumina 650Y arrays, with a GenCall Score cutoff of 0.25. Samples with an overall call rate < 98.5% were removed. No filtering of SNPs was done, so be aware that low quality SNPs are included in these files.",,Life Sciences,"genome, chromosomes",text,p3041,Biology,,,,,bfe8aea560da4d008349042b87522aa30eed2db9a97de726fb41a7999861f099,,False
47,47,47,47,Berling Climate NetCDF ,https://www.beringclimate.noaa.gov/cache/5b322cddd36bc.zip,0.000217,v1,zip,proprietary,public,static,,"Climate indices, atmosphere, ocean, fishery, biology, and sea ice data files. ",,Earth Sciences,"climate, ocean, atmosphere, temperature",text,p3043,Climate,,,,,f6ef09e18f3633dd28d689c08bbd6a8e97cf1af3dd85a0d06b8c8269bbe880e7,,True
48,48,48,48,CrossRef DOI URLs,https://archive.org/compress/doi-urls,0.5,1/1/2014,zip,proprietary,public,static,,DOI URLs from 2007-2013,,Electronics,URL,text,p3045,Networks,,,,,95de65a62483ef778187d132966a38e1522539a045ee0e843eb7f7623b37addd,,False
49,49,49,49,College Scorecard,https://ed-public-download.app.cloud.gov/downloads/CollegeScorecard_Raw_Data.zip,0.235,3/29/2018,zip,proprietary,public,,,"Data that appear on the College Scorecard, as well as supporting data on student completion, debt and repayment, earnings, and more. The files include data from 1996 through 2016 for all undergraduate degree-granting institutions of higher educatio",,Education,College,text,p3046,University,,,,,39d7809c0eda58f446c545e020fc196a8e3e0af5146774e439bc81ebfdb6aedf,,False
50,50,50,50,The Almanac of Minutely Power dataset ,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FIE0S4#,1.0,5/17/2016,zip,Creative Commons Attribution License,public,,,"This dataset contains electricity, water, and natural gas measurements at one minute intervals. This dataset contains a total of 1,051,200 readings for 2 years of monitoring (from April/2012 to March/2014) per meter. There are a total of 21 power meters, 2 water meters (with additional appliance usage annotations), and 2 natural gas meters.",,Energy and Utilities,Power,text,p3047,,,,,,d36206cb269c309bcd319a70e56cba7731aa56137dc93afba504193b226bbdcb,,False
51,51,51,51,Commercial Building Energy Dataset,http://combed.github.io/downloads/combed.zip,0.207,v1,zip,proprietary,public,,,COMBED contains a month of smart meter data collected from different sensing points in IIITD's academic building.,,Energy and Utilities,smart meter,text,p3048,,,,,,875e7b4f368c25d688b6964abb39d68f95981a2b8907949ced7ece2651f368f4,,False
52,52,52,52,Global Power Plant Database,http://datasets.wri.org/dataset/540dcf46-f287-47ac-985d-269b04bea4c6/resource/27c271ef-63c3-49c5-a06a-f21bb7b96371/download/globalpowerplantdatabasev110,0.0077,6/1/2018,zip,proprietary,public,continuous,,"The Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights for one’s own analysis. Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. ",,Energy and Utilities,"plant capacity, generation",text,p3049,Power Plant,,,,,7d7ff3bf4967072ca3373fddeec6bd1e3cc27866904a5227d09f72f478250bd4,,True
53,53,53,53,High Frequency EMI Data Set,https://goo.gl/NerkK1,0.014,v1,zip,,public,,,"This work highlights an extensive empirical study of conducted EMI, performed on a set of 24 loads with 4 different test setups in lab settings and with one test setup in home settings.",Lab Setting,Energy and Utilities,"EMI, experimental",text,p3050,,,,,,2ab99f85084e4b75c123ffad6a2059c2627f371c8d1a43329ca3ab0807f371b7,,False
54,54,54,54,PLAID:  Plug Load Appliance Identification Dataset,http://plaidplug.com/static/dataset/Plaid.tar.gz,0.031,v1,gzip,proprietary,public,,,"PLAID currently includes current and voltage measurements sampled at 30 kHz from 11 different appliance types present in more than 60 households in Pittsburgh, Pennsylvania, USA. Data collection took place during the summer of 2013, and winter of 2014. Each appliance type is represented by dozens of different instances of varying make/models. For each appliance, three to six measurements were collected for each state transition.",,Energy and Utilities,,text,p3051,,,,,,ec8cc667baed4943080101cb53d76ec9b229e483c8cc7429dcf70d6e62b6e335,,False
55,55,55,55,Tracebase,https://github.com/areinhardt/tracebase/archive/master.zip,2.7,v1,zip,Open Database License,public,,,"The tracebase data set is a collection of power consumption traces which can be used in energy analytics research. Traces have been collected from individual electrical appliances, at an average reporting rate of one sample per second.",,Energy and Utilities,,text,p3052,,,,,,eb3d6602e6316452d17cd6731ab9e2036833f9513ef4db0ee03c17ba427df1a0,,False
56,56,56,56,Cricsheet,https://cricsheet.org/downloads/all.zip,0.466,v1,zip,,public,,,"Ball-by-ball data for Men’s and Women’s Test Matches, One-day internationals, Twenty20 Internationals, some other international T20s, and all Indian Premier League seasons",,Media and Entertainment,cricket,text,p3053,sports,,,,,e45adc94d779f3757f80594b2996e886853c661527704afab48ad18a0c37882f,,False
57,57,57,57,Ergast,http://ergast.com/downloads/f1db_csv.zip,0.0165,6/24/2018,zip,Creative Commons Attribution Licence,public,continuous,,"Race results, driver standings, lap times, pit stops, circuits, seasons, etc.",,Media and Entertainment,,"image, text",p3054,Sports,,,,,8a57fd6137596a30947c3e1f25d0d4da135ec2b754c85fafb253ad9799f1844a,,False
58,58,58,58,Pinhooker: Thoroughbread Bloodstock Sale Data,https://github.com/phillc73/pinhooker/archive/master.zip,0.0045,v1,zip,,public,,,Historic thoroughbred bloodstock sales data.,,Media and Entertainment,horse,text,p3055,Sports,,,,,22db59bf3ccb561ae50930adebaf049baa774ba82eca07047e683840b9373cd9,,True
59,59,59,59,Retrosheet Baseball Statistics,http://www.retrosheet.org/events/allas.zip,0.002,v1,zip,,public,,,Complete set of All-Star baseball games,,Media and Entertainment,baseball,text,p3056,Sports,,,,,789f22146ed00b7dd0a51ab0222f1e41dc383356d2a90cc1a037222326ed432d,,True
60,60,60,60,Retrosheet Baseball Statistics,http://www.retrosheet.org/events/allpost.zip,0.011,v1,zip,,public,,,Complete set of Post-Season baseball games,,Media and Entertainment,baseball,text,p3056,Sports,,,,,37215c33ae2f2cb631f4ee66e09e2d1082940ff656d9a0355460075dab139236,,False
61,61,61,61,"ATP Tennis Rankings, Results, and Stats",https://github.com/JeffSackmann/tennis_atp/archive/master.zip,0.207,8/4/2015,zip,Creative Commons Attribution License,public,,,"ATP player information including id, name, hand, birth date, and country code. Rankings include date, ranking, player id, and ranking points. The ATP match results feature stats and additional player information.",,Media and Entertainment,tennis,text,p3057,Sports,,,,,97582d4e61d4104ce38a2960fba1e940522f904366ccc96e87970c37aba58906,,False
62,62,62,62,"WTA Tennis Rankings, Results, and Stats",https://github.com/JeffSackmann/tennis_wta/archive/master.zip,0.12,5/28/2018,zip,Creative Commons Attribution License,public,,,"WTA player information including id, name, hand, birth date, and country code. Rankings include date, ranking, player id, and ranking points. The WTA match results feature stats and additional player information.",,Media and Entertainment,tennis,text,p3058,Sports,,,,,e2ae80b74369a69dabbb805f2cbbb9528fd4f9cdca73ad4ca98a7727a077db56,,False
63,63,63,63,Airline On-Time Statistics and Delay Causes,https://www.transtats.bts.gov/OT_Delay/ot_delaycause1.asp?display=download&pn=0&month=4&year=2018,0.042,v1,zip,,public,,,"Bureau of Transportation Statistics: On-Time Arrival Performance National (June, 2003 - April, 2018)",,Travel and Transportation,"airline, flight, statistics",text,p3059,Airline,,,,,656e8b3fd24ef1d1ffa74f29414c908ee5e262ca509aa7f349335a38ecf0b329,,False
64,64,64,64,201801 Ford GoBike ,https://s3.amazonaws.com/fordgobike-data/201801-fordgobike-tripdata.csv.zip,0.003,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,,3090b6a2b488d86cd680fa5d85509d94ed444d8b0d24c8a8480108ef33e574a4,,True
65,65,65,65,201802 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201802-fordgobike-tripdata.csv.zip,0.0038,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,,c30fed51beecbe2f82460ebc513b44cc736c3d92b6b887b02b701f0eac732b5c,,True
66,66,66,66,201803 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201803-fordgobike-tripdata.csv.zip,0.004,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,,9fa3598f805e92b2955ae8d104d0431c89bd24dddc69f7b27a51153f4c49a945,,True
67,67,67,67,201804 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201804-fordgobike-tripdata.csv.zip,0.0048,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,,6b6d809f956597860f86b0aa7b7a654cd4a976a10fa2c6ee72786918bfc98312,,True
68,68,68,68,201805 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201805-fordgobike-tripdata.csv.zip,0.0064,6/8/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,,bfeb83f9fe8a8f6153b8384bf68188701e2b97dbcda12f2c9a82d96163ec6610,,True
69,69,69,69,2017 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/2017-fordgobike-tripdata.csv,0.117,3/15/2018,csv,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,,bdde277ec8fc083b4206b1a7c6fe138907bbf14da35ffac2dd07dd3620d659a5,,False
70,70,70,70,Divvy 2013 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Stations_Trips_2013.zip,0.094,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,43ea723770854bf9943498cdd47a41292092ba40a37455b97b9c76d5d896d442,,False
71,71,71,71,Divvy 2014 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Stations_Trips_2014_Q1Q2.zip,0.006,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,d4e9a9fcfc42392de1cce76ece1891ffeaaf3840f3136914c725e5fa0b2dd907,,True
72,72,72,72,Divvy 2014 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Stations_Trips_2014_Q3Q4.zip,0.0064,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,21fc521be5288c1f09bb2ef95a7012f905e394cf13f3a04105981d2827bd7d58,,True
73,73,73,73,Divvy 2015 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2015-Q1Q2.zip,0.0066,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,5d1fcc603119ae83d98ccec7f8866f2605cf92d6122a071e1584e10dc8e8c38a,,True
74,74,74,74,Divvy 2015 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2015_Q3Q4.zip,0.012,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,31f25357d2d3bbdc5781a7bf186b1051c1c79f73bed141ef3ab115a6aef99410,,False
75,75,75,75,Divvy 2016 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2016_Q1Q2.zip,0.005,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,7fc063c1a162a900d69c6e77ead3170f9b1c3bb10df22b0f6a908ef7eda73b2a,,True
76,76,76,76,Divvy 2016 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2016_Q3Q4.zip,0.014,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,5564a20618103eac9261a68bd8223c4652788614e9b685f8d16e72bf7f998670,,False
77,77,77,77,Divvy 2017 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2017_Q1Q2.zip,0.006,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,a05d8859a90bcb99e20a655975dc3b40398477bc4d18cb37566f980c6ac7d90a,,True
78,78,78,78,Divvy 2017 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2017_Q3Q4.zip,0.017,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,,4863350b2224fbbbf934a37fa3df911dafd412e00176b1628c7334c79f3ed278,,False
79,79,79,79,2010 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2010-capitalbikeshare-tripdata.zip,0.0024,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,b352c5fbbdd4f6b62366ff1ed7d22c88334baed884f7c9280722f7dc08907321,,True
80,80,80,80,2011 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2011-capitalbikeshare-tripdata.zip,0.025,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,ba892f15f997c4ff9ce8accb1c5d25b94b2dcb875a2ab276b0eaa316fdb3e995,,False
81,81,81,81,2012 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2012-capitalbikeshare-tripdata.zip,0.043,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,37de5e978bfc4b1de3113da4df5562ed52c4842978c33723e4fea09749b69602,,False
82,82,82,82,2013 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2013-capitalbikeshare-tripdata.zip,0.056,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,9689def2534b940685f318fe6d9bc8d3387e9c52fde62995ed3f405543ab25f4,,False
83,83,83,83,2014 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2014-capitalbikeshare-tripdata.zip,0.066,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,9d635480d66a6043f5b69c53e37b4b8c2ed122b447be14d7655cb27792d3c169,,False
84,84,84,84,2015 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2015-capitalbikeshare-tripdata.zip,0.073,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,3ae8f4d4f13711fe8c44a97d0e116c5c96c4a6480b9273e05d446a6d6476864d,,False
85,85,85,85,2016 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2016-capitalbikeshare-tripdata.zip,0.078,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,5975d3ab0f96f083a3752b50ab235883ace19e16895876f3f4f863d0b1314edc,,False
86,86,86,86,2017 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2017-capitalbikeshare-tripdata.zip,0.09,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,b36ca9f8f8f30db4c67a37b8a82332bf1f5ff49132fccb05722e3a7fed80d2d4,,False
87,87,87,87,201801 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201801-capitalbikeshare-tripdata.zip,0.004,4/30/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,6c83e71eeef7bb1d40f211f9f0cbebc32909f6eef725972ae4988d40845fac0c,,True
88,88,88,88,201802 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201802-capitalbikeshare-tripdata.zip,0.004,5/11/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,9b9a5025331fa4bbf940acc7c750076b12f5f93b339688a1b83c4fabd2f6166a,,True
89,89,89,89,201803 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201803-capitalbikeshare-tripdata.zip,0.0055,5/11/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,dbd56d884973f8c957085d92aa982ce329fb366820e9c6585a6072d60dca9d21,,True
90,90,90,90,201804 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201804-capitalbikeshare-tripdata.zip,0.0075,5/11/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,e5dd8517f8cd74ceb3995dab78cd6169253aec940c324cf304062e650abc81ec,,True
91,91,91,91,201805 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201805-capitalbikeshare-tripdata.zip,0.0085,6/8/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,,c1ca28981531ad8ddafb68f283bbc70323d94045d3cdc383bcf627a643614f70,,True
92,92,92,92,2010 Nice Ride data ,https://niceridemn.egnyte.com/dl/byJLtGzvHM,0.0014,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,207373b5ca2449b1b15c2517a517fc74fe2a54b52fd6b5b8e070d5742c3e0bb5,,True
93,93,93,93,2011 Nice Ride data ,https://niceridemn.egnyte.com/dl/8xAYjDuS3L,0.0029,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,6b6e79380216ca04d3bd115fbe2385e5ec13d023d6c2ac69d79dbaa768170cc9,,True
94,94,94,94,2012 Nice Ride data ,https://niceridemn.egnyte.com/dl/GlYmbU2Bh0,0.0038,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,3b4520aa31717646e1c40efe78b9609bbcdf3ab532ef9e7b3f4675dfa4e078bb,,True
95,95,95,95,2013 Nice Ride data ,https://niceridemn.egnyte.com/dl/kdJ4WP0mHC,0.0044,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,c0a000256a690f2bf0a4accb424347c1b0f8c883cfdcb06fee1000020b9fd67c,,True
96,96,96,96,2014 Nice Ride data ,https://niceridemn.egnyte.com/dl/MZxvOEELWQ,0.0056,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,82257094c0096b0ba9f997c6c080ee2af21e5f6f430beeee04bcd9afdaac282e,,True
97,97,97,97,2015 Nice Ride data ,https://niceridemn.egnyte.com/dl/9nSKfEfxQ8,0.0059,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,0469826e0708b688cb3546686652593343ea46ba0d20ed1d69af18cdda84b442,,True
98,98,98,98,2016 Nice Ride data ,https://niceridemn.egnyte.com/dl/gYLZtGrwEk,0.0055,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,2bc62b7644bec18f5a5f34c0eecf8e68caaab5d0912cc20a2e3e288bada32355,,True
99,99,99,99,2017 Nice Ride data ,https://niceridemn.egnyte.com/dl/QrR5Ih5Xeq,0.0058,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,,0e4e622cdf9352ac70fd74fbf38b4774c7e1d469f1e1a86d2b2712458506d2de,,True
100,100,100,100,GeoLife GPS Trajectory,https://www.microsoft.com/en-us/download/confirmation.aspx?id=52367,1.71,8/9/2012,zip,,public,static,,"A GPS trajectory of this dataset is represented by a sequence of time-stamped points, each of which contains the information of latitude, longitude and altitude. This dataset contains 17,621 trajectories with a total distance of about 1.2 million kilometers and a total duration of 48,000+ hours. These trajectories were recorded by different GPS loggers and GPS-phones, and have a variety of sampling rates. 91 percent of the trajectories are logged in a dense representation, e.g. every 1~5 seconds or every 5~10 meters per point.",,Travel and Transportation,GPS,text,p3064,,,,,,a7ba726b53dc8934470f4149c0370b58d6d5eceb4af1acb7fb1d029d9f3a85e0,,False
101,101,101,101,2014 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2014.zip,0.15,v1,zip,,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,,2550370a3028ab5d4cfce605c12058eae93b3718cd1a9a6ad746026c544ff336,,False
102,102,102,102,2015 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2015.zip,0.173,v1,zip,,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,,006b7dcaf188ebe4c2e6dd2ec4be24c9712141b12864d6b65684003ef116e456,,False
103,103,103,103,2016 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2016.zip,0.201,v1,zip,,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,,18ba27f9f4f48a30ecab368bfddb5f907323c6865c4c62255bf4248a31c64324,,False
104,104,104,104,2017 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2017.zip,0.238,v1,zip,,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,,6ba7d47093b58f2aef03090fcc3911b3ab76317736ba4de787bf4aca3200e0cd,,False
105,105,105,105,2018 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2018.zip,0.052000000000000005,v1,zip,,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,,3439af33d5871feb0cc7b78d62ce3111f0d53a0e831ca8262437791313b75484,,False
106,106,106,106,NYC Uber Trip Data,https://github.com/fivethirtyeight/uber-tlc-foil-response/archive/master.zip,0.4,v1,zip,,public,static,,"This directory contains data on over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015.",,Travel and Transportation,Uber,text,p3067,Automotive,,,,,c47c7dedcb33d5bbcf9ab96a9357f045b508d90590a4aa8e1768ad2d46bb1866,,False
107,107,107,107,2013 Housing Affordability Data System ,https://www.huduser.gov/portal/datasets/hads/hads2013n_ASCII.zip,0.018000000000000002,v1,zip,,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,,8147386cab99f0ae57bad1f1e6ad44aee8ff1d627921aa56972345600e34b59a,,False
108,108,108,108,2011 Housing Affordability Data System ,https://www.huduser.gov/portal/datasets/hads/hads2011(ASCII).zip,0.022,v1,zip,,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,,e4240c5e5579cc1bcbbe6cb7f58265ad74151eec45905c18cf3fd9716aa07ca0,,False
109,109,109,109,FDIC Failed Bank List,http://www.fdic.gov/bank/individual/failed/banklist.csv,5e-06,v1,csv,U.S. Government Work,public,,,"This list includes banks which have failed since October 1, 2000.",,Banking,failed banks,text,p3073,,,,,,4d2b298be5d30b05ea451554c029e8445d4e294a446c02f6172485bd47a1cac7,,True
110,110,110,110,Farmers Markets Directory and Geographic Data,https://apps.ams.usda.gov/FarmersMarketsExport/ExcelExport.aspx,0.0032,2/4/2018,csv,Creative Commons Attribution License,public,,,"This dataset includes longitude and latitude, state, address, name, and zip code of Farmers Markets in the United States.",,Consumer Products,Farmer's Markets,text,p3074,,,,,,50fd9cd8bd9140f52988f87b0648ea3f09e636f76e27240fdf569713165618d3,,True
111,111,111,111,Demographic Statistics,https://data.cityofnewyork.us/api/views/kku6-nxdu/rows.csv?accessType=DOWNLOAD,2.9e-05,4/4/2018,csv,,public,static,,New York demographic statistics by zip code,,Government,"Demographic, New York",text,p3075,,,,,,675550a0bc3aab8e6a02bcffff0f4618d3a404785e28fb69754af158250143e5,,True
112,112,112,112,2011 Zip Code Data,https://www.irs.gov/pub/irs-soi/2011zipcode.zip,0.235,v1,zip,,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,,36b8ef2ca6114a013e2358921a4bd317c2dba3d14cb9fc3152fd152de2cf1208,,False
113,113,113,113,2012 Zip Code Data,https://www.irs.gov/pub/irs-soi/2012zipcode.zip,0.226,v1,zip,,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,,f4e06b921daac3f0e8c65ec813ffb96cada0706d363ec5a7374c651fec968089,,False
114,114,114,114,2013 Zip Code Data,https://www.irs.gov/pub/irs-soi/zipcode2013.zip,0.51,v1,zip,,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,,d25b2143476a479e456cf388ea4be5d72ce69449cbcfb7f4b1b6cd1d359b197a,,False
115,115,115,115,2014 Zip Code Data,https://www.irs.gov/pub/irs-soi/zipcode2014.zip,0.365,v1,zip,,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,,5a8a06bc860bb2188d932972db471973e3ade24cb204807c2dc077a4999882cb,,False
116,116,116,116,2015 Zip Code Data,https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2015-zip-code-data-soi,0.379,v1,zip,,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,,be63e02c7f9ff1a4a801fbc42de69c1621d473aa86f966af6f36467367740004,,False
117,117,117,117,Consumer Complaint Database,https://data.consumerfinance.gov/api/views/s6ew-h6mp/rows.csv?accessType=DOWNLOAD,0.574,v1,csv,U.S. Government Work,public,static,,Bureau of Consumer Financial Protection: consumer complaints about financial products and services.,,Government,"Consumer, complaint",text,p3078,,,,,,30130d32cb3d54540183572d8c0019238baf5d47b24ba16e1dd49e96f6337bc5,,False
118,118,118,118,2010 Census Populations by Zip Code,https://data.lacity.org/api/views/nxs9-385f/rows.csv?accessType=DOWNLOAD,1.2e-05,2/3/2018,csv,,public,static,,This data comes from the 2010 Census Profile of General Population and Housing Characteristics. Zip codes are limited to those that fall at least partially within LA city boundaries.,,Government,"Los Angeles,  population",text,p3079,population,,,,,4fe2a9af791503f41cb8d2cdace355a8c87246eb9dd6579f935bab60d50d4174,,True
119,119,119,119,Crimes - 2001 to present,https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD,0.01,6/6/2018,csv,,public,continuous,,"This dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. ",,Government,Crime,text,p3080,,,,,,8c8b3aa0d6ed0912ef4fcaca02a30c7fa1254b376e8f09ac0f5c59971453212b,,False
120,120,120,120,Nottingham Music Database,http://ifdo.ca/~seymour/nottingham/nottingham_database.zip,0.0005,v1,zip,,public,static,,This database contains over 1000 Folk Tunes converted to ABC format,,Media and Entertainment,Music,text,p3082,,,,,,c6aa001963d67776c8ab2111f3101e2b49a75bd4ca7718dd6170c37377601765,,True
121,121,121,121,Complication Rates by Hospital,https://data.medicare.gov/views/bg9k-emty/files/e0ad4b50-10b2-41e8-a9a7-85b705c75178?content_type=application%2Fzip%3B%20charset%3Dbinary&filename=Hospital_Revised_FlatFiles.zip,0.306,5/23/2018,zip,,public,,,"These data allow you to compare the quality of care at over 4,000 Medicare-certified hospitals across the country.",,Healthcare,,text,p3084,,,,,,dc5910301325dd8facea5eaf7c60e52240368b178e1efa7aba2b88142c86e7a3,,False
122,122,122,122,E-MTAB-6659,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6659/E-MTAB-6659.processed.1.zip,0.016,6/1/2018,zip,,public,static,,"Transcription profiling by array of wild type pea seeds and seeds with embryo-specific expression of a bacterial trehalose 6-phosphate phosphatase (TPP), encoded by the otsB gene to investigate the influence of decreased T6P content on gene expression in developing cotyledons.",,Life Sciences,Pisum sativum,text,p3089,Biology,,,,,a91914f2831f7ecbe9815957dc35f916689eba32a60537bd0daf1a607e9d048b,,False
123,123,123,123,E-MTAB-5783,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-5783/E-MTAB-5783.processed.1.zip,0.038,5/22/2018,zip,,public,static,,"RNA-seq of formalin-fixed, paraffin-embedded uninvolved terminal ileal tissue obtained from ileo-colic resection surgeries of Crohns disease and control patients.",,Life Sciences,Homo sapiens,text,p3090,Biology,,,,,60935b3006086401e34a63fc399c75bcb31aa0e1804f226c5c39143aff089cb4,,False
124,124,124,124,E-MTAB-6406,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6406/E-MTAB-6406.processed.1.zip,0.0007,5/19/2018,zip,,public,static,,MIRNA - Integrative multi-omics survey of effects carbon-based engineered nanomaterials on lung derived cell-lines.,,Life Sciences,Homo sapiens,text,p3091,Biology,,,,,f05040b5d210325cf153789933e35cf8601b4ee3776ea17775b5bf1db15d2bf6,,True
125,125,125,125,E-MTAB-6397,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6397/E-MTAB-6397.processed.1.zip,0.764,5/19/2018,zip,,public,static,,DNA-METHYLATION - Integrative multi-omics survey of effects carbon-based engineered nanomaterials on lung derived cell-lines.,,Life Sciences,Homo sapiens,text,p3092,Biology,,,,,3cd44767331ce18dbebc39ea0185e02f663bccf8663e5b9c49057c72ab60631b,,False
126,126,126,126,E-MTAB-6396,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6396/E-MTAB-6396.processed.1.zip,0.077,5/19/2018,zip,,public,static,,MRNA-Integrative multi-omics survey of effects carbon-based engineered nanomaterials on lung derived cell-lines.,,Life Sciences,Homo sapiens,text,p3093,Biology,,,,,f95f1bca1a4ed22bd030035d332758e03c2d2502990c66015e07f725dab3bebe,,False
127,127,127,127,E-MTAB-5776,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-5776/E-MTAB-5776.processed.1.zip,0.0089,5/16/2018,zip,,public,static,,Microarray transcription profiling of healthy and PVYNTN infected potato tuber tissues (necrotic and non-necrotic).,,Life Sciences,Solanum tuberosum,text,p3094,Biology,,,,,fcb3565b4b3e3fc785bc83c3a71249da6ebc2aa97e3d3579a32b2bed0b0a2b92,,True
128,128,128,128,Large Age Gap (LAG),http://www.ivl.disco.unimib.it/wp-content/uploads/2016/09/LAGdataset_100.zip,0.07,v1,zip,proprietary,public,static,,"Large Age-Gap (LAG) dataset is a dataset containing variations of age in the wild, with images ranging from child/young to adult/old. The dataset contains 3,828 images of 1,010 celebrities. For each identity at least one child/young image and one adult/old image are present.",,Life Sciences,Human,image,p3095,,,,,,b06094fef32eac07e795fbadaf235972a198c34c28cd275a88b369bc1aeac5e6,,False
129,129,129,129,DAVIS: Densely Annotated Video Segmentation 2016,https://graphics.ethz.ch/Downloads/Data/Davis/DAVIS-data.zip,2.0,v1,zip,proprietary,public,static,,"DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motion-blur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation.",,Electronics,"Research, motion",video,p3096,Computer Vision,,,,,1ef9a0612937fdedc784ce05b4cb54477cff7aeb32e4c2fca9bbd2a0f26f6846,,False
130,130,130,130,DAVIS: Densely Annotated Video Segmentation 2017,https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-Full-Resolution.zip,3.2,v1,zip,proprietary,public,static,,"DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motion-blur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation.",,Electronics,"Research, motion",video,p3096,Computer Vision,,,,,c12ffb99f8eb01037dc17f6866a6d6bc8111578ed84a1e0cda324dc1b0ac7a90,,False
131,131,131,131,NYU Symmetry Database,https://symmetry.cs.nyu.edu/Database.zip,0.011,v1,zip,proprietary,public,,,The mirror symmetry database contains 176 single-symmetry and 63 multiple-symmetry images (.png files) with accompanying ground-truth annotations (.mat files).,,Electronics,"Research, motion","image, text",p3100,Computer Vision,,,,,cf8f7bc7e03b09ab1bf69071e4609b80cfcd8d1bd2f46303e666a1b08efe93cb,,False
132,132,132,132,Mouse Embryo Tracking Database,http://celltracking.bio.nyu.edu/MouEmbTrkDtb.zip,3.58,v1,zip,proprietary,public,,,"The database contains, for each of the 100 examples: (1) the uncompressed frames, up to the 10th frame after the appearance of the 8th cell; (2) a text file with the trajectories of all the cells, from appearance to division (for cells of generations 1 to 3), where a trajectory is a sequence of pairs (center, radius); (3) a movie file showing the trajectories of the cells.", NYU Center for Genomics and Systems Biology,Electronics,"Research, biology","image, text",p3101,Computer Vision,,,,,a5d8820caf24a2fefab847f556ea0aeccdd461b6889a8fb2c79922e02b5c4ef5,,False
133,133,133,133,Edge milling heads Dataset,http://pitia.unileon.es/varp/sites/default/files/MillingHeadDataSet.zip,0.118,v1,zip,proprietary,public,,,"This data set comprises 144 images of an edge profile cutting head of a milling machine. It contains a total of 30 cutting inserts. The cutting head is formed by 6 diagonals of inserts in radial direction along the tool perimeter, encompassing 5 inserts per diagonal in axial direction. Positions of the last and first inserts of consecutive diagonals are aligned in the same vertical. Therefore, even though we have 30 inserts, there are 24 equally spaced positions of inserts along the tool perimeter. Additionally, inserts are squared shape with four 90o indexable cutting edges. Inserts are fastened with a screw. Rake angle is 0.",,Electronics,"Consumer product, research, detection",image,p3102,Computer Vision,,,,,c781857e619d08f06d093a144f5a720fd6869b244ab0dd145036062cf35cc392,,False
134,134,134,134,Georgia Tech Face Database,http://www.anefian.com/research/gt_db.zip,0.128,v1,zip,proprietary,public,,,"This dataset contains images of 50 people taken in two or three sessions between 06/01/99 and 11/15/99 at the Center for Signal and Image Processing at  Georgia Institute of Technology. All people in the database are represented by 15 color JPEG images with cluttered background taken at resolution 640x480 pixels. The average size of the faces in these images is 150x150 pixels. The pictures show frontal and/or tilted faces with different facial expressions, lighting conditions and scale.",,Electronics,"Research, facial recognition",image,p3103,Computer Vision,,,,,c6e6976b7c67382ee47e1699b451f86496118451f964e12b4c8eb519325f3eb5,,False
135,135,135,135,Labeled Faces in the Wild,http://vis-www.cs.umass.edu/lfw/lfw.tgz,0.173,v1,tgz,proprietary,public,,,"The data set contains more than 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. 1680 of the people pictured have two or more distinct photos in the data set.",University of Massachussettes,Electronics,facial recognition,image,p3124,,,,,,66fc96fe6fd4878b2af62ea638974eb0df3b75a6ea5748de6baec921df98ca8b,,False
136,136,136,136,Microsoft Marco Training,https://msmarco.blob.core.windows.net/msmarco/train_v2.1.json.gz,3.83,v2.1,gzip,MIT License,public,,,"MS MARCO(Microsoft Machine Reading Comprehension) is a large scale dataset focused on machine reading comprehension and question answering. In MS MARCO, all question have been generated from real anonymized Bing user queries which grounds the dataset in a real world problem and can provide researchers real contrainsts their models might be used in.The context passages, from which the answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated.",Training Set,Electronics,Machine Learning,text,p3127,,,,,,c4641ae8cbcb8a1b8934a434381892322c547a2ae0a1b16d737a49acba7a780b,,False
137,137,137,137,Microsoft Marco Dev,https://msmarco.blob.core.windows.net/msmarco/dev_v2.1.json.gz,0.47,v2.1,gzip,MIT License,public,,,"MS MARCO(Microsoft Machine Reading Comprehension) is a large scale dataset focused on machine reading comprehension and question answering. In MS MARCO, all question have been generated from real anonymized Bing user queries which grounds the dataset in a real world problem and can provide researchers real contrainsts their models might be used in.The context passages, from which the answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated.",Dev Set,Electronics,Machine Learning,text,p3127,,,,,,239bf0f810b86118320b83b9a9432016c720d1845ade6e149eba995201faa426,,False
138,138,138,138,Microsoft Marco Evaluation,https://msmarco.blob.core.windows.net/msmarco/eval_v2.1_public.json.gz,0.47,v2.1,gzip,MIT License,public,,,"MS MARCO(Microsoft Machine Reading Comprehension) is a large scale dataset focused on machine reading comprehension and question answering. In MS MARCO, all question have been generated from real anonymized Bing user queries which grounds the dataset in a real world problem and can provide researchers real contrainsts their models might be used in.The context passages, from which the answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated.",Evaluation Set,Electronics,Machine Learning,text,p3127,,,,,,c0d6e9f9784604d09e04abd87dd66fff71fc8ebf66fc68122edb120e06b51064,,False
139,139,139,139,MNIST,https://pjreddie.com/media/files/mnist_train.csv,0.109,v1,csv,,public,,,"Dataset of 25x25, centered, B&W handwritten digits.",,Electronics,Machine Learning,image,p3128,Computer Vision,,,,,86854c2b9146bb91435b718d7ce5423efff5276975ea0e291cdc456b26c6a86a,,False
140,140,140,140,LSUN Saliency Prediction (iSun),http://lsun.cs.princeton.edu/challenge/2015/eyetracking/data/image.zip,2.0,v1,zip,,public,,,"The data is collected by gaze tracking from Amazon Mechanical Turk using a web-cam. All our images are from the SUN database. For each image, we provide the image content in JPG, image resolution, scene category, and ground truth (including gaze trajectory, fixation points, and saliency mask, for training and validation sets only).",images,Electronics,Object recognition,image,p3129,Machine Learning,,,,,447e4b0b1d4ea968f33639892ecca39dea40f5f7aea484a706da36beb20fa467,,False
141,141,141,141,LSUN Saliency Prediction (Salicon),http://lsun.cs.princeton.edu/challenge/2015/eyetracking_salicon/data/image.zip,3.0,v1,zip,,public,,,"The data is collected via mouse cursor tracking in a new psychophysical paradigm from Amazon Mechanical Turk by NUS VIP Lab. All the images are from MS COCO dataset. For each image, we provide the image content in JPG, image resolution and ground truth (including mouse trajectory, fixation points, and saliency mask, for training and validation sets only).",images,Electronics,Object recognition,image,p3129,Machine Learning,,,,,4afd04f7f5b5db13e56ebd0b9b9409b4ec07c81aae92f536221caff62b4ea73c,,False
142,142,142,142,LSUN Saliency Prediction (iSun) Ground Truth,http://lsun.cs.princeton.edu/challenge/2015/eyetracking/data/saliency.zip,12.0,v1,zip,,public,,,"The data is collected by gaze tracking from Amazon Mechanical Turk using a web-cam. All our images are from the SUN database. For each image, we provide the image content in JPG, image resolution, scene category, and ground truth (including gaze trajectory, fixation points, and saliency mask, for training and validation sets only).",Saliency Map Ground Truth,Electronics,Object recognition,image,p3129,Machine Learning,,,,,cdb78801d942a70769e9560d8c6d0a933f271a65ab0b564515b38aacbd4f69cd,,False
143,143,143,143,LSUN Saliency Prediction (Salicon) Ground Truth,http://lsun.cs.princeton.edu/challenge/2015/eyetracking_salicon/data/saliency.zip,19.0,v1,zip,,public,,,"The data is collected via mouse cursor tracking in a new psychophysical paradigm from Amazon Mechanical Turk by NUS VIP Lab. All the images are from MS COCO dataset. For each image, we provide the image content in JPG, image resolution and ground truth (including mouse trajectory, fixation points, and saliency mask, for training and validation sets only).",Saliency Map Ground Truth,Electronics,Object recognition,image,p3129,Machine Learning,,,,,919ddadfd17c3dc19235a81ba77c197c01a0de8e2c8ea725a8306d1379efc1c9,,False
144,144,144,144,LSUN Room Layout Estimation,http://lsun.cs.princeton.edu/challenge/2015/roomlayout/data/image.zip,2.0,v1,zip,,public,,,"In this task, an algorithm needs to estimate the room layout from a single indoor scene image. All the images are indoor. They are from the SUN database and our LSUN scene classification database. We assume that a room showed in an image can be represented by a part of a 3D box. Therefore, the room layout estimation is formulated as a way to predict the positions of intersection between planar walls, ceiling and floors. There are 4000 images for training, 394 images for validation and 1000 images for testing. All the images have valid room layout that can be clearly annotated by human.",images,Electronics,Object recognition,image,p3129,Machine Learning,,,,,06396074478288e5e207c32f37459fd61db53343870d70c09020dcfdbd6be853,,False
145,145,145,145,The Visual Genome Objects,http://visualgenome.org/static/data/dataset/objects.json.zip,0.35,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,977f3f6516232db8217c95e5288dec3c96fa992b4966581c6d6ff6b0a2da60eb,,False
146,146,146,146,The Visual Genome Relationships,http://visualgenome.org/static/data/dataset/relationships.json.zip,0.7,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,8523d6476abc8f50a198be433f51719204d5bb0628da50207a05eb4394729a7f,,False
147,147,147,147,The Visual Genome Object Aliases,http://visualgenome.org/static/data/dataset/object_alias.txt,6e-05,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,f8b9d9d55485000faaf347e8e89083ade2f25ca87bb3948e317d496cc32750d2,,True
148,148,148,148,The Visual Genome Relationship Aliases,http://visualgenome.org/static/data/dataset/relationship_alias.txt,0.000122,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,6f2c06495efa104befe30ae8b9b51a94f77cbaf37046822475f887417b375101,,True
149,149,149,149,The Visual Genome Synsets for Objects,http://visualgenome.org/static/data/dataset/object_synsets.json.zip,0.000357,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,8d4548f438d232601455472357fdd045a9fd016611b73c2746974dd8a778604f,,True
150,150,150,150,The Visual Genome Synsets for Attributes,http://visualgenome.org/static/data/dataset/attribute_synsets.json.zip,0.000171,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,b47acbcc12c0b5f6f4efd6c1b639c8082180f41c6391a29b3d769e233cc29b10,,True
151,151,151,151,The Visual Genome Synsets for Relationships,http://visualgenome.org/static/data/dataset/relationship_synsets.json.zip,9.8e-05,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,2b4a8bd57548174b6efadbfcdef5f06d577917d24ec17bc1072ad57138743fe6,,True
152,152,152,152,The Visual Genome Images 01,https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip,9.2,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",image,p3130,Machine Learning,,,,,9264a689d12b5988c176c69fcdd6596f41757d3ff79918da77b6179ba6f45756,,False
153,153,153,153,The Visual Genome Images 02,https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip,5.47,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",image,p3130,Machine Learning,,,,,899491aaa5a60bedbafc50aaacf40625329c4716b8b813019f76d5b4269f7751,,False
154,154,154,154,The Visual Genome Image Metadata,http://visualgenome.org/static/data/dataset/image_data.json.zip,17.6,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,fd2a346b31c6a660789a8a5750a44bdfe72b44d8718801d2114be2f433a4d17d,,False
155,155,155,155,The Visual Genome Region Descriptions,http://visualgenome.org/static/data/dataset/region_descriptions.json.zip,0.7120000000000001,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,759b718b0472f2eb0e24fc39e7f8386d71063e6b142e4ef3bc25556f6f09e8d0,,False
156,156,156,156,The Visual Genome Attributes,http://visualgenome.org/static/data/dataset/attributes.json.zip,0.462,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,2cc0b7904bd716fb7e3a2df5731ce8b42fd9bfaf5bdf50e0eb59e70f80a70f4d,,False
157,157,157,157,The Visual Genome Question Answers,http://visualgenome.org/static/data/dataset/question_answers.json.zip,0.8029999999999999,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,,693428de19f5357bd5db9c7d2e53a3aae836875fefa95f0086f6e8006f98a0f3,,False
158,158,158,158,The Children's Book Test,http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz,1.7,v1,tgz,,public,,,"Baseline of (Question + context, Answer) pairs extracted from Children’s books available through Project Gutenberg. Useful for question-answering, reading comprehension, and factoid look-up.",,Education,"Machine learning, question",text,p3134,Machine Learning,,,,,1aeaca2e1b662b4076cd7e6106adafd56565821cbe661a8b382148e0412d5460,,False
159,159,159,159,Stanford Sentiment Analysis,http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip,0.006,v1,zip,,public,,,A standard sentiment dataset with fine-grained sentiment annotations at every node of each sentence’s parse tree.,,Electronics,"Sentiment, analysis",text,p3135,Machine Learning,,,,,372fd48a06a3e6c068e9792df7849ae89e5ba4ba6874e8c93df0fd69ec9cd680,,True
160,160,160,160,20 Newsgroups,http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz,0.09,v1,gzip,,public,,,"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It is one of the classic datasets for text classification, usually useful as a benchmark for either pure classification or as a validation of any IR / indexing algorithm.",,Electronics,Machine Learning,text,p3136,Machine Learning,,,,,26cf8c567dd2e41b311ce4e3fd9b381ebf45f9b84b089fcf68c8cf9e5198b486,,False
161,161,161,161,Libri Speech 100 hours,http://www.openslr.org/resources/12/train-clean-100.tar.gz,6.3,v1,gzip,proprietary,public,,,Audio books data set of text and speech.,,Electronics,"Machine Learning, speech",audio,p3137,Machine Learning,,,,,5fb9bb44e6fff65a2b9f47054d7d2d87c4ad0c8b1bc503c6b22adadc8b71cae2,,False
162,162,162,162,Libri Speech 360 hours,http://www.openslr.org/resources/12/train-clean-360.tar.gz,23.0,v1,gzip,proprietary,public,,,Audio books data set of text and speech.,,Electronics,"Machine Learning, speech",audio,p3137,Machine Learning,,,,,4de1221f7a09a1df8143f273843f3d85ad27b964f7ee0e86fee8ba82ec88dc76,,False
163,163,163,163,Libri Speech 500 hours,http://www.openslr.org/resources/12/train-other-500.tar.gz,30.0,v1,gzip,proprietary,public,,,Audio books data set of text and speech.,,Electronics,"Machine Learning, speech",audio,p3137,Machine Learning,,,,,1a18e0d130270961bb013f8100cf93e5405d496b0e4f9d2e9b353daabd0d8ada,,False
164,164,164,164,MovieLens 20M Dataset,http://files.grouplens.org/datasets/movielens/ml-20m.zip,0.19,v1,zip,proprietary,public,,,"Stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags.",,Media and Entertainment,"Machine Learning, review, ratings",text,p3138,Machine Learning,,,,,e6fa19bd11c692d9e5ca8a2e36befcf96c092ac20072f8dcaab91bb8e1a6031a,,False
165,165,165,165,MovieLens Latest Dataset,http://files.grouplens.org/datasets/movielens/ml-latest.zip,0.224,v1,zip,proprietary,public,,,"26,000,000 ratings and 750,000 tag applications applied to 45,000 movies by 270,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags.",,Media and Entertainment,"Machine Learning, review, ratings",text,p3139,Machine Learning,,,,,2f0fc620ae11b5da8b1d98e5460d9572377f0be028c3726779a2d6181c474d84,,False
166,166,166,166,WikiLens,http://files.grouplens.org/datasets/wikilens/wikilens.20080202.tar.gz,0.0034,2/1/2008,gzip,proprietary,public,,,"WikiLens was a generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items.",,Electronics,"Machine Learning, review, ratings",text,p3140,Machine Learning,,,,,79cac77ee3644721649cc7fe7d43dc88222b31ff36b05db97137e69848719d9a,,True
167,167,167,167,Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_stereo_centre_01.tar,2.83,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,,da7bebf38a6f9027f85ed1a21cf9ad49f7aa28ec1fde9dbf6600bd888498f0e2,,False
168,168,168,168,Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_stereo_left_01.tar,2.77,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,,ebbe648ca5fe7e6309b15395e155538115cc7206d37e0dbfdcefff4f98365113,,False
169,169,169,169,Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_stereo_right_01.tar,2.86,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,,5e59cbc9af795f3b7258b6d14686ce546fad465d6e2bf0a56ec45e14920c318a,,False
170,170,170,170,Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_vo.tar,0.44,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,,8d38af53f2a183faae8b9cbf1393cc02972938f0df68b1a326c21b70cbcb0e54,,False
171,171,171,171,Grasshopper 2 right,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_mono_right_01.tar,2.03,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"right side view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1002,Self-driving cars,,,,,6e15f8acab6fa89cd180eea82f53136e7b8d63afd714c3c0a687428249915aca,,False
172,172,172,172,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"114 frames (00:11 minutes), 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc",,Automotive,city,image,P1007,Self-driving cars,,,,,72280dffd2433db06112197a41506a65937d40a935c32faa1c585f9e8f9b9813,,False
173,173,173,173,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"114 frames (00:11 minutes), 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc",,Automotive,city,image,P1007,Self-driving cars,,,,,213ae94c3e4a4eb412daedfc690e3fbc559e84d92e9839d88adf64d33aace127,,False
174,174,174,174,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_tracklets.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"114 frames (00:11 minutes), 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc",,Automotive,city,text,P1007,Self-driving cars,,,,,d8a44c65f83ecb90f85fc8fea1e28cb45cb892fb873bd33771c92f7d7b54dbaf,,False
175,175,175,175,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_extract.zip,0.3,9/26/2011,zip,proprietary,public,,initial,"83 frames (00:08 minutes), 1 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1008,Self-driving cars,,,,,a00c7812d9d585eb52205f98e7cc64723b094796b9b0be27e7180b82497e87a0,,False
176,176,176,176,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0009/2011_09_26_drive_0009_extract.zip,1.8,9/26/2011,zip,proprietary,public,,initial,"453 frames (00:45 minutes), 89 Cars, 3 Vans, 2 Trucks, 3 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 1 Misc",,Automotive,city,image,P1010,Self-driving cars,,,,,f97b7e8d3a2b89747174cfdff621bff277430cb0b3215db6709082391c259315,,False
177,177,177,177,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0011/2011_09_26_drive_0011_extract.zip,0.9,9/26/2011,zip,proprietary,public,,initial,"238 frames (00:23 minutes), 15 Cars, 1 Vans, 1 Trucks, 1 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",,Automotive,city,image,P1011,Self-driving cars,,,,,35436c248cba28ff2b372123729448d46db5b57dd958906fbc17fd5ede567a99,,False
178,178,178,178,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0014/2011_09_26_drive_0014_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"320 frames (00:32 minutes), 8 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1012,Self-driving cars,,,,,2061116248f3094c1a5368ce56dba90a76fb87cf635505135118ceb95b04ffed,,False
179,179,179,179,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0017/2011_09_26_drive_0017_extract.zip,0.5,9/26/2011,zip,proprietary,public,,initial,"120 frames (00:12 minutes), 4 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1013,Self-driving cars,,,,,6d8417c8f1982a68e0121a2e2310d2441e7ebfad1bea52abfb9f48ec3332ee69,,False
180,180,180,180,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0018/2011_09_26_drive_0018_extract.zip,1.1,9/26/2011,zip,proprietary,public,,initial,"276 frames (00:27 minutes), 11 Cars, 2 Vans, 2 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1014,Self-driving cars,,,,,02f3e03ba4fdad06de132eabc364a931af1ff57ae4b6686a536efbcd2c1116b2,,False
181,181,181,181,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0048/2011_09_26_drive_0048_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"444 frames (00:44 minutes), 26 Cars, 15 Vans, 1 Trucks, 3 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 2 Misc",,Automotive,city,image,P1015,Self-driving cars,,,,,204a7b3bac96e0d45c7fd38fed90df89bd6cb68528ef6ad7d65da23304a85bd6,,False
182,182,182,182,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0056/2011_09_26_drive_0056_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"300 frames (00:30 minutes), 13 Cars, 3 Vans, 1 Trucks, 2 Pedestrians, 0 Sitters, 1 Cyclists, 6 Trams, 2 Misc",,Automotive,city,image,P1016,Self-driving cars,,,,,cd31a22eccce298bd4614da45100a93b2c6780785811fa423283dbcf605ab19a,,False
183,183,183,183,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0057/2011_09_26_drive_0057_extract.zip,1.4,9/26/2011,zip,proprietary,public,,initial,"367 frames (00:36 minutes), 22 Cars, 4 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1017,Self-driving cars,,,,,d9088d1fa1fc182c9d64b94bbc01758fb571ad46635dbf2f2a96649b2e2b1a7a,,False
184,184,184,184,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0059/2011_09_26_drive_0059_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"52 Cars, 3 Vans, 0 Trucks, 5 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1018,Self-driving cars,,,,,2e9db53a5a84c596ead85017021346906a53ba346fcd73bdc3a54fc7a875e18a,,False
185,185,185,185,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0060/2011_09_26_drive_0060_extract.zip,0.3,9/26/2011,zip,proprietary,public,,initial,"84 frames (00:08 minutes), 2 Cars, 0 Vans, 0 Trucks, 1 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1019,Self-driving cars,,,,,2e7b198ffd84c81335c69c179c919050546197d55e6a9cd5aef71e1bd4319116,,False
186,186,186,186,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0084/2011_09_26_drive_0084_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"389 frames (00:38 minutes), 49 Cars, 1 Vans, 0 Trucks, 3 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 2 Misc",Imag resolution: 1392 x 512 pixels,Automotive,city,image,P1020,Self-driving cars,,,,,78f553be1b3eafdbb48b5bf604c7946b127432e2ded7b99ba7669cd5ada8c951,,False
187,187,187,187,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0091/2011_09_26_drive_0091_extract.zip,1.3,9/26/2011,zip,proprietary,public,,initial,"346 frames (00:34 minutes), 2 Cars, 1 Vans, 0 Trucks, 42 Pedestrians, 14 Sitters, 8 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1021,Self-driving cars,,,,,6586326c18ac5564468fb48fbaa0305d962faf103e10905fbab448ffaec17553,,False
188,188,188,188,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0093/2011_09_26_drive_0093_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"439 frames (00:43 minutes), 54 Cars, 2 Vans, 0 Trucks, 4 Pedestrians, 2 Sitters, 2 Cyclists, 0 Trams, 2 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1022,Self-driving cars,,,,,dcfcc7c51439d17c18d5c354e0efdd6bab04c0b0adda2bac3086afd88d0d0d9c,,False
189,189,189,189,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0095/2011_09_26_drive_0095_extract.zip,1.1,9/26/2011,zip,proprietary,public,,initial,"274 frames (00:27 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1023,Self-driving cars,,,,,dd8051607d6b1f303e7a78e08f0b61cf2fd90288a2871e62078523f443013df1,,False
190,190,190,190,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0096/2011_09_26_drive_0096_extract.zip,1.9,9/26/2011,zip,proprietary,public,,initial,"481 frames (00:48 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1024,Self-driving cars,,,,,4dcd2583175a5284537786c4223408c5b1597bbf28c50f70666d13eb5f7d5b68,,False
191,191,191,191,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0104/2011_09_26_drive_0104_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"318 frames (00:31 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1025,Self-driving cars,,,,,edcecf8a5c03bc33985bdc31fa1d4bc8ae15c83ee0d6b0362c351e3eb9fe7127,,False
192,192,192,192,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0106/2011_09_26_drive_0106_extract.zip,0.9,9/26/2011,zip,proprietary,public,,initial,"233 frames (00:23 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1026,Self-driving cars,,,,,b8936f2b334f968036ea598de230f0211152da5af33240c7b648768d86519f6e,,False
193,193,193,193,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0117/2011_09_26_drive_0117_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,93 frames (00:09 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1027,Self-driving cars,,,,,b4f369c34c79941cb174f278b6b97a110f993a2ad1cbbd64afeaccf5ebd54b25,,False
194,194,194,194,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0001/2011_09_28_drive_0001_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,111 frames (00:11 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1029,Self-driving cars,,,,,1dbbd42359e016b3b8882b9b40d427f5d7639ee1d0f71315e6859c7080ad5cb5,,False
195,195,195,195,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0002/2011_09_28_drive_0002_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,382 frames (00:38 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1030,Self-driving cars,,,,,d91e7183755b583f3edc5b9e13b396d9af5aae279ac77d7ec40dab120b1b0e24,,False
196,196,196,196,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_29_drive_0026/2011_09_29_drive_0026_extract.zip,0.6,9/26/2011,zip,proprietary,public,,initial,164 frames (00:16 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1031,Self-driving cars,,,,,9c8d661161e24398b4dabd1fcba47662cd9f590c6a27140a9b753037b8fffdec,,False
197,197,197,197,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_29_drive_0071/2011_09_29_drive_0071_extract.zip,4.1,9/26/2011,zip,proprietary,public,,initial,1065 frames (01:46 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1032,Self-driving cars,,,,,dad74c1a14ceeae78a94b4051fb3820468f7dbe4aeced4e7349d9d5778644668,,False
198,198,198,198,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0019/2011_09_26_drive_0019_extract.zip,1.9,9/26/2011,zip,proprietary,public,,initial,"487 frames (00:48 minutes), 13 Cars, 2 Vans, 0 Trucks, 3 Pedestrians, 0 Sitters, 7 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1033,Self-driving cars,,,,,e7cc77f7a848fe1ea6c0b745ae62e41a1305135fcebe8a796b9da1fe69fe16b1,,False
199,199,199,199,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0020/2011_09_26_drive_0020_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"92 frames (00:09 minutes), 5 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1034,Self-driving cars,,,,,f274592ac9694f523eb98565f563eb82d7bf12ebe231995d56f2a2f19204c436,,False
200,200,200,200,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0022/2011_09_26_drive_0022_extract.zip,3.1,9/26/2011,zip,proprietary,public,,initial,"806 frames (01:20 minutes), 53 Cars, 4 Vans, 1 Trucks, 2 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 3 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1035,Self-driving cars,,,,,3e275f42ec1c8c5cabf196ff2bbcff69f2b410db7cd2cd1af6ae77f3e23032af,,False
201,201,201,201,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0023/2011_09_26_drive_0023_extract.zip,1.9,9/26/2011,zip,proprietary,public,,initial,"480 frames (00:48 minutes), 150 Cars, 6 Vans, 1 Trucks, 1 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 8 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1036,Self-driving cars,,,,,9858a9447182bd5ff04c25c8e6d1c7eae40e0cb82a4ed862fd8e72d692f37b0f,,False
202,202,202,202,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0035/2011_09_26_drive_0035_extract.zip,0.5,9/26/2011,zip,proprietary,public,,initial,"137 frames (00:13 minutes), 23 Cars, 6 Vans, 2 Trucks, 2 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 3 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1037,Self-driving cars,,,,,2d4fe9a65d3ee56714cffea9efd223611dedf2dbba7d0a00e9cde852993f5129,,False
203,203,203,203,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0036/2011_09_26_drive_0036_extract.zip,3.1,9/26/2011,zip,proprietary,public,,initial,"809 frames (01:20 minutes), 80 Cars, 7 Vans, 1 Trucks, 1 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1038,Self-driving cars,,,,,f4ea2d1c76dff7980b0179b0cbbd2126484b47d29d7ce91314147ed19f6908c2,,False
204,204,204,204,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0039/2011_09_26_drive_0039_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"401 frames (00:40 minutes), 35 Cars, 4 Vans, 1 Trucks, 2 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1039,Self-driving cars,,,,,707efb8562a9b6f0e2ff5593f646679452ff09bcaf8bee798c74e8ae6c1b94e7,,False
205,205,205,205,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0046/2011_09_26_drive_0046_extract.zip,0.5,9/26/2011,zip,proprietary,public,,initial,"131 frames (00:13 minutes), 8 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1040,Self-driving cars,,,,,1f03251c9ba1e44cb95144c25c47d9500ced8cfb56b9f6da5eff757e4cdb7d03,,False
206,206,206,206,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0061/2011_09_26_drive_0061_extract.zip,2.7,9/26/2011,zip,proprietary,public,,initial,"709 frames (01:10 minutes), 39 Cars, 0 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 3 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1041,Self-driving cars,,,,,2b134dfa171a9627870f9d758c41fab8f71778a57ff5726db470382c6eb2fca6,,False
207,207,207,207,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0064/2011_09_26_drive_0064_extract.zip,2.2,9/26/2011,zip,proprietary,public,,initial,"576 frames (00:57 minutes), 38 Cars, 8 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1042,Self-driving cars,,,,,41b9e8764515e2b8bff9c6e7e9319143b4d89ae97905c04a12573a75fb715def,,False
208,208,208,208,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0079/2011_09_26_drive_0079_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"107 frames (00:10 minutes), 2 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1043,Self-driving cars,,,,,457f4dbc9e9c106f5b9a54028ae72f8b6cc8b24b7571d13d74b109abd66a7ff8,,False
209,209,209,209,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0086/2011_09_26_drive_0086_extract.zip,2.7,9/26/2011,zip,proprietary,public,,initial,"711 frames (01:11 minutes), 3 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1044,Self-driving cars,,,,,b8090b893f100d87833ec86d7a1f35576cbd31f28d7c36ecbe799cc5738a5a1a,,False
210,210,210,210,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0087/2011_09_26_drive_0087_extract.zip,2.8,9/26/2011,zip,proprietary,public,,initial,"735 frames (01:13 minutes), 6 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1045,Self-driving cars,,,,,cfcb5c6c351896c81ff1d441c09967101c49b13392808b3e0fd49bf9c3315c83,,False
211,211,211,211,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0018/2011_09_30_drive_0018_extract.zip,10.7,9/26/2011,zip,proprietary,public,,initial,2768 frames (04:36 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1046,Self-driving cars,,,,,ac1832c51002793fc5c6cdb990d2dfaa5319dd6fa627d329dc4adfbdea15f74d,,False
212,212,212,212,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0020/2011_09_30_drive_0020_extract.zip,4.3,9/26/2011,zip,proprietary,public,,initial,1111 frames (01:51 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1047,Self-driving cars,,,,,6594a426609bbce667803e9826834d4bce6b1a29b00f7afcaec937b37aee7f15,,False
213,213,213,213,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0027/2011_09_30_drive_0027_extract.zip,4.3,9/26/2011,zip,proprietary,public,,initial,1112 frames (01:51 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1048,Self-driving cars,,,,,d2bab789304d63afc2e69768a264e68914121affa69ba442ee9f69b3b48632ae,,False
214,214,214,214,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0028/2011_09_30_drive_0028_extract.zip,20.0,9/26/2011,zip,proprietary,public,,initial,5183 frames (08:38 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1049,Self-driving cars,,,,,abb4dbc74de2c86d0703cedd58d576f15c608b4e523504c31d047b4fb6988705,,False
215,215,215,215,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0033/2011_09_30_drive_0033_extract.zip,6.2,9/26/2011,zip,proprietary,public,,initial,1600 frames (02:40 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1050,Self-driving cars,,,,,1891a08fbd65192cf36a3db479adeafd577f9f99f85bdaca6391a84f17d9f21e,,False
216,216,216,216,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0034/2011_09_30_drive_0034_extract.zip,4.8,9/26/2011,zip,proprietary,public,,initial,1230 frames (02:03 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1051,Self-driving cars,,,,,9eb9a7dbc14394e3eedbdf69052ad79353afefc03b07957fcd0a398d78b12b1f,,False
217,217,217,217,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0027/2011_10_03_drive_0027_extract.zip,17.6,9/26/2011,zip,proprietary,public,,initial,4550 frames (07:35 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1052,Self-driving cars,,,,,126638e68d33d18edfb690ee6c2bc2232194a613d69e8602df3e36cb26304adb,,False
218,218,218,218,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0034/2011_10_03_drive_0034_extract.zip,18.0,9/26/2011,zip,proprietary,public,,initial,4669 frames (07:46 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1053,Self-driving cars,,,,,270e6138affb3c27eb9ed870e716fe26d5aa32f7621cc3d9d8ab68163891e19d,,False
219,219,219,219,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0015/2011_09_26_drive_0015_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"303 frames (00:30 minutes), 33 Cars, 1 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1054,Self-driving cars,,,,,43acd034f71937306c7775f68ef2d688086d4ed84e920ebad6f5f04d0fca84a1,,False
220,220,220,220,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0027/2011_09_26_drive_0027_extract.zip,0.7,9/26/2011,zip,proprietary,public,,initial,"194 frames (00:19 minutes), 3 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1055,Self-driving cars,,,,,a9d1173f3bf8cbfe7f60a4f9a9e321262d1e14c9f0f4b0e68dde477d708fb79f,,False
221,221,221,221,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0028/2011_09_26_drive_0028_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"435 frames (00:43 minutes), 9 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1056,Self-driving cars,,,,,4f86d0b0311a6aae010f3255ddce7e24a281859f13fe03aa6471445848dc039d,,False
222,222,222,222,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0029/2011_09_26_drive_0029_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"3 Cars, 0 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1057,Self-driving cars,,,,,dcf0f66adec89f6f594f7b639f68f122b9e25dd5f86ca8898c46367ea87d290b,,False
223,223,223,223,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0032/2011_09_26_drive_0032_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"396 frames (00:39 minutes), 21 Cars, 4 Vans, 2 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1058,Self-driving cars,,,,,1f094a810f3a753250020239da0ff7a3e20c7e44c3be39b7e1f5960c27c4e883,,False
224,224,224,224,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0052/2011_09_26_drive_0052_extract.zip,0.3,9/26/2011,zip,proprietary,public,,initial,"84 frames (00:08 minutes), 4 Cars, 4 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1059,Self-driving cars,,,,,fb113d3c73c4eb271566ab040bc1319c0190ac9e0a6c850dafe153a6042e926c,,False
225,225,225,225,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0070/2011_09_26_drive_0070_extract.zip,1.6,9/26/2011,zip,proprietary,public,,initial,"426 frames (00:42 minutes), 2 Cars, 2 Vans, 0 Trucks, 2 Pedestrians, 0 Sitters, 2 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1060,Self-driving cars,,,,,0b68484798e4fb20dc2e696cc0e2d79a0c4a1368ae6955abf14fc915bcdef56e,,False
226,226,226,226,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0101/2011_09_26_drive_0101_extract.zip,3.6,9/26/2011,zip,proprietary,public,,initial,941 frames (01:34 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1061,Self-driving cars,,,,,6f763231bba55af8daecadc95a5b8f102485bdbb9b4121d9decda874551937ba,,False
227,227,227,227,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_29_drive_0004/2011_09_29_drive_0004_extract.zip,1.3,9/26/2011,zip,proprietary,public,,initial,345 frames (00:34 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1062,Self-driving cars,,,,,4f4984837e0ced4b79d447a8645d32a070b289a962644e792714231bec88d0b7,,False
228,228,228,228,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0016/2011_09_30_drive_0016_extract.zip,1.1,9/26/2011,zip,proprietary,public,,initial,285 frames (00:28 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1063,Self-driving cars,,,,,2ab901432c7e34414c6107047fff21806ee608384a39181c9ab0f0a2c840753f,,False
229,229,229,229,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0042/2011_10_03_drive_0042_extract.zip,4.5,9/26/2011,zip,proprietary,public,,initial,1176 frames (01:57 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1064,Self-driving cars,,,,,f1e3bf0ede4bba01afd74d346cd7fdd635259bfd01a7afbf9d56ad5df19ae588,,False
230,230,230,230,Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_extract.zip,3.3,9/26/2011,zip,proprietary,public,,initial,844 frames (01:24 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1065,Self-driving cars,,,,,35ddff3c7ce1345bdb387f61b75cfbe7b7a9644c8838cf8df963e11f89ff5cab,,False
231,231,231,231,Ford Campus Vision and Lidar Data Set,robots.engin.umich.edu/uploads/SoftwareData/Ford/dataset-1.tar.gz,78.0,11/1/2009,tar,,public,,initial,"large and small-scale loop closures, Ford Research campus and downtown Dearborn, Michigan","professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system",Automotive,"GPS, campus, ford research, downtown",image,P1066,Self-driving cars,,,,,d599f226bbafb18e6725fd07dad26c070297792cdd96a2131d08a9c7f4c7e7fb,,False
232,232,232,232,Ford Campus Vision and Lidar Data Set,robots.engin.umich.edu/uploads/SoftwareData/Ford/dataset-2.tar.gz,119.0,11/1/2009,tar,,public,,initial,a loop inside the Ford campus in Dearborn Michigan,"professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system",Automotive,"GPS, campus, ford research, downtown",image,P1067,Self-driving cars,,,,,6dd91541cf9947f50555d4b7870948a32b451d539ae69b358e058c7c00bceaff,,False
233,233,233,233,Ford Campus Vision and Lidar Data Set,robots.engin.umich.edu/uploads/SoftwareData/Ford/dataset-1-subset.tgz,6.0,11/1/2009,tar,,public,,initial,"large and small-scale loop closures, Ford Research campus and downtown Dearborn, Michigan","professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system",Automotive,"GPS, campus, ford research, downtown",image,P1068,Self-driving cars,,,,,28278aa787d61782a9439e8803671fe735dbfcbfb2fc26929061158b038cb249,,False
234,234,234,234,The CCSAD dataset,https://www.dropbox.com/s/uhl5j354ncfesdu/S1A-20140527_151946.zip?dl=0,6.2,5/27/2014,zip,,public,,Sequence S1A,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,,16e87d89ff2d1c8f6564ed93b5df1177a79617c7b5af582f2f500de666af3e70,,False
235,235,235,235,The CCSAD dataset,https://www.dropbox.com/s/v2f3yugihwaaxyt/S1B-20140527_153617.zip?dl=0,4.5,5/27/2014,zip,,public,,Sequence S1B,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,,f2dccb001a4fa57d9dee9b09b730a20870984a9869d473e813430f3e1a2dc8c0,,False
236,236,236,236,The CCSAD dataset,https://www.dropbox.com/s/6e55lfkeqsx3zm4/S1D-20140527_160412.zip?dl=0,4.9,5/27/2014,zip,,public,,Sequence S1D,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,,7e0afb882c1b2caf4e8e6f15afce591547294ccd820b53e289070452153ede1f,,False
237,237,237,237,The CCSAD dataset,https://www.dropbox.com/s/xb0lvr4bo19kddx/S1E-20140527_162403.zip?dl=0,4.5,5/27/2014,zip,,public,,Sequence S1E,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,,30831156318f2ed61acd70297b15a7a333f6da530ed2b9c9dcd8cc31b9a42875,,False
238,238,238,238,The CCSAD dataset,https://www.dropbox.com/s/om4v4kdxpdu9oh0/S1F-20140527_163102.zip?dl=0,5.6,5/27/2014,zip,,public,,Sequence S1F,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,,18e7e4d9529601a16e800ba88acad5f6ec7621dfe895ec386d6d775266e15fb0,,False
239,239,239,239,The CCSAD dataset,https://www.dropbox.com/s/4is8rx0tbfb8z0w/S1G-20140527_163836.zip?dl=0,4.6,5/27/2014,zip,,public,,Sequence S1G,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,,8cd336de4355de1743290ad8b04f9f44ffaf744df9313f86b4e5d4501ce30113,,False
240,240,240,240,The CCSAD dataset,https://www.dropbox.com/s/rrd1f3cksj7ow5t/S1C-20140527_154854.zip?dl=0,4.5,5/27/2014,zip,,public,,Sequence S1C,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,151a305e911a544dbd2c178dc65f1bd1d287fd619a9b06568142093545afd836,,False
241,241,241,241,The CCSAD dataset,https://www.dropbox.com/s/ydjoj2gfi8qwawc/S1H-20140527_164804.zip?dl=0,5.6,5/27/2014,zip,,public,,Sequence S1H,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,710d7212acdb892c77180332598830f4ed01d8631a1bbcdbcc7a58ecb3ae30c3,,False
242,242,242,242,The CCSAD dataset,https://www.dropbox.com/s/7f18payobkg7b4w/S1I-20140527_165728.zip?dl=0,4.1,5/27/2014,zip,,public,,Sequence S1I,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,3d25635f17acde72d88ea95ebf73cfb4fe8901534a9bca8291e80d1382b8ad3f,,False
243,243,243,243,The CCSAD dataset,https://www.dropbox.com/s/xrdvlk2jwmeyvlv/S1J-20140527_170958.zip?dl=0,6.9,5/27/2014,zip,,public,,Sequence S1J,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,3b139e7e1af51c6c27b823723cf08e52af8e497fe0124f3b4f7ce7a9a6fe467e,,False
244,244,244,244,The CCSAD dataset,https://www.dropbox.com/s/mfz20gazymkh5bt/S1K-20140527_172001.zip?dl=0,5.7,5/27/2014,zip,,public,,Sequence S1K,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,6e87db66c17e6feeee4be0afbc00422a99545a7ac5a4f010c6790e57c52b5980,,False
245,245,245,245,The CCSAD dataset,https://www.dropbox.com/s/3fu1xlq3dkke6hk/S1L-20140527_172711.zip?dl=0,4.1,5/27/2014,zip,,public,,Sequence S1L,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,8f3cd89008089a0c5512d72955f6666c92ac2817358265fafb34d98ceb9f5578,,False
246,246,246,246,The CCSAD dataset,https://www.dropbox.com/s/otpii6h5zd63qk0/S2C-20140604_152758.zip?dl=0,7.2,5/27/2014,zip,,public,,Sequence S2C,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,5ed7723bfd184bd93ad47193ef1bd1ecfeba6fa16f42d706def740165491067f,,False
247,247,247,247,The CCSAD dataset,https://www.dropbox.com/s/q98vplqlmjnzurx/S2D-20140604_153453.zip?dl=0,7.4,5/27/2014,zip,,public,,Sequence S2D,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,abd368b08761802f7fbf4ed54a52ed72b23ee7d49ba3eba2388cde534c958a3d,,False
248,248,248,248,The CCSAD dataset,https://www.dropbox.com/s/7rveus8mxaxayjc/S2E-20140604_155133.zip?dl=0,8.5,5/27/2014,zip,,public,,Sequence S2E,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,2c399e7f77f759d75e2030f3da718412ece15eb38ff148cc78372de9ed38eee1,,False
249,249,249,249,The CCSAD dataset,https://www.dropbox.com/s/zysmz33yl4vme3v/S2H-20140604_164515.zip?dl=0,9.3,5/27/2014,zip,,public,,Sequence S2H,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,9bf8774fc895a950ace2131fd1a183ea3a0f6d9a2f33bc30efb4d9deef70d14a,,False
250,250,250,250,The CCSAD dataset,https://www.dropbox.com/s/1dqgc3zgppry7hx/S3F-20140704_190936.zip?dl=0,7.2,5/27/2014,zip,,public,,Sequence S3F,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,2f2177b1ac8abcecba6f0edde42da3bd0daff21c5c3f6b6cc28272769bafcc0a,,False
251,251,251,251,The CCSAD dataset,https://www.dropbox.com/s/ksga8hh1fz7hwgn/S3G-20140704_191748.zip?dl=0,6.6,5/27/2014,zip,,public,,Sequence S3G,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,b99fbc6f80725770c6b65280da4665fd1c95d6e28a7575fd6a9e83da49b81604,,False
252,252,252,252,The CCSAD dataset,https://www.dropbox.com/s/0q4hspny9uvcksd/S3S-20140705_012726.zip?dl=0,6.8,5/27/2014,zip,,public,,Sequence S3S,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,,f4b2bae23a9757ce7faa74555375cd3cf79c2ff4fa1727e3e1e4f16cf1c9ea5d,,False
253,253,253,253,The CCSAD dataset,https://www.dropbox.com/s/3m9k8slmm7t4ktm/S2A-20140604_144706.zip?dl=0,6.0,5/27/2014,zip,,public,,Sequence S2A,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,89eb3226b9dfa7f267fa51e89bd377c51739afb260be5cf8bf7872478077b844,,False
254,254,254,254,The CCSAD dataset,https://www.dropbox.com/s/y026offhq7zckcb/S2B-20140604_151558.zip?dl=0,7.5,5/27/2014,zip,,public,,Sequence S2B,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,c3af457a38019a3c0d5cb92009b7c15846285ca3a1bc4d0ced7f87c09576a46e,,False
255,255,255,255,The CCSAD dataset,https://www.dropbox.com/s/8q2h6lg5nrnfwld/S2F-20140604_161757.zip?dl=0,8.0,5/27/2014,zip,,public,,Sequence S2F,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,3f522ea8c94ecf0142a92eb19a09bcf1774dfb5f4e57579816e4945ae6a1c974,,False
256,256,256,256,The CCSAD dataset,https://www.dropbox.com/s/4eopiqjvyhjsxkl/S2G-20140604_162539.zip?dl=0,7.7,5/27/2014,zip,,public,,Sequence S2G,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,1e791dcd87cba4318f5f8ed98c6e93ff94464730dddc5873e8ba8008b132b1c6,,False
257,257,257,257,The CCSAD dataset,https://www.dropbox.com/s/fndyv96tsptnzoh/S2I-20140604_165737.zip?dl=0,7.1,5/27/2014,zip,,public,,Sequence S2I,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,5d46935a4b410e46da1fef0572fa84f91e666406254af92b0a5ce93c580d7e48,,False
258,258,258,258,The CCSAD dataset,https://www.dropbox.com/s/w6gksoajngmxucn/S2J-20140604_170400.zip?dl=0,6.7,5/27/2014,zip,,public,,Sequence S2J,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,8e89361d246fd7cfeaabe561e58409368f72af776e1bfc0d78c0780073354945,,False
259,259,259,259,The CCSAD dataset,https://www.dropbox.com/s/caigrl4vogx7b23/S3A-20140704_181506.zip?dl=0,5.6,5/27/2014,zip,,public,,Sequence S3A,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,80c8f1a6d2cad05f3251fba4fee7825e89c4f0386f793c368f57aef3c49c5b74,,False
260,260,260,260,The CCSAD dataset,https://www.dropbox.com/s/v1rizjc66okhqke/S3B-20140704_182151.zip?dl=0,5.6,5/27/2014,zip,,public,,Sequence S3B,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,6364548ce0f7caa10c819808e33a24f8a1d75fe40470f203d71ccb849cdf0741,,False
261,261,261,261,The CCSAD dataset,https://www.dropbox.com/s/4gjx6gni91qzlyj/S3C-20140704_182720.zip?dl=0,6.0,5/27/2014,zip,,public,,Sequence S3C,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,0cd6e2811a781819f4040153d659b3b54f919bb7067e372b4075cd830a1743f3,,False
262,262,262,262,The CCSAD dataset,https://www.dropbox.com/s/0fbchwr930a3f7v/S3D-20140704_183242.zip?dl=0,6.3,5/27/2014,zip,,public,,Sequence S3D,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,11d3da75233b0f7e3cef6a1083b2d91e193e610aaf775c5aaff8a74643031186,,False
263,263,263,263,The CCSAD dataset,https://www.dropbox.com/s/t8f7mowvutvcu9i/S3E-20140704_190046.zip?dl=0,6.6,5/27/2014,zip,,public,,Sequence S3E,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,a8358599047f9fa58e9ddf72b06925dada8b64bb5d6b7b1ad5dd4b330f3aaaa4,,False
264,264,264,264,The CCSAD dataset,https://www.dropbox.com/s/suhkhcpoj8sb9ir/S3H-20140704_192518.zip?dl=0,7.6,5/27/2014,zip,,public,,Sequence S3H,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,d339ab8293c46e907af261c9d8b47c5d216995f3cfc1958ff51321bb12c88498,,False
265,265,265,265,The CCSAD dataset,https://www.dropbox.com/s/jnw6r52cxnf3b9b/S3I-20140704_195540.zip?dl=0,7.9,5/27/2014,zip,,public,,Sequence S3I,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,5ece1da929e1ea96beb5d4ac3631bce3b24f498e1f7634634f9d8600dafeeadb,,False
266,266,266,266,The CCSAD dataset,https://www.dropbox.com/s/snx75w43cxiziwm/S3J-20140704_200203.zip?dl=0,7.0,5/27/2014,zip,,public,,Sequence S3J,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,32acd974c10b956e120477e33cfabe24581ca0043ab33d8be3a9c68e4cbe58f3,,False
267,267,267,267,The CCSAD dataset,https://www.dropbox.com/s/g4aj1chwab21a40/S3K-20140704_201624.zip?dl=0,3.8,5/27/2014,zip,,public,,Sequence S3K,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,,08634ed77155b352054fd836b064354b2cc8ac6ed1dc81f3a5e852e5e3b7125d,,False
268,268,268,268,The CCSAD dataset,https://www.dropbox.com/s/hgav8b09kegp60y/S3L-20140705_000919.zip?dl=0,6.8,5/27/2014,zip,,public,,Sequence S3L,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,,6095f78c43f246d5f0b98e7962a304f48180605e9a6b2dd9e0dbe7c75a0a54c4,,False
269,269,269,269,The CCSAD dataset,https://www.dropbox.com/s/0lw0a0l3ianbgbc/S3M-20140705_001513.zip?dl=0,4.8,5/27/2014,zip,,public,,Sequence S3M,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,,059c349cdcd688cae83511c1b6854ff02a1d38e8442810e78195523937ce061a,,False
270,270,270,270,The CCSAD dataset,https://www.dropbox.com/s/vmgdzborashkxat/S3N-20140705_002134.zip?dl=0,6.2,5/27/2014,zip,,public,,Sequence S3N,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,,73a657f4230598a1a497b8fc63b0f6cea0ebde264267aba3bc38bbd3fa107cf6,,False
271,271,271,271,The CCSAD dataset,https://www.dropbox.com/s/oo9vp8vim9275fc/S3O-20140705_002829.zip?dl=0,5.9,5/27/2014,zip,,public,,Sequence S3O,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,,e5603e6a5867dc74d60b9198d64a8268f6cdeba0b7a98875d524c641f67ef3a7,,False
272,272,272,272,The CCSAD dataset,https://www.dropbox.com/s/gzeed4kj790irwx/S3P-20140705_003440.zip?dl=0,5.2,5/27/2014,zip,,public,,Sequence S3P,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,,363c6fc45ce3ced23dd580115a0de431a3738b78aa62428395b6b23ca04baa64,,False
273,273,273,273,The CCSAD dataset,https://www.dropbox.com/s/nm66meabt8xrr0f/S3Q-20140705_003929.zip?dl=0,7.2,5/27/2014,zip,,public,,Sequence S3Q,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,,0f2f43fb744e373f800384612491370b291893d947624706d1d2b88e7733d958,,False
274,274,274,274,The CCSAD dataset,https://www.dropbox.com/s/z9nif9jxpan7xai/S3R-20140705_011358.zip?dl=0,6.8,5/27/2014,zip,,public,,Sequence S3R,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,,11948b81989db9fc6116b5dcb657f8427fc49b654efc24efec8b8bab76e63d58,,False
275,275,275,275,MIT Age Labe dataset,https://www.dropbox.com/s/6h6bojpurnzus53/mit-agelab-sync-sample-car-dataset.zip?dl=1,2.0,1/1/2016,zip,MIT License,public,,initial,"contain the video for front, dash, and face videos,","supposedly MIT license, as it's directed by MIT AGE LAB",Automotive,"1,000+ hours of multi-sensor driving datasets",video,P1073,Self-driving cars,,,,,0dec06065d8738ae8e9708fc4d33c0b87c88b847e0b0c4c2a0f412edb43ae0fe,,False
276,276,276,276,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/DefinedTS.tar.gz,0.0013,1/1/2011,tar,proprietary,public,,initial,Defined Traffic Signs,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,defined traffic signs,image,P1074,Self-driving cars,,,,,b132e94190bb78c6ffb99fe1d2530938fa2606e0487009034b7232fd85c1e1ff,,True
277,277,277,277,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera00.tar,0.22,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,fcd3866054fb8fa8a354952bc3684511d7a3b391d26d43a33d6a5eabad74ddbc,,False
278,278,278,278,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera01.tar,0.98,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,c6c36fb26457b5c6804ca3c7091210df27ab4db1c187926bd83d8373d2d4b84d,,False
279,279,279,279,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera02.tar,0.36,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,1031fb33204cccd9c57a9a3bf780657873816eed2dac4f8acd0ed8d9e9062d30,,False
280,280,280,280,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera03.tar,0.28,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,8c96567366e8672ff471158eb0a98d6eef5cd39e08d1e66405c430838d875c7e,,False
281,281,281,281,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera04.tar,0.73,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,61ba0c4d3fac4c62d08219c9ebe8389017cfafc4727495630f89e44042e0e0a5,,False
282,282,282,282,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera05.tar,0.63,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,c9120e6db773a04fc5f4bf5dd07626f2dfbb0d2daa824f4061838e921564e873,,False
283,283,283,283,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera06.tar,0.18,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,f07d14be65df09243034d8459e242412e5bbe9020b3aa075171bf2944dd235c4,,False
284,284,284,284,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera07.tar,0.23,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,e9bcb282a2c500e52c5f1a3bb111c781a67482800cc0494882ac762db09fdd70,,False
285,285,285,285,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/annotations.tar,0.0017,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,b4e8d639342ea4569e36044224c3f626a5e993bcf9e7bc228584e7d2eeb93f3f,,True
286,286,286,286,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar00,1.0,1/1/2011,tar,,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,179d96f725594db0c442d2b97390961da662f9f58d32f89652cae60cbbc236d3,,False
287,287,287,287,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar01,1.0,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,bfac45339d55677e3aec7e9145c50687365ffe33e1532cee36792498e040297b,,False
288,288,288,288,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar02,1.0,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,8c5b3050053f162c0c940e0c5c1eac88915f5b81a06793e4dda8d7421d1dfbcb,,False
289,289,289,289,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar03,1.0,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,2c9b0e210ac694b4b53aff485742263166d7a207f75de53e2c345ccb6038d5fc,,False
290,290,290,290,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_00.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,fcd8b6308f983760161661b042c4735ff4d83e0fad92209ccc76e0c525396cb9,,False
291,291,291,291,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_01.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,db9849d4489d809b18e0636b7c172393957af09f46c8f257e6b34bbddb06484e,,False
292,292,292,292,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_02.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,1bdc4153e3069a1cd0190e6f5c7554a6aa6774fc1437b18bab79b77ed10f4a7c,,False
293,293,293,293,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_03.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,0bdf8e5744069346cace18dcc3aee0969d6fa3abb9abb0fd93dbd6af70cc1ec5,,False
294,294,294,294,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_04.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,08a6491b4e787c8dba4ad9cc175068c6dd758b54e2a81b3874ac8ca5232d9990,,False
295,295,295,295,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_05.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,45837753f997fd0de32275b6389bc026c20b5ceac143be7994539b1349ba5de3,,False
296,296,296,296,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_06.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,c9d4d82c81a71cb8c785299309afa8037db0a6eec204ee07490c581f7c9f11da,,False
297,297,297,297,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_07.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,61190310971c2ad5cb82400001e2db5dfe7a8b3a122a148a143404e52d548a75,,False
298,298,298,298,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_00.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,f12df22dd0b23c9cfbedfa2b13a28aad0c34ea9c235bd1e2096761fa1b3d3448,,False
299,299,299,299,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_00.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,53cea90895184364dc38105445de393163c1389a4556fe024bfe8b0f7193d11c,,False
300,300,300,300,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_01.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,e6d761d49dbc965609f8d2ed831a0c7a3a0fa8ad7908db8e1e736cba11bb1344,,False
301,301,301,301,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_01.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,b28f6897d8dd312a39e668d7872ac3f2c904a6d4c2c53a1214fe883dc258c8c0,,False
302,302,302,302,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_02.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,540a37b652b9c950971b1e97ba14163783d297ee75bb4bac21e731b292a87686,,False
303,303,303,303,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_02.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,a80ef8c05ef75fd965cb923e97bf6704b5909362e33ebfc4a4b007d34bffdab9,,False
304,304,304,304,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_03.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,c9f7b3f25d2b1e2098741a962a6c684ba0cad954bc89c4499fa675160d47bc6e,,False
305,305,305,305,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_03.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,e2688fcc775c3f303dedb3330337930059c2fb1d54d39789c8ffa878228d73cc,,False
306,306,306,306,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_04.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,cee78258012a81cf1fd9486086c6bba5f45782da925843dd01b16284b70dcf60,,False
307,307,307,307,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_04.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,2f8ca6608e48baa112a3268e5a25811b14d1ef5723a947257e3e29c992aa9dd3,,False
308,308,308,308,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_05.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,6f804c4c4c46f95803a0e3a16c95888faea24f91680cac347b18650204432633,,False
309,309,309,309,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_05.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,3163467c4f383af27b4d017bf86a556ff04003ae9b67454c83bbbb4a550ec0d3,,False
310,310,310,310,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_06.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,9547bd423bb311a9009e03142697d8d2092e4decbf13a55a9e3834090dca5fb2,,False
311,311,311,311,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_06.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,e394c2e4ecc9802a02b99e5084419831fe7d83dda28062cd41b1f0884bb6e73f,,False
312,312,312,312,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_07.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,6f92bafd0be3c7699c8e540c3a763f29d75a8b7c506e72ba1eb5da304023cdb6,,False
313,313,313,313,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_07.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,2c145b9e28e65c10220be957a693564b812728ae95649efaf236e232e7ba609d,,False
314,314,314,314,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_00.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,93c93f2c2a0c3babb15fbfb708e030fe9dbe8ee8edfc1354a9d321d9c5fb5367,,False
315,315,315,315,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_01.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,02223c6f5386ae49a60f2f875a28adf73f072a4098da0b6b7361a119de440dd1,,False
316,316,316,316,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_02.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,f84f175119fe25e6b41ffdac0175fe88b81567ba186f2f4f50340b910305672c,,False
317,317,317,317,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_03.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,6d0ca5d59b1ccc17937c1bef4cba5bc8a0622726653ecab84de8915ee1a65ffd,,False
318,318,318,318,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_04.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,2ea73b3b6e820dac9912312b577a0a8eca724f2499859baa97421dfd199915db,,False
319,319,319,319,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_05.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,1e1a6f58c2c25813dad853fcd4455dbb27dd30ac7a69d4e47eabd9bf10ce7e06,,False
320,320,320,320,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_06.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,51b165c10e239da72ae99b8e969691c1614bdc179e64a13d99771d9590c995fd,,False
321,321,321,321,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_07.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,f385ae19ab0122702dfe07bdfc5aa910403822a4b0afb2eca17fbfec819437aa,,False
322,322,322,322,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_00.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,fa6f74167dc762133991103313550759de73fe5c2de2693e11fc84114c074595,,False
323,323,323,323,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_00.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,961a41598f2d2ab239c2c3dddc42a61209eaef8b44e44f247a6a29abcafb023c,,False
324,324,324,324,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_01.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,f50bd1344c430b54c575909308e09c2c84e4c6e0e0f1929dedfea287f2d4fa89,,False
325,325,325,325,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_01.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,da9292d26c9ee85d8421dcd765c2001a7772dafe9eacc105580072defdba9698,,False
326,326,326,326,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_02.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,f5336cf7a3e31886768bf9b12c1854feb96e676d7e091ad7c8ceb3441409f6cc,,False
327,327,327,327,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_02.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,33c2d00e069fcb9c9f104a99c5e5082b810873f73f3e9a47df79a48811dc7819,,False
328,328,328,328,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_03.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,3607a3fdb8d90d38726fcd0c048e8cc635c51d5d1c7a0f31b4c578d8575ebdf8,,False
329,329,329,329,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_03.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,ef4795e309fb7e7949439154ebca345200445926d518b049009ec89f050454ee,,False
330,330,330,330,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_04.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,d41c84fbac6210ec3134e3c9080b70dcf6e7f3e73364f2bfd8954be7098e09cf,,False
331,331,331,331,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_04.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,e8b0d9328145d4d385ba873fe0a36056cf8dd1c6487c8b5a4a7d350aa49183b1,,False
332,332,332,332,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_05.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,da846d4ad3764408b64929da43528d4469ba38e8bcd50609ae4d3d8debff1e3d,,False
333,333,333,333,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_05.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,8ef0a02b0057d502e62664bd9638a490bb948e9149d2bde8d4d52c32f3e8ff0d,,False
334,334,334,334,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_06.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,adc12e7415dd3aab2f377ad40b57de29b3de51e2ec9b4b583113babcce52a88a,,False
335,335,335,335,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_06.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,6ff6b888cf3dd1f3347524f3e6a6dcf4f34343dc024db3bfa843eea0a8ff9cf6,,False
336,336,336,336,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_07.tar00,1.0,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,1f5a1a8f8164533cbcca34e344bfae43f5485264294b80657f4eae0a1410feac,,False
337,337,337,337,BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_07.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,,9bae3958a5a655b73e5c4e27c20ec92b1afff7feab4393025e3c8486d3c192f6,,False
338,338,338,338,hciLab driving dataset,https://www.hcilab.org/wp-content/uploads/hcilab_driving_dataset.zip,0.036000000000000004,10/30/2013,zip,Open Database License,public,,initial,Instance-level & Pixel-level labels,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,"road types, exits, on-ramps, workload",image,P1075,Self-driving cars,,,,,21eed4fd3af59d15d2ca4c3720a81955821e64f1fff879251e6a5e604a435f97,,False
339,339,339,339,Joint Attention for Autonomous Driving (JAAD) Dataset,data.nvision2.eecs.yorku.ca/JAAD_dataset/data/JAAD_clips.zip,3.1,3/6/2018,zip,proprietary,public,,4,"videos 1-60: GoPro Hero+ (1920x1980), videos 61-70: Highscreen Box Connect (1280x720), videos 71-346: Garmin GDR-35 (1920x1980)",Annotations for the dataset can be downloaded from our GitHub repo (https://github.com/ykotseruba/JAAD_dataset),Automotive,"pedestrians, behaviour",video,P1076,Self-driving cars,,,,,b710719f2c3478d5abfc1af57091c9308195832a09e944d45ec8ef4da3239394,,False
340,340,340,340,Predicting Driver Intent from Models of Naturalistic Driving in Intelligent Transportation Systems,its.acfr.usyd.edu.au/wordpress/wp-content/uploads/2014/05/20150423T032510_ivssg-2_EKF_ALL_GNSS.csv.gz,0.0092,1/1/2015,zip,,public,,initial,"This is a rich dataset derived from a vehicle driving around urban streets around the Australian Centre for Field Robotics in Sydney. The data were collected by a system fusing GPS and dead reckoning information from gyroscopes and odometry, at a resolution of 10 hertz.",,Automotive,"urban streets, Australia",GPS,P1077,Self-driving cars,,,,,cd076ec038b4720668356d851a96f1261e0df8e1d7e1c37c96d80ef0bdfbd950,,False
341,341,341,341,Predicting Driver Intent from Models of Naturalistic Driving in Intelligent Transportation Systems,its.acfr.usyd.edu.au/wordpress/wp-content/uploads/2014/05/20150414T041122_ivssg-2_EKF_All_GNSS.csv.gz,0.026,1/1/2015,zip,,public,,initial,"This is a rich dataset derived from a vehicle driving around urban streets around the Australian Centre for Field Robotics in Sydney. The data were collected by a system fusing GPS and dead reckoning information from gyroscopes and odometry, at a resolution of 10 hertz.",,Automotive,"urban streets, Australia",GPS,P1077,Self-driving cars,,,,,46ac0c589c5d01d512ec22e83c58fc30ac13e77974304bf88684eff0af0ca180,,False
342,342,342,342,Predicting Driver Intent from Models of Naturalistic Driving in Intelligent Transportation Systems,its.acfr.usyd.edu.au/wordpress/wp-content/uploads/2014/05/20150323T041228_ivssg-2_EKF_All_GNSS.csv.gz,0.02,1/1/2015,zip,,public,,initial,"This is a rich dataset derived from a vehicle driving around urban streets around the Australian Centre for Field Robotics in Sydney. The data were collected by a system fusing GPS and dead reckoning information from gyroscopes and odometry, at a resolution of 10 hertz.",,Automotive,"urban streets, Australia",GPS,P1077,Self-driving cars,,,,,321b1222234c1f1c1c516c45fda6d513110a8d7861600f2612ea7c25e143c6fe,,False
343,343,343,343,DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving,deepdriving.cs.princeton.edu/DeepDrivingCode_v2.zip,1.2,1/1/2015,zip,,public,,2,"Source Code and Setup File, map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving",,Automotive,"Convolutional Neural Network, direct perception based approach",image,P1078,Self-driving cars,,,,,63fc16f74b42a33b0a80742e39f2e64329e0f3ab3d447e6ec678f2dcac33a31e,,False
344,344,344,344,DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving,deepdriving.cs.princeton.edu/TORCS_trainset.zip,50.2,1/1/2015,zip,,public,,inital,"the images are in the integer data field, while the corresponding labels are in the float data field",,Automotive,"Convolutional Neural Network, direct perception based approach",image,P1078,Self-driving cars,,,,,1c0a714917c1bcdb47f0203e9a35d2550aca86ea4ae0734d5e1e3f20328b4b50,,False
345,345,345,345,DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving,deepdriving.cs.princeton.edu/TORCS_baseline_testset.zip,8.2,1/1/2015,zip,,public,,inital,A fixed testset to compare with the baselines,Our real testing is to let the ConvNet to drive the car in the game. So the testing images are generated on-the-fly,Automotive,"Convolutional Neural Network, direct perception based approach",GIST,P1078,Self-driving cars,,,,,6d01a627f7102c7c58c4a4afce867cc24f2fb427cf05fe8272b04e61c52da30b,,False
346,346,346,346,the comma.ai driving dataset,https://archive.org/download/comma-dataset/comma-dataset.zip,45.0,8/2/2016,zip,Creative Commons Attribution License,public,,inital,7 and a quarter hours of largely highway driving. Enough to train what we had in Bloomberg.,,Automotive,"speed, acceleration, steering angle, GPS coordinates, gyroscope angles",video,P1080,Self-driving cars,,,,,649122f9361d78dfac7d2f754b7ba7daa773691e774f02e7c52b4e5e717915c1,,False
347,347,347,347,1 iROADS Dataset (Intercity Roads and Adverse Driving Scenarios),https://www.cs.auckland.ac.nz/~m.rezaei/Publications/iROADS%20Dataset-Full%20Set.zip,1.5,v1,zip,,public,,initial,"Images: Daylight 903, Night 1050 , Rainy day 1049, Rainy night 431, Snowy 569, Sun strokes 307, Tunnel 347","Sequence Length (Frames): 4656, Colour / Grey Colour, Colour Depth: 24‐bit, Resolution (pixels): 640 x 360, Stereo Rectified: NA, Ego‐motion data: NA",Automotive,"various daytimes, different weather conditions, light, sun, rain, clouds",image,P1081,Self-driving cars,,,,,10dbf9a29cd54cbe2d1380b44bfe0fe8ef798536852a3f8960e69fd0d55f8445,,False
348,348,348,348,Places,http://places.csail.mit.edu/model/placesCNN_upgraded.tar.gz,0.234,v1,gzip,proprietary,public,,,"205 scene categories and 2.5 millions of images with a category label. Using convolutional neural network (CNN), we learn deep scene features for scene recognition tasks, and establish new state-of-the-art performances on scene-centric benchmarks.",,Electronics,Machine learning,image,p3141,Computer Vision,,,,,90bc2c5d40f90778d1578a176c923d5358ee10097a2a86cdb02333eb550b5140,,False
349,349,349,349,AN4,http://www.speech.cs.cmu.edu/databases/an4/an4_raw.bigendian.tar.gz,0.095,v1,gzip,proprietary,public,,,948 training and 130 test utterances,,Electronics,Machine learning,audio,p3142,Computer Vision,,,,,4f3483260042dcdf56c0e2f624125dd55aacb6001fcf0ef86d4ffe8a4a9fb01b,,False
350,350,350,350,BSDS images,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz,0.022,v1,tar,proprietary,public,,,"12,000 hand-labeled segmentations of 1,000 Corel dataset images from 30 human subjects. Half of the segmentations were obtained from presenting the subject with a color image; the other half from presenting a grayscale image. The public benchmark based on this data consists of all of the grayscale and color segmentations for 300 images. The images are divided into a training set of 200 images, and a test set of 100 images.",,Electronics,Machine learning,image,p3143,Computer Vision,,,,,86561d06f658e1f1897cbd0698594282d4e5ece62758a0dd3bb12246458d6076,,False
351,351,351,351,BSDS segmentations,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-human.tgz,0.098,v1,tar,proprietary,public,,,"12,000 hand-labeled segmentations of 1,000 Corel dataset images from 30 human subjects. Half of the segmentations were obtained from presenting the subject with a color image; the other half from presenting a grayscale image. The public benchmark based on this data consists of all of the grayscale and color segmentations for 300 images. The images are divided into a training set of 200 images, and a test set of 100 images.",,Electronics,Machine learning,text,p3143,Computer Vision,,,,,72762cb193eebb1a8031ac135286ea22deec39c1cc62770fce088cabde7dcdec,,False
352,352,352,352,CIFAR-10,https://www.cs.toronto.edu/~kriz/cifar.html,0.18600000000000005,v1,gzip,proprietary,public,,,"The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.",,Electronics,Machine learning,image,p3144,Computer Vision,,,,,a3da953bb2e5028244186dbf442130e6efa7e7342580b84781833094c345e4a6,,False
353,353,353,353,Oxford 102 Flowers 01,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz,0.363,v1,gzip,proprietary,public,,,This set contains images of flowers belonging to 102 different categories. The images were acquired by searching the web and taking pictures. There are a minimum of 40 images for each category.,images,Electronics,Machine learning,image,p3145,Computer Vision,,,,,fd1df169f037ea3bc82943e7cae8a3457ba8e38834e04150e42a8645b6292bb1,,False
354,354,354,354,Oxford 102 Flowers 02,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102segmentations.tgz,0.25,v1,gzip,proprietary,public,,,This set contains images of flowers belonging to 102 different categories. The images were acquired by searching the web and taking pictures. There are a minimum of 40 images for each category.,segmentations,Electronics,Machine learning,text,p3145,Computer Vision,,,,,332a36a996509f2042a39d8b8230689d12a88c7b053a2069fc096c0f3d9680a0,,False
355,355,355,355,Annotated Driving Dataset,bit.ly/udacity-annoations-crowdai,1.5,v1,tar,,public,,,"Mountain View California and neighboring cities during daylight conditions. 65,000 labels across 9,423 frames collected from a Point Grey research cameras running at full resolution of 1920x1200 at 2hz.",,automotive ,"Car, Truck, Pedestrian",text,p1083,,,,,,75a98139c09079873fe42daef9feeece6202e2093219797ce98af67594f8072f,,False
356,356,356,356,Annotated Driving Dataset,bit.ly/udacity-annotations-autti,3.3,v1,tar,,public,,," Mountain View California and neighboring cities during daylight conditions. 15,000 frames collected from a Point Grey research cameras running at full resolution of 1920x1200 at 2hz.",,automotive ,"Car, Truck, Pedestrian",text,p1084,,,,,,70ff95ad21bd5be89c9954bb4ba0e204e7e377adb82a22682cdb2d1ed431bef7,,False
357,357,357,357,Velodyne SLAM - Dataset,www.mrt.kit.edu/z/publ/download/velodyneslam/data/result_scenario1.zip,0.236,v1,zip,,public,,,"recorded with the Velodyne HDL64E-S2 scanner in the city of Karlsruhe, Germany",scenario 1,automotive ,,p3d,p1085,,,,,,eada246db37c8daa7e147c9a198578e2478a5b9b16d8a69344091813bed54f51,,False
358,358,358,358,Velodyne SLAM - Dataset,www.mrt.kit.edu/z/publ/download/velodyneslam/data/result_scenario2.zip,0.18,v1,zip,,public,,,"recorded with the Velodyne HDL64E-S2 scanner in the city of Karlsruhe, Germany",scenario 2,automotive ,,p3d,p1086,,,,,,3ee429adf60badec72943231d03b7a12166dd8e0a989deecfb878b00bf66c93c,,False
359,359,359,359,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-01.zip,0.883,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,31a9f0872364cf32f0145809194852ab8846e37ce3e18968439f1f06fb3b6b11,,False
360,360,360,360,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-02.zip,2.0,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,774878ba4fb64d27644cb95077710f6e378631a7003b22a8d402adeddc72fdcf,,False
361,361,361,361,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-03.zip,0.7959999999999999,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,3069f54347968eb0b0ae6f8455eeeba994de0fc7a1b84c0cbc6ea2a6e8858929,,False
362,362,362,362,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-04.zip,0.653,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,fdcf20a20bee674b112aaa22a89af36e58fb964f03ef1daaa2168ac995272fdc,,False
363,363,363,363,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-05.zip,4.4,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,c44421bc638b7c373f534f22ed495a9f0c292d971b25e677c38ddc82d7cfa839,,False
364,364,364,364,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-06.zip,4.4,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,ba7a5dea415aa7ff2339664af86232e5a3f23d59faa5306186fa22f898023696,,False
365,365,365,365,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-07.zip,2.2,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,1c1188f583071ae0f5326c50d3b3ed558b3a313238aa166c12eca6eed17a4379,,False
366,366,366,366,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-08.zip,10.0,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,20afac48c9518b003d6cd68fc0c31e7881cedab7ef4a161c6fc5206b4958b6b0,,False
367,367,367,367,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-09.zip,0.968,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,0c65ca0c9fd765937c5872b5f3a1d6ca60981f70682454ed141483cd55170869,,False
368,368,368,368,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-10.zip,17.0,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,139e69f0f4531107a37daefeb153a4f0a882523d06091e8c516ccaccc1d158a5,,False
369,369,369,369,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-11.zip,2.3,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,4de05235f7c284e141736e4e68077da8db04898aba4e7dbc3f0e2641818629bb,,False
370,370,370,370,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-12.zip,8.5,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,3b881e8115a66467716771cad475ee74607806759fa470604dfa54c359a17521,,False
371,371,371,371,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-13.zip,33.0,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,984a8ffbe4d6fea13e10cf56d0b52985526cf41f6b00505a9af2cabbd8cc2334,,False
372,372,372,372,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-14.zip,1.9,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,94262834b91ed719437d496897dfd0d8d38fe133fab6e8c116fe76acd643d808,,False
373,373,373,373,The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-15.zip,1.3,v3,zip,,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,,c7897bbeceaf19598b03813df1759e0a3c4eba57a3a568aacd8910f89ce62cee,,False
374,374,374,374,Dataset for Mobile Robotics Olfaction,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/DataSet_for_Odor_Classification.tar.gz/download,0.0107,v1,gzip,,,,,A collectionof 3 data-sets for Odor Classification with a Mobile Robot: Classification_DataSet_1 --> Controlled gas pulses. Classification_DataSet_2 --> Classification with different gas sensors (MCE-nose). Classification_DataSet_3 --> Classification in turbulent environments.,,Electronics,"eNose, gas, odor","text, .rawlog",P5002,Robotics,,,,,3610d2d5b96dbaf95490badaa1c019fe29f2634d4af4d8bb2e2d49551feb6d65,,False
375,375,375,375,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_360.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,80d5a0de3137edb48023984050e86874341b4d1eb1e37e489c7fd7d3190e996d,,False
376,376,376,376,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_desk2.tgz,0.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,570ac13ad3f4d3687d62ebd8c8890f67092c406042990bd61b50b99e56110d6d,,False
377,377,377,377,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_desk.tgz,0.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,33a4b8c38fb9d99b17fe595e105442b97d425d571b3fe1d7a432da5c4304b03c,,False
378,378,378,378,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_floor.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,8636147264c682ddb8682fc3d86a1d82b1de41a638c677229f2e5b9e11179bc5,,False
379,379,379,379,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_ir_calibration.tgz,0.6,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,d63ef6fc68f015cf72c2a3cf4a76d1e134a287eecc187ae12c5a6b573dbfdf93,,False
380,380,380,380,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_plant.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,51f07e7778f31f1cc0a0852cefd006b9434656866fdef61d7d3cf38e0c129b21,,False
381,381,381,381,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_rgb_calibration.tgz,0.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,86855bba7ac5706c21195b4668866458494df1edd8cbc397293ad436ac31ee74,,False
382,382,382,382,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_room.tgz,0.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,de3a4733cce04bbec176b3b75a2526b2a8c8aec85c3137a249e80a263d7f43fa,,False
383,383,383,383,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_rpy.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,be47225127548c2640d16d246c5c81cf37ece7ad2153ba305c07ce6c254b282c,,False
384,384,384,384,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_teddy.tgz,0.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,d75e6b2835ec07a9d48c1f0d54c45b3552730fb38eb6892400a02b2d510d95ec,,False
385,385,385,385,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_xyz.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,3cc321836916fe4ca6e3a8f318846e35c0f2bf5673c484289842a7ef7c19097d,,False
386,386,386,386,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_360_hemisphere.tgz,1.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,2f438dfca73b23e277b634005aeaa2be8312b058bd187de1d1112a939d36bc5c,,False
387,387,387,387,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_360_kidnap.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,7bea61b448ef16267dff8eba72db5eb800afed0b7416ebb7b5b510767f68fda3,,False
388,388,388,388,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_coke.tgz,1.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,45845465cd1ec45d689b52be40c4b041b9ef2a62c430937f369142c90b8c67aa,,False
389,389,389,389,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_desk.tgz,1.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,2941b011625f1480cd602b58c1d2cb21270080bcb7d3288d1cff9df5899dfa1b,,False
390,390,390,390,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_desk_with_person.tgz,2.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,ee7705f05b56fdbb2fba67beb740c96b6d80ea2bcbf3cc7104814099ef6b3e38,,False
391,391,391,391,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_dishes.tgz,1.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,3db491700968f21a766055a13417662cc50f745b49231586f4fe5a1bd63346fc,,False
392,392,392,392,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_flowerbouquet_brownbackground.tgz,1.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,82856f926929743880c28a639af69e48a7a2752b75fa172050732bc924aeb9c7,,False
393,393,393,393,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_flowerbouquet.tgz,1.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,4e1a45cd861b1985c30bba7041235de51100baa8baa00dd631824e7ef950452f,,False
394,394,394,394,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_ir_calibration.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,b15af922a5f6e51315a191122ea3d91ea29731535bc76519dd1246d71baa993a,,False
395,395,395,395,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_large_checkerboard_calibration.tgz,1.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,a481bf1bb9aa8858df071c910c520174887e3421b3f5dfcbbe9e03328f44e802,,False
396,396,396,396,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_large_no_loop.tgz,1.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,d9ddedadf0002db8f935658eae02bbf59cf612d9cee56f8db2c34b83107ea590,,False
397,397,397,397,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_large_with_loop.tgz,2.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,e1abf24bd2d68b1395721936fdf32adccea3e5e51ce56e9344d0f2f94978c737,,False
398,398,398,398,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_metallic_sphere2.tgz,1.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,36c10318976fa0fc89705690964ad9b4d0f6124769432b9765d12a95ea26dd4a,,False
399,399,399,399,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_metallic_sphere.tgz,1.0,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,e56eedde26d99db3ab51984b7c4f657494e5bbb05c939004b843ba992f487261,,False
400,400,400,400,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_360.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,30dd68eeb1bc5023265f458322654620529d80a385a4f96de491d15c53648410,,False
401,401,401,401,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_slam2.tgz,1.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,c68d75c3dc580d206be4ddff3bbd16effae89ee6eb9483cfe337a4da0db23732,,False
402,402,402,402,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_slam3.tgz,0.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,767ad9948720359271a4623c91394e9caa73b4ea3f98af167d336c583f977a40,,False
403,403,403,403,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_slam.tgz,1.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,18bb624001f7ec6b09ff6b7e40c8889890028ac02ee370c10cdb7aecd80f45a0,,False
404,404,404,404,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_rgb_calibration.tgz,1.1,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,f3cb218ab8ff550b0a9d734e793f9312691e0eb39c51717c3bb15461fdd034cd,,False
405,405,405,405,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_rpy.tgz,2.0,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,68115d8abc1b53b486af58fed9425923e3ef58e6cc7ecb1085c1606b07e31dac,,False
406,406,406,406,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_teddy.tgz,1.1,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,680d32b4de3b63c2ff0a97c67895bde5ddaf07b41e85a2f9e42e7393cfd532e3,,False
407,407,407,407,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_xyz.tgz,2.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,9553b6a1782a808e1843aebc726bd69b8ac1edc32a400c9e051b21476c6551da,,False
408,408,408,408,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_texture_near.tgz,0.6,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,1d4dbd9270f3c8af442c0590511e9a24c0a6fa6fc9f52f991d80f6e2d60f115b,,False
409,409,409,409,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_texture_far.tgz,0.6,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,789284d596141c997dffc1494eb1b75df9d5ccee208f786b8b8692e604d77719,,False
410,410,410,410,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_notexture_near.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,8845d93296f5e8fabcb65b19a52bbaffc3280de7ec50c52af4114a2fbc01255c,,False
411,411,411,411,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_notexture_far.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,9eef96b5df00da139f042e48318ad01b3aaa71583baf3c36f205d05faada5719,,False
412,412,412,412,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_nostructure_texture_far.tgz,0.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,5e9a468a67694f46c53893b8e00860b617c861988ae96492ec6fa04eb75e18c6,,False
413,413,413,413,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_nostructure_notexture_far.tgz,0.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,b997e3ff7f87fbb698280cd1b2ce3f3b3ac3587600cb988b618c32ad74d662a3,,False
414,414,414,414,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_walking_xyz.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,f9397f8462a60007b8c36085558512599534ae68a8dbd9c3359c3bfa44abd35c,,False
415,415,415,415,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_walking_static.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,afeeca983a69d7b82a4ad66bb9b075afe7be557407119c0ab71bdca034e2d94a,,False
416,416,416,416,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_sitting_halfsphere.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,593ba92fcf12e7d886798fb4feab7049cb084a2ec55e7c34dfdde7967b9cce85,,False
417,417,417,417,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_sitting_xyz.tgz,0.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,27aba055307f7f650f1e831e26b8b30c18e1bc3fa1eb86b1179aee683992adb6,,False
418,418,418,418,Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_sitting_static.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,,3254a723c2937b0d8686dd3cd10e90ba82de172cce2cd43e73cb4eb30c008602,,False
419,419,419,419,Málaga dataset 2009 – Campus 0L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_parking_2L.zip/download,2.1,v1,zip,,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5004,Robotics,,,,,7820da70655914a7a3c7c9a45da45df0b86128901b3bcf2273486c2d55fb7b28,,False
420,420,420,420,Málaga dataset 2009 – Campus 2L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_campus_2L.zip/download,3.6,v1,zip,,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5005,Robotics,,,,,557895d399314e8866f784dda865095343f0d6c755a52a813a4e58396eb986a0,,False
421,421,421,421,Málaga dataset 2009 – Campus RT,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_campus_RT.zip/download,1.9,v1,zip,,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5006,Robotics,,,,,45aa5f5fde93755a2b22eeed2aeb1e5c09dc2822f86edcd64bb22747012b5051,,False
422,422,422,422,Málaga dataset 2009 – Parking 0L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_parking_0L.zip/download,1.3,v1,zip,,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5007,Robotics,,,,,9d4c32d1879b1d83ebb6c3c3c540ac7209e255aa4fb2924ee4e5b83bd7666b78,,False
423,423,423,423,Málaga dataset 2009 – Parking 6L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_parking_6L.zip/download,2.3,v1,zip,,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5009,Robotics,,,,,0e3d2b08999d7c0695db95120af8a6c417d7352f7951ab7c1b41e2093138fa34,,False
424,424,424,424,Málaga 2006 campus dataset,http://downloads.sourceforge.net/mrpt/dataset_malaga20060121.tgz,0.0046,v1,tgz,,,,,"Most successful works in Simultaneous Localization and Mapping (SLAM) aim to build a metric map under a probabilistic viewpoint employing Bayesian filtering techniques. This work introduces a new hybrid metrictopological approach, where the aim is to reconstruct the path of the robot in a hybrid continuous-discrete state space which naturally combines metric and topological maps. Our fundamental contributions are: (i) the estimation of the topological path, an improvement similar to that of RaoBlackwellized Particle Filters (RBPF) and FastSLAM in the field of metric map building; and (ii) the application of grounded methods to the abstraction of topology (including loop closure) from raw sensor readings. It is remarkable that our approach could be still represented as a Bayesian inference problem, becoming an extension of purely metric SLAM.","Odometry, SICK LMS",Travel and transportation,"SLAM, RBPF, FastSLAM, maps, mobile robots","image, .rawlog",P5010,Robotics,,,,,ca35e5cf7f5fbdc6d15390320fc23f19bada8a0d1e611e40ca728ca9dad60d3d,,False
425,425,425,425,3 horizontal lasers,http://downloads.sourceforge.net/mrpt/dataset_threelasers_20070517.tgz,0.00056,v1,tgz,,,,,"Sensors: Odometry, SICK LMS, 2x Hokuyo LMS",Robot: SENA PATH LENGTH: 20m RAWLOG ENTRIES: 505,Travel and transportation,"Odometry, SICK LMS, 2x Hokuyo LMS","image, .rawlog",P5011,Robotics,,,,,92a5eccb6dc7c2bf8e785d8362ae38fde9cadceaaa80bfdfee3f6ac086959365,,False
426,426,426,426,"With eNose, at Malaga Office 2.2.30",http://downloads.sourceforge.net/mrpt/dataset_laser_eNoses_20061218.tgz,0.0013,v1,tgz,,,,,"Odometry, SICK LMS, Hokuyo, 2x eNoses",PATH LENGTH: 21m RAWLOG ENTRIES: 1768.,Electronics,"eNose, SICK, LMS, odometry","image, .rawlog",P5012,Robotics,,,,,044763891f4e0a2a81465cc3e37fc106b340786c2a122c9e6b4da39c96ee12b1,,False
427,427,427,427,"Málaga, corridor 2.3",http://downloads.sourceforge.net/mrpt/dataset_malaga_floor2.3_2lasers_stereo.tgz,0.086,v1,tgz,,,,,"Odometry, SICK LMS, Hokuyo, Stereo",PATH LENGTH: 175m. RAWLOG ENTRIES: 3768.,Electronics,"Odometry, SICK LMS, Hokuyo, Stereo","image, .rawlog",P5013,Robotics,,,,,84bba8b717844f18212cb7f916cf518a05cbdfa2b21aeb8a9fb6f887a65cd634,,False
428,428,428,428,"Málaga, corridor 2.3 (vertical laser)",http://downloads.sourceforge.net/mrpt/dataset_malaga_corridor2.3_vertical_laser_20060120.tgz,0.01,v1,tgz,,,,,"Odometry, SICK LMS (horz), Hokuyo (vert.)",PATH LENGTH: 385m. RAWLOG ENTRIES: 2190.,Electronics,"Odometry, SICK LMS, Hokuyo, Stereo","image, .rawlog",P5014,Robotics,,,,,f735da20ff3bb9d29ea9696f3377e21b37fe6591391846cb1405e98cc79d4947,,False
429,429,429,429,Long walk in corridor 2.3,http://downloads.sourceforge.net/mrpt/dataset_malaga_large_floor2.3_1laser_20070517.tgz,0.0021,v1,tgz,,,,,"Odometry, SICK LMS, Hokuyo",PATH LENGTH: 290m. RAWLOG ENTRIES: 2444.,Electronics,"Odometry, SICK LMS, Hokuyo, Stereo","image, .rawlog",P5015,Robotics,,,,,cd70df61ca468121726e83b07ef9176bd49331fd030ca3e4aa93fa4b01c652e1,,False
430,430,430,430,Kenmore,http://downloads.sourceforge.net/mrpt/dataset_kenmore_pradoroof_20061120.tar.gz,0.0925,v1,gzip,,,,,"Robot/Vehicle: A Toyota Prado with two SICK LMS on its roof. PATH LENGTH: 18 Km approx. RAWLOG ENTRIES: 262850. See also the next rawlog for artificially added odometry from scan matching. AUTHORS: Michael Bosse, from Australia's Commonwealth Scientific and Industrial Research Organisation (CSIRO)","2xSICK LMS; Suburban streets in Kenmore, QLD, Australia (2006-NOV-20).",Travel and transportation,"SICK LMS, Rawlog","image, .rawlog",P5016,Robotics,,,,,673104d22473583303dff4e4a285db60dde98b272da3a8870edc5fb0f0da6e93,,False
431,431,431,431,Kenmore (decimated + estimated odometry),http://downloads.sourceforge.net/mrpt/dataset_kenmore_pradoroof_20061120_decimated+odometry.tgz,0.0103,v1,tgz,,,,,"Robot/Vehicle: A Toyota Prado with two SICK LMS on its roof. PATH LENGTH: 18 Km approx. RAWLOG ENTRIES: 26283. AUTHORS OF THE ORIGINAL DATASET: Michael Bosse, from Australia's Commonwealth Scientific and Industrial Research Organisation (CSIRO).","2xSICK LMS; Suburban streets in Kenmore, QLD, Australia (2006-NOV-20).",Travel and transportation,"SICK LMS, Rawlog","image, .rawlog",P5017,Robotics,,,,,bbfb7777b67f17f907ccc93a8155436e14d5a7c84eda092fa928f8c7afb7c913,,False
432,432,432,432,Edmonton 2002,http://downloads.sourceforge.net/mrpt/dataset_edmonton_20020728.tar.gz,0.002,v1,gzip,,,,,"Odometry, SICK LMS","PATH LENGTH: 327m approx. RAWLOG ENTRIES: 6010. AUTHORS: Nick Roy, MIT",Travel and transportation,"SICK LMS, Rawlog, Odometry","image, .rawlog",P5018,Robotics,,,,,7d46511948b506ca2c11e7c68d1cc29d91b3bfa42daef870339284604103475a,,False
433,433,433,433,The Victoria Park,http://downloads.sourceforge.net/mrpt/dataset_victoria_park.tar.gz,0.002,v1,gzip,,,,,"Odometry, SICK LMS, GPS","PATH LENGTH: 4Km approx. RAWLOG ENTRIES: 4832. AUTHORS: Dr. Jose Guivant, from the University of Sydney",Travel and transportation,"SICK LMS, Rawlog, GPS","image, .rawlog",P5019,Robotics,,,,,ed2c3cac48564856d88609f62b288f4660555e3b140b9d0cdd3811e6f4e17cf2,,False
434,434,434,434,A. Davison’s Monoslam test video,http://downloads.sourceforge.net/mrpt/dataset_monoslam_davison.tar.gz,0.032,v1,gzip,,,,,"Monoslam test video recorded with camera in an indoor, office scenario","PATH LENGTH: A few meters. RAWLOG ENTRIES: 1000. AUTHORS: Dr. Andrew Davison, from the Imperial College London",Electronics,"Monoslam, Camera, Office, Rawlog","image, .rawlog",P5020,Robotics,,,,,d77330264577c2f340015c3c2ffb295d01ccc7393870ac6f31d74bc4c1cc2ab9,,False
435,435,435,435,Intel (2003),http://downloads.sourceforge.net/mrpt/dataset_intel.tar.gz,0.0018,v1,gzip,,,,,Interior of the Intel Research Lab in Seattle (2003),"PATH LENGTH: 506m approx. RAWLOG ENTRIES: 5452. AUTHORS: Dieter Fox, University of Washington",Travel and transportation,"Odometry, SICK LMS","image, .rawlog",P5021,Robotics,,,,,c0e594859932a65650e3e4744ec2a414b526dc1f4e38fc67a10e2635a7ef3e2a,,False
436,436,436,436,fr079,https://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/dataset_fr079.tar.gz/download,0.0068,v1,gzip,,,,,"This dataset was recorded by Cyrill Stachniss in Building 079 at the University of Freiburg. This is a converted version from CARMEN logs, which were downloaded from: - D. Holz & S. Behnke: http://www.ais.uni-bonn.de/~holz/spmicp/ - G.D. Tipaldi : http://www.openslam.org/flirtlib.html Note: This dataset package includes an already built simplemap and gridmap (as binary object and as image), and two rawlogs with original and with ""corrected"" odometry.",,Travel and transportation,"Odometry, SICK LMS","image, .rawlog",P5022,Robotics,,,,,4f2a41265e063c57d4b95e7e349a50bf17f746e300a5bab746a919eada5c8e03,,False
437,437,437,437,Bike Video Dataset,https://storage.googleapis.com/brain-robotics-data/bike/BikeVideoDataset.tar,32.0,v1,tar,Creative Commons Attribution License,,,,"The Bike Video Dataset is created by recording videos using a hand-held phone camera while riding a bicycle. This particular camera offers no stabilization. The image frame rate is 30fps, with a resolution of 720×1280.",Google Brain Robotics Data,Electronics,"Google, video, brain robotics",Video,P5033,Robotics,,,,,9a024b49f2ea94125baca46754be0557d7ff159a7f1225bf42f868dc0242f572,,False
438,438,438,438,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_01.zip,1.3,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_01, Frames: 4757, Duration: 95.13s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,af60647a5ca23c62fd97b414d75a6ab665dde0c849a4002e258353cc012178b5,,False
439,439,439,439,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_02.zip,0.88,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_02, Frames: 3500, Duration: 69.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,c612bd0a09754a68533b0509730b57e210bf28efccae2b6d0db2dbda9b61b071,,False
440,440,440,440,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_03.zip,1.51,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_03, Frames: 5427, Duration: 108.55s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,8dfa1499fdfb1acd2f18aa2c4affc166e95916948e9c74dc2e28434514fb05cf,,False
441,441,441,441,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_04.zip,1.83,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_04, Frames: 6921, Duration: 138.47s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,c189d017a84b584304c784b21ff9005c26ab8ed8951e0af53041165922aa78fd,,False
442,442,442,442,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_05.zip,2.01,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_05, Frames: 6300, Duration: 125.97s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,d1f38c8afeaec131af53797a20d557c3cb82637a7754ef152b2f093187e00bd6,,False
443,443,443,443,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_06.zip,1.21,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_06, Frames: 4500, Duration: 90.04s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,2e3766c5920dcf2cca05cfc483f6227c3f0f62e73aea3891797e19dcf778ffb6,,False
444,444,444,444,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_07.zip,0.94,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_07, Frames: 3556, Duration: 71.10s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,57e839ba5556647c47458c4ab09c79a14e55087167607f24cef9f3196f935e56,,False
445,445,445,445,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_08.zip,1.01,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_08, Frames: 4300, Duration: 86.08s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,5d416b99e29488872105e6ae105f7c0e6c6ec28c44248229b29f2306ff36647c,,False
446,446,446,446,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_09.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_09, Frames: 2300, Duration: 46.00s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,47d03eb5da400375fb5b3f991427b0b620a9314b99e6d792ab198d2188ca1a73,,False
447,447,447,447,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_10.zip,0.36,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_10, Frames: 2100, Duration: 41.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,93f64bf403073e578b931c65b64c431ad1c84c738785eb786f8845f6902e95c3,,False
448,448,448,448,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_11.zip,0.25,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_11, Frames: 1500, Duration: 59.95s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,1dabbccf20adbda1291e8dd8ca66765c66fb05dcb28dcd76d77b9b403276ec93,,False
449,449,449,449,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_12.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_12, Frames: 2250, Duration: 89.99s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,c02c1bd21a453265d235c37f872c0ce2c8e334c4a092ba4456bd1e99132d4a9f,,False
450,450,450,450,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_13.zip,0.28,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_13, Frames: 1800, Duration: 71.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,949e93330184bc3860ef44fa64c992fcac42b7f5368066f506652e4b8bf61c67,,False
451,451,451,451,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_14.zip,0.23,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_14, Frames: 1550, Duration: 61.93s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,77ce19a1dab655d1c20f94e9ab4a4e24056d10a6a09730e54c41e8e51587dc1e,,False
452,452,452,452,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_15.zip,0.5,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_15, Frames: 2700, Duration: 107.91s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,a52ec74042bebc6a3d2980038d05b931aca3ce34a7e4e9cc280ebee07edc834b,,False
453,453,453,453,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_16.zip,0.3,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_16, Frames: 1850, Duration: 73.93s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,ebb1a846f3db6ce30bb6fdc91b857838723af76cba4df9221804b39467f55683,,False
454,454,454,454,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_17.zip,0.99,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_17, Frames: 4980, Duration: 124.39s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,28afba1147c6077655def6c64122a047df880f5b118c76c49d467a3eb48a4787,,False
455,455,455,455,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_18.zip,1.12,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_18, Frames: 6200, Duration: 154.94s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,b815b408579478e8c249cd90541682f3d8aa04eb46b489b76007fc5d8bb0a93c,,False
456,456,456,456,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_19.zip,1.65,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_19, Frames: 8380, Duration: 209.43s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,c3c24b2d54c5e3a4500dead156513438aad1b7f0b7cd1712acf14442708334ef,,False
457,457,457,457,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_20.zip,1.1,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_20, Frames: 5380, Duration: 134.45s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,a13b43cc7940cac3185c6d0841dccaf47d38eb834ad6f6b47a15a4c0a8ab3a29,,False
458,458,458,458,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_21.zip,1.47,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_21, Frames: 5470, Duration: 273.68s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,c2eebc21d78ceeee7e3c3185c45db02f34021e9112e7d0d0dc3a390587bb7719,,False
459,459,459,459,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_22.zip,1.79,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_22, Frames: 6340, Duration: 316.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,9edf69eb41f9ef13e9107c957508ae5064c57d717b1415cf1118887993f757e1,,False
460,460,460,460,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_23.zip,0.89,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_23, Frames: 3740, Duration: 124.64s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,05c9b60059147edbcf6c491e0f85c99512f99c2aef1adb0e9368fe4bbe01a9d0,,False
461,461,461,461,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_24.zip,0.77,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_24, Frames: 3500, Duration: 116.64s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,98adf0ec66f0f2352d87a790821e244cf4fc9230a18f8da9a32c857a9632eab2,,False
462,462,462,462,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_25.zip,0.96,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_25, Frames: 4090, Duration: 136.31s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,ba978b08fe4474ee195a0c02f3c50d8aed6be212752229a4d6d5e3bfc1ca9684,,False
463,463,463,463,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_26.zip,0.36,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_26, Frames: 2760, Duration: 91.95s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,005b85eb662d624fa480ef4e4b3f887d5ed839f8c62d1a3572756951482eeb2e,,False
464,464,464,464,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_27.zip,0.69,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_27, Frames: 3480, Duration: 115.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,fc525ca42ff6ad4f32189d35cfb12787df607c21f5a09993952d416a1c2e459a,,False
465,465,465,465,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_28.zip,0.7,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_28, Frames: 5550, Duration: 185.31s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,024a494de3d28def410a6261ac8660443a13c4f1cfba693e771d728145241c81,,False
466,466,466,466,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_29.zip,2.3,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_29, Frames: 8400, Duration: 280.03s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,d2f6657c5fabe52f7206b8a6184fcca80842fe3c8ee4ced37f60ec6f12544b19,,False
467,467,467,467,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_30.zip,0.45,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_30, Frames: 1800, Duration: 85.30s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,619abf1e56995cbbbf888edbf853584b7af72dc5e86b32f7835fdb9401e569bc,,False
468,468,468,468,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_31.zip,0.85,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_31, Frames: 3240, Duration: 153.59s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,8f3f54243275509b06956a2f056846bc962585054a5d2d8363c277892768101c,,False
469,469,469,469,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_32.zip,0.72,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_32, Frames: 2700, Duration: 127.99s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,ec9cfb6ddd24266cd5453a2483e8736b4a7170f6291f8071332b6162074dd441,,False
470,470,470,470,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_33.zip,0.68,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_33, Frames: 2760, Duration: 91.92s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,1e543ca393ccd42b287d04f5a36dfabaf0f959cf6821ddec043f9a5aa7e12c61,,False
471,471,471,471,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_34.zip,1.12,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_34, Frames: 4290, Duration: 203.38s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,f780c4d759d3c755dd285a0036dafd0b4243dc6209d8de638d4756fc8f07576f,,False
472,472,472,472,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_35.zip,0.36,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_35, Frames: 2550, Duration: 85.06s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,e2ffbbf5cff2ff2b7da593effaffa2d3a90b6a6a9e9139a688b46e425ed5fb99,,False
473,473,473,473,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_36.zip,0.32,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_36, Frames: 2350, Duration: 78.29s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,a67a3964b45e4be2bfacadeb5dc1a864577931909e47bb08ccc3524f678aa362,,False
474,474,474,474,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_37.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_37, Frames: 2970, Duration: 98.96s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,1ee30d1ea4016bed1e06ce546aa8f0b39bc0ab4a486b0e847778b446dc10d286,,False
475,475,475,475,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_38.zip,0.37,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_38, Frames: 3330, Duration: 133.17s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,2edc7a96ede150fd27e91b2d90b3c3e9d536f5779a1060fadb6a4fb609f875ca,,False
476,476,476,476,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_39.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_39, Frames: 3540, Duration: 141.65s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,798c39b8a7831c1f193a5250f9b4db0b70ca416673a5b2a246198f728bb84a98,,False
477,477,477,477,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_40.zip,0.44,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_40, Frames: 4350, Duration: 174.42s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,a9225641c3175d6f282c0696b2b63a2a86a14256bfae3bcbd4b58cc2c3d320ef,,False
478,478,478,478,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_41.zip,0.42,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_41, Frames: 3100, Duration: 123.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,113f6f04c94f13aa9b513aff9748e5481ae1c467304493a3b6d08b5c311cea37,,False
479,479,479,479,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_42.zip,1.14,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_42, Frames: 4830, Duration: 224.49s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,3638f6202b4e99739c9fac300536d0d4b28ab5c7a6aeda831315c29f15e5efb6,,False
480,480,480,480,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_43.zip,0.66,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_43, Frames: 2160, Duration: 100.28s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,f824b09a30c62678c9cbe5ab469e62dd7173c1318cbe8a9517d148137a931aa9,,False
481,481,481,481,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_44.zip,0.45,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_44, Frames: 2100, Duration: 97.50s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,abec9989ac270d64f81e78778cd91c26cf5ee609dfd64e0371044e08f60532e0,,False
482,482,482,482,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_45.zip,0.93,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_45, Frames: 3000, Duration: 99.99s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,46aa2b9190bfea89b6d8b3f86e23e3738b8abf6224cf365f2792457bdf12c2ef,,False
483,483,483,483,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_46.zip,1.02,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_46, Frames: 4110, Duration: 137.07s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,8c34def190a5671775e97a2d7f87e820e118e804192c2f8a1e4a264df0eaa487,,False
484,484,484,484,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_47.zip,0.99,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_47, Frames: 3260, Duration: 129.84s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,4961bc56e4b1b2941cfbd6150e87f517818e8f413798ce2033daa8282a57f4b2,,False
485,485,485,485,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_48.zip,0.96,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_48, Frames: 3250, Duration: 129.41s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,cf1359d7ab12a184f5320945324807c2785fc679f5695c125a5d5dd4464e9e92,,False
486,486,486,486,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_49.zip,0.82,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_49, Frames: 3255, Duration: 129.48s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,d858240b9ca2aaf1d31fcbb3c8031852b0c4c45958b5d27f27ec96e6a2c2b393,,False
487,487,487,487,Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_50.zip,1.12,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_50, Frames: 4050, Duration: 161.12s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,,e3f8b1e8ab2204bc90261ffa617e3f43905ba3648410b81d15b54e1224ee00c8,,False
488,488,488,488,data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/dem.tif.gz,0.574,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Digital Elevation Map ,Tagged Image File ,P1087,,,,,,d4096433da6cd01cf4c5246a19605ebe016a59d5add8c7ed89ef75a8c977be91,,False
489,489,489,489,data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/ir-raw.tif.gz,0.0017,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Infrared Raw Color,Tagged Image File ,P1087,,,,,,18004197e18083ac18115395901e483bc0787d4d385a1482c8b435a7872f314b,,False
490,490,490,490,data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/ir-falsecolor.tif.gz,0.0012,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Infrared False Color ,Tagged Image File ,P1087,,,,,,4b0d9e18a764f913fe99ab001a926d6c372a5e906c497742d271bf8c78fd7aad,,False
491,491,491,491,data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/optical.tif.gz,0.211,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Optical ,Tagged Image File ,P1087,,,,,,b2cd5e91def0f0f66fbb1d93ce7fea182121e2257cbdcfc93001af2fd965b648,,False
492,492,492,492,data selfdriving cars,data.selfracingcars.com/thunderhill/velodyne/velodyne.tar.gz,1.2,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Manuals and Application Notes,Tagged Image File ,P1087,,,,,,fb3f4bc0e843085387771faa660a87053a0a1c211fd77ff9944b7d4d3ca2fd26,,False
493,493,493,493,data selfdriving cars,data.selfracingcars.com/thunderhill/fcs_xsens/fcs_xsens_data.tar.gz,0.35600000000000004,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Fairchild and Xsens Data Set,google docs ,P1087,,,,,,efdce7d2803544c37b9d109e7d9d7b2e9a379e2f287508ee21be4f9fb54960f7,,False
494,494,494,494,CVRR-HANDS 3D,cvrr.ucsd.edu/LISA/Datasets/hands.zip,2.0,01/01/2013,zip,,Public,,,"The CVRR-HANDS 3D dataset was designed in order to study natural human activity under difﬁcult settings of cluttered background, volatile illumination, and frequent occlusion. "," usage for educational, research and non-profit purposes, without fee",Automotive,"Hand localization, hand and objects localization, and 19 hand gestures for occupant-vehicle interaction",Image,P1088,,,,,,e28e950cc72ad4b4e18daa644a22d63c877ca94cfc5974c218c4c6b876c14cc8,,False
495,495,495,495,2010 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/vedg-c5sb/rows.csv?accessType=DOWNLOAD,4.5e-05,8/21/2011,csv,,public,,,"This dataset includes employee information, salary, and position specifics. ",,Government,"salary, whitehouse",text,p3146,,,,,,9a7056849543595bb3f7c3826fc7bccd5d87a33e722fee244c8903422ccea9a6,,False
496,496,496,496,VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_dayTrain.zip,11.6,1/1/2015,zip,,public,,,Day Train Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,,52599e2171a79d1687cad732d0955c9e7e29f73d1b1b7d0680aaea06af5f76d8,,False
497,497,497,497,The White House - Nominations & Appointments,https://opendata.socrata.com/api/views/n5m4-mism/rows.csv?accessType=DOWNLOAD,0.000259,12/29/2015,csv,,public,,,"This dataset includes information about candidate name, position, agency, nomination date, and confirmation vote. ",,Government,"vote, nomination",text,p3147,,,,,,b481e5e96dbe81fa289ab54f672b68199265539dc440ed6828bd53c61ef144e9,,False
498,498,498,498,2011 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/73t8-rw4g/rows.csv?accessType=DOWNLOAD,4e-05,08/05/2012,csv,,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees.",,Government,"salary, whitehouse",text,p3148,,,,,,c39919e789ed05d47cc5bd547c36b0ed11a1d90cf203907fd7ef95c42ef049ae,,False
499,499,499,499,Milk RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/pkfj-5jsd/rows.csv?accessType=DOWNLOAD,1.1e-05,10/23/2011,csv,,public,,,"Dataset provided by the Environmental Protection Agency, contains milk testing information about radiation.",,Life Science,,text,p3149,,,,,,63f16b83ba5d54590f72af27d998af0e042eafac52b9e8bc6f55c8bdc9fd3205,,False
500,500,500,500,VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_nightTrain.zip,0.8,1/1/2015,zip,,public,,,Night Train Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,,9bf7040a05e4b6a9390e327cd58934e973cd74da44c0c714b3f1529c85bc7265,,False
501,501,501,501,VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_dayTest.zip,3.8,1/1/2015,zip,,public,,,Day Test Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,,05e302ece3ced4cc3b9ccc4c9409499288bc8b6cc93052766d1974e26852a259,,False
502,502,502,502,VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_nightTest.zip,2.6,1/1/2015,zip,,public,,,Night Test Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,,714af72aa247121520c857c3f3b3ae405269620cbbdd86aa1e53dc58473c0f1a,,False
503,503,503,503,Sorted RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/w9fb-tgv6/rows.csv?accessType=DOWNLOAD,0.0001,7/23/2013,csv,,public,,,Dataset provided by the US Environmental Protection Agency. Includes information about a RadNet labratory analysis. ,,Life Science,"radiation, lab test",text,p3150,,,,,,cb9debbf1ec03061d4b84920674e608aa0ce1c2fe793866a2de32d7e2157d914,,False
504,504,504,504,Precipitation RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/e2xy-undq/rows.csv?accessType=DOWNLOAD,3e-05,08/21/2011,csv,,public,,,Dataset provided by the US Environmental Protection Agency. Includes information about a Precipitation RadNet labratory analysis. ,,Life Science,"radiation, lab test",text,p3151,,,,,,d67a0a5f56034de7bcf1985fd67ddd3b15574def4f0edf3dbf2fa1de0c9c6568,,False
505,505,505,505,2012 Annual Report to Congress on White House Staff,https://opendata.socrata.com/api/views/jv7a-cjdv/rows.csv?accessType=DOWNLOAD,4e-05,6/29/2012,csv,,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees.",,Government,"salary, whitehouse",text,p3152,,,,,,ffd6f4aa8a99c391145a116a340b40fc924c03b8584ef059574c2dfbf99202e5,,False
506,506,506,506,2013 Salaries: Pennsylvania State System of Higher Education,https://opendata.socrata.com/api/views/26jq-uk2i/rows.csv?accessType=DOWNLOAD,0.0008,3/19/2013,csv,,public,,,Pennsylvania State System of Higher Education salaries for 2013.,,Government,"education, salary",text,p3153,education,,,,,5cf6bd6d91e7445ddde36afea9cdb7e594cb7bc17b25124dff0854741d649a3e,,False
507,507,507,507,Drinking Water RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/4ig7-9eqd/rows.csv?accessType=DOWNLOAD,3e-05,08/21/2011,csv,,public,,,Dataset provided by the US Environmental Protection Agency. Includes information about a drinking water RadNet labratory analysis. ,,Life Science,"water, lab test, radiation",text,p3154,,,,,,57d5e380639a45614292dde5c500f9c87404be032448fa27c2b651f079a5592f,,False
508,508,508,508,2013 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/44xn-rs2p/rows.csv?accessType=DOWNLOAD,5e-05,6/28/2013,csv,,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees. ",,Government,"salary, whitehouse",text,p3155,,,,,,04cb9669d21dd81948a998e6b894fe1667f2ef79ea5af1c0180a28022e67a25a,,False
509,509,509,509,2009 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/pc5g-zfsx/rows.csv?accessType=DOWNLOAD,5e-05,8/21/2011,csv,,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees. ",,Government,"salary, whitehouse",text,p3156,,,,,,8f87a0cf032040d1f753837aaa9d07e83c09d62fb6514b6fe70b761da4c77e5a,,False
510,510,510,510,Franchise Failureby Brand 2011,https://opendata.socrata.com/api/views/5qh7-7usu/rows.csv?accessType=DOWNLOAD,3e-05,6/15/2012,csv,,public,,,"SBA Portfolio Performance by Franchise Code - Data as of 09/30/2011 Combined 7(a) & 504 loan performance data based on loans approved between 10/01/2001 and 09/30/2011, which were designated with a franchise code* and subsequently disbursed.",,Consumer Products,,text,p3157,,,,,,80b9a7b6ab3fb7cd8609e1fe68b851347f69c2b26db748229c3a582e345560f2,,False
511,511,511,511,Unclaimed bank accounts,https://opendata.socrata.com/api/views/n2rk-fwkj/rows.csv?accessType=DOWNLOAD,0.001,6/26/2014,csv,,public,,,A list of all the abandoned bank accounts at branches in the Edmonton area and or registered to addresses in the Edmonton area.,,Banking,,text,p3158,,,,,,4b9342271b436fa91851cb33867e038d3056e40c0796d080549d701b0c6d4f9f,,False
512,512,512,512,Country List ISO 3166 Codes Latitude Longitude,https://opendata.socrata.com/api/views/mnkm-8ram/rows.csv?accessType=DOWNLOAD,8e-06,8/21/2011,csv,,public,,,"Country code, latitude, longitude, and numeric code.",,Government,location,text,p3159,,,,,,c804f68e03ea27aa479470f357f56d66470edaa816a55dbbb937de7f90310ea8,,False
513,513,513,513,Groups That Have Lost Their Tax-Exempt Status,https://opendata.socrata.com/api/views/j5va-k6fu/rows.csv?accessType=DOWNLOAD,0.08,9/10/2011,csv,,public,,,"Use the searchable the database or browse the list of the 275,000 organizations that the IRS says are no longer tax-exempt because they did not file legally required documents. The IRS announced the revocations on June 8.",,Government,tax,text,p3160,,,,,,fe6f06bc4969c0e5598c2ac1ba21da6f335bc330ef8ad92fd80e31dbecb6cf0c,,False
514,514,514,514,South Carolina State Employee Salary Database,https://opendata.socrata.com/api/views/67f6-9d58/rows.csv?accessType=DOWNLOAD,0.002,2/6/2014,csv,,public,,,"All employees of the State of South Carolina that make at least $50,000 per year. Imported from the South Carolina Budget and Control Board website.",,Government,salary,text,p3161,,,,,,835b6ea823a40c0809381217e0e5d4289c8dd1ef2f75db1585b2713dc6ca38cd,,False
515,515,515,515,Airplane Crashes and Fatalities Since 1908,https://opendata.socrata.com/api/views/q2te-8cvq/rows.csv?accessType=DOWNLOAD,0.002,08/21/2011,csv,,,,,"Full history of airplane crashes throughout the world, from 1908-present.",,Travel and transportation,"airplane, fatalities, crash",text,p3162,,,,,,e45249073918e49441904b9375c82c9272af6d5ee3a8a0c4d71f7d1269b53cf0,,False
516,516,516,516,Alcohol Consumption Per Country,https://opendata.socrata.com/api/views/hj43-2bpj/rows.csv?accessType=DOWNLOAD,0.003,08/21/2011,csv,,public,,,The World Health Organization (WHO)'s breakdown of per capita alcohol consumption among adults over 15.,,Government,alcohol,text,p3162,,,,,,5cde7ba5be3e499aa09e2303499a7ec53a3135a77d6c4c136bea7c4ca1275bf3,,False
517,517,517,517,Population Growth,https://datahub.io/core/population-growth-estimates-and-projections/r/population-growth-estimates-and-projections_zip.zip,0.028,1/1/2018,zip,,public,,,"Data comes from United Nations’ Population Division datasets. Total population (both sexes combined) by region, subregion and country, annually for 1950-2100 (thousands). Data is cleaned, normalized, “un-pivoted”, and represented in nice machine readable way in CSV format.",,Government,population,text,p3163,,,,,,3fb98f696bfd383771b34fdd210b104dc065ca1da37f2c904c50e076dadc816b,,False
518,518,518,518,Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6248/soft/GDS6248_full.soft.gz,0.0149,3/1/2014,zip,,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",full SOFT file,P1091,,,,,,85707a43806a4121ccec396a4e553615b1192486c41659023952e494548687f5,,False
519,519,519,519,City Population Annual Timeseries (UN Statistics Division),https://datahub.io/core/population-city/r/population-city_zip.zip,0.002,1/1/2018,zip,,public,,,"UNSD Demographic Statistics: City population by sex, city and city type. Data Source: UNData. UNSD Demographic Statistics.",,Government,"population, statistics",text,p3164,,,,,,7e656a23a6c2c0aecb1a77e7e9e0e641fcdd535a3d43650f32240c6686c548a8,,False
520,520,520,520,IMF World Economic Outlook Database,https://datahub.io/core/imf-weo/r/imf-weo_zip.zip,0.005,3/1/2018,zip,,public,,,IMF World Economic Outlook (WEO) database. The [IMF World Economic Outlook][weo] is a twice-yearly survey by IMF staff that presents IMF staff economists' analyses of global economic developments during the near and medium term.,,Government,economics,text,p3165,,,,,,4ef7a5acd22a6c3c8b9df3dc3421e1e80cfb6e3086913353a07a34e4a9e9c02a,,False
521,521,521,521,European Union Emissions Trading System,https://datahub.io/core/eu-emissions-trading-system/r/eu-emissions-trading-system_zip.zip,0.0009,3/1/2018,zip,,public,,,Data about the EU emission trading system (ETS). The EU emission trading system (ETS) is one of the main measures introduced by the EU to achieve cost-efficient reductions of greenhouse gas emissions and reach its targets under the Kyoto Protocol and other commitments.,,Government,"environment, emissions",text,p3166,,,,,,d8f50d9578f98020ddb22ea1ff70d7e0a6a0c7c0bdc940aa2ff7d7d5ab6f012b,,False
522,522,522,522,Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6248/soft/GDS6248.soft.gz,0.0073,3/1/2014,zip,,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",DataSet full SOFT file,P1091,,,,,,6d554096d3a94c59ac79077080c4ce21ac7c1329a5eeba6fb9f45ae7e3b99854,,False
523,523,523,523,Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39549/soft/GSE39549_family.soft.gz,0.041,3/1/2014,zip,,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",DataSet SOFT file,P1091,,,,,,480d8df1abd0e5eb4bf93d32c25e166260fc3c421db2761eb2637a4dbbf85ac7,,False
524,524,524,524,Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39549/miniml/GSE39549_family.xml.tgz,0.041,3/1/2014,zip,,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",Series family SOFT file,P1091,,,,,,414a3ce9f12ea2434c22987e633820cf3303a4e1bb0cbaab3f19404879cd9bcb,,False
525,525,525,525,Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6887/annot/GPL6887.annot.gz,0.006999999999999999,3/1/2014,zip,,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",Series family MINiML file,P1091,,,,,,ea5655b07dbfaed4b6bad81863ae97934b123175888c2ea57904aede9639ef81,,False
526,526,526,526,UN-LOCODE Codelist,https://datahub.io/core/un-locode/r/un-locode_zip.zip,0.006,3/1/2018,zip,,public,,,"The United Nations Code for Trade and Transport Locations is a code list mantained by UNECE, United Nations agency, to facilitate trade. Data Data comes from the UNECE page, released at least once a year.",,Government,,text,p3167,,,,,,877f2d7147aab959630a411e35317f5b1f5fd34bbd02dd0d2fca02120e4a3a2e,,False
527,527,527,527,Natural Earth Admin1 Polygons as GeoJSON,https://datahub.io/core/geo-ne-admin1/r/geo-ne-admin1_zip.zip,0.009000000000000001,6/1/2018,zip,,public,,,Geodata data package providing geojson polygons for the largest administrative subdivisions in every countries. ,,Government,"geo, country",text,p3168,,,,,,bac4a76ffba6582742a912e650ce4bf6e2415c6a461fcb30fd1d0a0d5eea68c8,,False
528,528,528,528,Gross capital formation (% of GDP),https://datahub.io/world-bank/ne.gdi.totl.zs/r/ne_gdi_totl_zs_zip.zip,0.0004,,zip,,public,,,"Gross capital formation (formerly gross domestic investment) consists of outlays on additions to the fixed assets of the economy plus net changes in the level of inventories. Fixed assets include land improvements (fences, ditches, drains, and so on); plant, machinery, and equipment purchases.",,Government,GDP,text,p3169,,,,,,2a88ff361592f484667e717e5eaf0b3a5c94370c87f3db198097bea350a3321a,,False
529,529,529,529,Diet-induced obesity model: white adipose tissue,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6247/soft/GDS6247_full.soft.gz,0.0013,3/1/2014,zip,,public,,,Analysis of epididymal and mesenteric white adipose tissues (WAT) of mice fed a high fat diet for up to 24 weeks. Excessive fat accumulation was evident in visceral WAT depots after 4 weeks. Results provide insight into the molecular events that occur during the development of diet-induced obesity.,,Health,"mice, liver, Mus musculus",DataSet full SOFT file,P1092,,,,,,ce6b24514e191419a22c1854a55fdb92d29614c4cc4807214027afd67bbe5f1b,,False
530,530,530,530,Diet-induced obesity model: white adipose tissue,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6247/soft/GDS6247.soft.gz,0.006,3/1/2014,zip,,public,,,Analysis of epididymal and mesenteric white adipose tissues (WAT) of mice fed a high fat diet for up to 24 weeks. Excessive fat accumulation was evident in visceral WAT depots after 4 weeks. Results provide insight into the molecular events that occur during the development of diet-induced obesity.,,Health,"mice, liver, Mus musculus",DataSet SOFT file,P1092,,,,,,163d801bb088a0ed069b671363266981dbfe1fa8de3cb4e06ac4d17730e50446,,False
531,531,531,531,"Labor force, female (% of total labor force)",https://datahub.io/world-bank/sl.tlf.totl.fe.zs/r/sl_tlf_totl_fe_zs_zip.zip,0.000237,6/1/2018,zip,,public,,,Female labor force as a percentage of the total show the extent to which women are active in the labor force. Labor force comprises people ages 15 and older who supply labor for the production of goods and services during a specified period.,,Government,"salary, labor, women",text,p3170,,,,,,25c9048f9b693ccdabdb6a3f6d8ebd2b789bb530f0b0dabcf8922b1fb04ee850,,False
532,532,532,532, Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6177/soft/GDS6177_full.soft.gz,0.013,7/25/2013,zip,,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1093,,,,,,4228bb503509e96c136ba5b855b5d2d266a76ddc519335acd8bbab07f73114f2,,False
533,533,533,533, Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6177/soft/GDS6177.soft.gz,0.0048,7/25/2013,zip,,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1093,,,,,,4d7c57fd74459b79d4ed43d732bdb88b6db7521c5ad5e20f50f09e03a3c1ec4d,,False
534,534,534,534, Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE20nnn/GSE20489/soft/GSE20489_family.soft.gz,0.043,7/25/2013,zip,,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1093,,,,,,316a3604a4707a821bdae7689f3c3f06b29f913639b314cfbae2f2da554bd14d,,False
535,535,535,535, Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE20nnn/GSE20489/miniml/GSE20489_family.xml.tgz,0.043,7/25/2013,zip,,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1093,,,,,,39a74a75ca4dc747f99c99e621a9cec4aa090be076b27bd1edbf93a6afa6e2ab,,False
536,536,536,536, Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPLnnn/GPL570/annot/GPL570.annot.gz,0.0081,7/25/2013,zip,,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1093,,,,,,967b149da8bdf04766cd2a1475d1e59a7cc998d39205c88820666f9b03229c69,,False
537,537,537,537,Caspase-1 deficiency effect on lipid-loaded intestines,https://www.ncbi.nlm.nih.gov/sites/GDSbrowser/,0.0081,9/15/2013,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,full SOFT file,P1093,,,,,,3eb84ec6ab1ae7e3534a0bb3a347c3976bb4f539fcb53baa56ddd7297ab29cd2,,False
538,538,538,538,Energy intensity level of primary energy,https://datahub.io/world-bank/eg.egy.prim.pp.kd/r/eg_egy_prim_pp_kd_zip.zip,0.00023,6/1/2018,zip,,public,,,Energy intensity level of primary energy is the ratio between energy supply and gross domestic product measured at purchasing power parity. Energy intensity is an indication of how much energy is used to produce one unit of economic output.,,Energy and Utilities,"energy, intensity",text,p3171,,,,,,879ce633d1be63ed5f5a0ce7efc9b09c17b0a70753a017c68cbfe78eef6037a1,,False
539,539,539,539,Arable land (% of land area),https://datahub.io/world-bank/ag.lnd.arbl.zs/r/ag_lnd_arbl_zs_zip.zip,0.0004,6/1/2018,zip,,public,,,"Arable land includes land defined by the FAO as land under temporary crops (double-cropped areas are counted once), temporary meadows for mowing or for pasture, land under market or kitchen gardens, and land temporarily fallow. Land abandoned as a result of shifting cultivation is excluded.",,Consumer Products,"land, agriculture",text,p3172,,,,,,2741777ca1ed321663071de54d75aecbe347a3d3ce9a644a936939bfdb2aa69c,,False
540,540,540,540,Alternative and nuclear energy (% of total energy use),https://datahub.io/world-bank/eg.use.comm.cl.zs/r/eg_use_comm_cl_zs_zip.zip,0.0003,6/1/2018,zip,,public,,,"Clean energy is noncarbohydrate energy that does not produce carbon dioxide when generated. It includes hydropower and nuclear, geothermal, and solar power, among others.",,Energy and Utilities,clean energy,text,p3173,,,,,,c08d033a1fd29983fd2aa5d5aacc41fe1833cd9196056f8f917d1d83666a2933,,False
541,541,541,541,"Population, female (% of total)",https://datahub.io/world-bank/sp.pop.totl.fe.zs/r/sp_pop_totl_fe_zs_zip.zip,0.0004,6/1/2018,zip,,public,,,"Female population is the percentage of the population that is female. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship.",,Government,"population, female",text,p3174,,,,,,ec6da7407890e09346375b2d18fbffb792d488ebb8f33275725831cc0e410eb8,,False
542,542,542,542,Domestic credit to private sector (% of GDP),https://datahub.io/world-bank/fs.ast.prvt.gd.zs/r/fs_ast_prvt_gd_zs_zip.zip,0.00035800000000000003,6/1/2018,zip,,,,,"Domestic credit to private sector refers to financial resources provided to the private sector by financial corporations, such as through loans, purchases of nonequity securities, and trade credits and other accounts receivable, that establish a claim for repayment.",,Banking,"credit, private sector",text,p3175,,,,,,82402d297555683983eea3d9784cb184b1fdeb139b0a0f427045d2a6f5b2e27b,,False
543,543,543,543,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6176/soft/GDS6176.soft.gz,0.002,9/15/2013,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,DataSet full SOFT file,P1093,,,,,,fb96b1506373ece39bfa60c11f71214d5ddf0866b05e7e2b13fb22f0388fabc5,,False
544,544,544,544,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE32nnn/GSE32515/soft/GSE32515_family.soft.gz,0.0115,9/15/2013,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,DataSet SOFT file,P1093,,,,,,608977063dbc587353aa4b962898fcfeef0d60e0ce14d71515522beb224a877e,,False
545,545,545,545,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE32nnn/GSE32515/miniml/GSE32515_family.xml.tgz,0.0114,9/15/2013,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,Series family SOFT file,P1093,,,,,,96f6b28c53b594e1a2b04bc846805bc254057050fdfc09de9452ef3f6cb58f82,,False
546,546,546,546,Population density,https://datahub.io/world-bank/en.pop.dnst/r/en_pop_dnst_zip.zip,0.0005,6/1/2018,zip,,public,,,"Population density is midyear population divided by land area in square kilometers. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship–except for refugees not permanently settled in the country of asylum, who are generally considered part of the population of their country of origin.",,Government,"population, land",text,p3176,,,,,,662c81e9cf814a9c2be5ac9ce94ac6ce8f2f34a43591ac379894e7981fb56d61,,False
547,547,547,547,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6100/soft/GDS6100_full.soft.gz,0.0093,4/21/2015,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1094,,,,,,ea71989c90425f4633c178d28467890e431ebce32ea254affc2e0742052c9551,,False
548,548,548,548,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6100/soft/GDS6100.soft.gz,0.002,4/21/2015,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1094,,,,,,bc1c940c59c794d84d2e4532d4514cd9404418aa0029b920c2b3bc90f92a32de,,False
549,549,549,549,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE57nnn/GSE57820/soft/GSE57820_family.soft.gz,0.021,4/21/2015,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1094,,,,,,bf5a8a29903d701fd75517f8f2a60ca518e5f375b986f341f3b7dcb44adecf1f,,False
550,550,550,550,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE57nnn/GSE57820/miniml/GSE57820_family.xml.tgz,0.021,4/21/2015,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1094,,,,,,a5ba25c81a3fbb680d98304a7437bec055e96f2e05d7e5426455401ac4768266,,False
551,551,551,551,Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL10nnn/GPL10558/annot/GPL10558.annot.gz,0.0071,4/21/2015,zip,,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1094,,,,,,375ab54ebbd3a2ace9ec3c635e1044e428187c2c4a8f103ddbc004b928d125a7,,False
552,552,552,552,"Employment in agriculture, female",https://datahub.io/world-bank/sl.agr.empl.fe.zs/r/sl_agr_empl_fe_zs_zip.zip,0.0002,6/1/2018,zip,,public,,,"Employment is defined as persons of working age who were engaged in any activity to produce goods or provide services for pay or profit, whether at work during the reference period or not at work due to temporary absence from a job, or to working-time arrangement.",,Government,"agriculture, employment",text,p3178,,,,,,d6bbc58013394759236c033115ad559f3b65359b9bd1fd03f658c85fd886c086,,False
553,553,553,553,Prevalence of anemia among children,https://datahub.io/world-bank/sh.anm.chld.zs/r/sh_anm_chld_zs_zip.zip,0.000127,6/1/2018,zip,,,,,"Prevalence of anemia, children under age 5, is the percentage of children under age 5 whose hemoglobin level is less than 110 grams per liter at sea level.",,Health,"anemia, children",text,p3179,,,,,,08445b6329f6ac2ef5c64cdaa8d15cc94c1d959df0c58cbe24dc3a9f5826a826,,False
554,554,554,554,Renewable electricity output,https://datahub.io/world-bank/eg.elc.rnew.zs/r/eg_elc_rnew_zs_zip.zip,0.000204,6/1/2018,zip,,public,,,Renewable electricity is the share of electrity generated by renewable power plants in total electricity generated by all types of plants.,,Energy and Utilities,"energy, electricity, renewable energy",text,p3180,,,,,,66265c22e7662fb8d108fab1ade10a25f33b1fb6e0bba550b3c9b124915055ac,,False
555,555,555,555,"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6083/soft/GDS6083_full.soft.gz,0.011,3/31/2015,zip,,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1095,,,,,,5f368d284341bcf1ec399cdb1f18220a9873f702bd32037ff4b74787bb111eed,,False
556,556,556,556,"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6083/soft/GDS6083.soft.gz,0.0025,3/31/2015,zip,,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1095,,,,,,15911b25f913fcf52cb443ce3fe41691c7d15c903697c622979871c414a6a416,,False
557,557,557,557,"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62533/soft/GSE62533_family.soft.gz,0.016,3/31/2015,zip,,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1095,,,,,,d3b249b68eaa6279dd51b62c480d32b5ce6b70a4057e710221eaa382a84f8490,,False
558,558,558,558,"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62533/miniml/GSE62533_family.xml.tgz,0.016,3/31/2015,zip,,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1095,,,,,,a1e3a7270624d780ece39c25fd0623aea10d5becb79f982f5a57bfd762e8bace,,False
559,559,559,559,Urban Population,https://datahub.io/world-bank/sp.urb.totl.in.zs/r/sp_urb_totl_in_zs_zip.zip,0.00032900000000000003,6/1/2018,zip,,public,,,Urban population refers to people living in urban areas as defined by national statistical offices. The data are collected and smoothed by United Nations Population Division.,,Government,"population, urban",text,p3181,,,,,,8f5c63d7bb097b4e87c3d6452f3cfd09bb3a7b19b52848139780ae085df2b0de,,False
560,560,560,560,"Mortality rate, infant",https://datahub.io/world-bank/sp.dyn.imrt.in/r/sp_dyn_imrt_in_zip.zip,0.000195,6/1/2018,zip,,public,,,"Infant mortality rate is the number of infants dying before reaching one year of age, per 1,000 live births in a given year.",,Health,"mortality, infant",text,p3182,,,,,,47c733763215fbba0323c439b42c70e968a288da6bcce044cf72464e19cc280c,,False
561,561,561,561,Comic Characters,https://datahub.io/five-thirty-eight/comic-characters/r/comic-characters_zip.zip,0.002,6/1/2018,zip,,public,,,"This folder contains data behind the story Comic Books Are Still Made By Men, For Men And About Men. The data comes from Marvel Wikia and DC Wikia.",,Media and Entertainment,,text,p3183,,,,,,1d9050035ff2511fdbb2112ca6389fdbdc3b4f49ca2cd89818ebc9ebf8e76120,,False
562,562,562,562,Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6082/soft/GDS6082_full.soft.gz,0.0084,3/25/2015,zip,,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1096,,,,,,f494c1621dd12e6ee33362ef3479b961013ad0f59a63380cae28f9e504d594a9,,False
563,563,563,563,Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6082/soft/GDS6082.soft.gz,0.0014,3/25/2015,zip,,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1096,,,,,,9b379a617fe41b4477fa9bdbf2018070269df1b4e2882b10c05c5010d0c47d6f,,False
564,564,564,564,Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67198/soft/GSE67198_family.soft.gz,0.0014,3/25/2015,zip,,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1096,,,,,,3eb8d7dbc113c58885b00d2952e12c04acda68813b1fb9e3c3e5c718b55fd435,,False
565,565,565,565,Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67198/miniml/GSE67198_family.xml.tgz,0.0175,3/25/2015,zip,,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1096,,,,,,5f237822f6c207fe149c12a6149317af3eb7a2ba2f5f7401c39ffa041b031032,,False
566,566,566,566,Congress Age,https://datahub.io/five-thirty-eight/congress-age/r/congress-age_zip.zip,0.001,6/1/2018,zip,,public,,,This folder contains the data behind the story Both Republicans And Democrats Have an Age Problem congress-terms.csv has an entry for every member of congress who served at any point during a particular congress between January 1947 and Februrary 2014.,,Government,"congress, whitehouse",text,p3184,,,,,,e3609d12787caa651a559b3cb044582824bb8b56996f8adde6bd7b8b892c7c92,,False
567,567,567,567,Mayweather Mcgregor,https://datahub.io/five-thirty-eight/mayweather-mcgregor/r/mayweather-mcgregor_zip.zip,0.002,6/1/2018,zip,,public,,,"This folder contains 12,118 tweets that contain one or more emojis and match one or more of the following hashtags: #MayMac, #MayweatherMcGregor, #MayweatherVMcGregor, #MayweatherVsMcGregor, #McGregor and #Mayweather.",,Media and Entertainment,"fight, mayweather",text,p3185,,,,,,0902dea1a8e57072dea92b5c527a9c88dd3534bf0ae9b5e0c57555c2786221ab,,False
568,568,568,568,Soccer Spi,https://datahub.io/five-thirty-eight/soccer-spi/r/soccer-spi_zip.zip,0.001,6/1/2018,zip,,public,,,This file contains links to the data behind our Club Soccer Predictions and Global Club Soccer Rankings.,,Media and Entertainment,"sports, soccer",text,p3186,,,,,,8d78f751b18460b2b3bd154835cdd804c0d3c6e55c95f7cb4edde7b539b17e18,,False
569,569,569,569,Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6064/soft/GDS6064_full.soft.gz,0.0092,3/1/2015,zip,,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1096,,,,,,4c44bad503e3dd90801dfc9b2785457ff1aeca250a0b71f4cb2c046f5a84c8e4,,False
570,570,570,570,Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6064/soft/GDS6064.soft.gz,0.002,3/1/2015,zip,,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1096,,,,,,d303db983eb930a876a7f595d29742ad713261a338d4fbc73359bf526367cebd,,False
571,571,571,571,Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61140/soft/GSE61140_family.soft.gz,0.0138,3/1/2015,zip,,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1096,,,,,,22a7f78ac5f5345c1975620fc5997abb7942514d40006386efb9a8f8145c43cf,,False
572,572,572,572,Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61140/miniml/GSE61140_family.xml.tgz,0.0137,3/1/2015,zip,,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1096,,,,,,9e1685c85e81547d94cd8856736dff7d979fe41aec7b6708dc64f9f3ae039a13,,False
573,573,573,573,Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.0137,3/1/2015,zip,,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1096,,,,,,30a6d6788933341886677b7570293eb2b1ffacc07034d0b90008e55621f32e29,,False
574,574,574,574,Real Estate Across The United States Building Inventory,https://datahub.io/JohnSnowLabs/real-estate-across-the-united-states-building-inventory/r/datapackage_zip.zip,0.001,1/1/2018,zip,,public,,,Real estate inventory (United States). ,,Consumer Products,"real estate, housing",text,p3187,Real Estate,,,,,71463d12d7eedf2321d7d2784db7432cd09f6dc4f716dfac798c157f768a1562,,False
575,575,575,575,Employee Salaries Montgomery County MD 2014,https://datahub.io/JohnSnowLabs/employee-salaries-montgomery-county-md-2014/r/datapackage_zip.zip,0.001,1/1/2018,zip,,public,,,Employee salaries in Montgomery county. ,,Government,salary,text,p3188,,,,,,ceaab2f68e5e3bfa55aa1152cbdb9a5245abb91fc020efe273af8839a2958e46,,False
576,576,576,576,State of the City Corpus,https://datahub.io/etachov/state-of-the-city/r/datapackage_zip.zip,0.001,1/1/2018,zip,,public,,,A corpus of State of the City speeches made by mayors across America. Right now I'm focusing three years of speeches from the fifteen largest cities and am tracking my progress here.,,Government,"speeches, mayor",text,p3189,,,,,,c31b7fe5e7c943c845436aafd93a6327198e378e38d96f33a475821d5f577b86,,False
577,577,577,577,Estimates Emissions of CO2,https://datahub.io/JohnSnowLabs/estimates-emissions-of-co2-at-country-and-global-level-starting-1751/r/estimates-emissions-of-co2-at-country-and-global-level-starting-1751_zip.zip,0.001,1/1/2018,zip,,public,,,Estimates of CO2 emissions at at country and global level (starting in 1751).,,Government,emissions,text,p3190,,,,,,53e1f7c29b87bd269facd21f790e45c3a9c7f6da408bcb3d0a115a5af9f8692a,,False
578,578,578,578,GDP by Industry and Country,https://datahub.io/JohnSnowLabs/gdp-by-industry-and-country/r/gdp-by-industry-and-country_zip.zip,0.003,1/1/2018,zip,,public,,,"GDP by industry and country - includes information about currency, indicator, and yearly estimates.",,Government,GDP,text,p3191,,,,,,9cfa127d1db9af3561ccfc3dc855f99157c93170cb867d5a210096af4f93865f,,False
579,579,579,579,Iswc2018 dataset,https://datahub.io/sunzequn/iswc2018-dataset/r/iswc2018-dataset_zip.zip,2.0,5/1/2018,zip,Apache License 2.0,public,,,"Datasets for Embedding-based Entity Alignment We considered the following four aspects to build our datasets: source KG, dataset language, entity size and difference of degree distributions between the extracted datasets and original KGs.",,Electronics,embedding,text,p3192,,,,,,83d84590fe13aac06b6a267b0b125a7a3d698227d6a772b23e2776fe0c789027,,False
580,580,580,580,"Charges for the use of intellectual property, receipts",https://datahub.io/test/bx.gsr.royl.cd/r/bx_gsr_royl_cd_zip.zip,0.000196,6/1/2018,zip,CC-BY-4.0,public,,,"Charges for the use of intellectual property are payments and receipts between residents and nonresidents for the authorized use of proprietary rights (such as patents, trademarks, copyrights, industrial processes and designs including trade secrets, and franchises) and for the use, through licensing agreements, of produced originals or prototypes (such as copyrights on books and manuscripts, computer software, cinematographic works, and sound recordings) and related rights (such as for live performances and television, cable, or satellite broadcast). Data are in current U.S. dollars.",,Consumer Products,"trade, property",text,p3193,,,,,,4017c523676504bf8ce8a2d57ab655317849547522805ec30c83a4f0fc4f7384,,False
581,581,581,581,"Life expectancy at birth, male (years)",https://datahub.io/world-bank/sp.dyn.le00.ma.in/r/sp_dyn_le00_ma_in_zip.zip,0.0003,6/1/2018,zip,CC-BY-4.0,public,,,Life expectancy at birth indicates the number of years a newborn infant would live if prevailing patterns of mortality at the time of its birth were to stay the same throughout its life.,,Health,"life expectancy, male",text,p3194,,,,,,20c768bfdb582d8993ebcbc8f10ca9d8430896f31da094595cfd99a6aa15aedc,,False
582,582,582,582,	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6063/soft/GDS6063_full.soft.gz,0.0091,2/1/2016,zip,,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1097,,,,,,03e5cd3193404a8dae90aa41b598d155e87a03331f7c0f44105982ac0cda10c8,,False
583,583,583,583,	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6063/soft/GDS6063.soft.gz,0.0019,2/1/2016,zip,,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1097,,,,,,2524d27fbeb35ec9c08dd6f0fc6960de085c2e4166bf0feb247dc03ef5d335a2,,False
584,584,584,584,	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE68nnn/GSE68849/soft/GSE68849_family.soft.gz,0.021,2/1/2016,zip,,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1097,,,,,,f0f331f822b7979a1deceb4cb6b06785e53e39fc8d0f9ac194351fff79d2e2e5,,False
585,585,585,585,	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE68nnn/GSE68849/miniml/GSE68849_family.xml.tgz,0.00216,2/1/2016,zip,,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1097,,,,,,79ecd0933f6bed4baf62b264a30a013a6eb8f07fcbcc3c2ac37ee764410f6782,,False
586,586,586,586,"Primary completion rate, total (% of relevant age group)",https://datahub.io/world-bank/se.prm.cmpt.zs/r/se_prm_cmpt_zs_zip.zip,0.00023,6/1/2018,zip,CC-BY-4.0,public,,,"Primary completion rate, or gross intake ratio to the last grade of primary education, is the number of new entrants (enrollments minus repeaters) in the last grade of primary education, regardless of age, divided by the population at the entrance age for the last grade of primary education. ",,Education,"completion, education, rate",text,p3195,,,,,,8f3b8102dcf03586c694251cbd4ee267883c07860be63625bf35ee4431b1b26a,,False
587,587,587,587,"Total debt service (% of exports of goods, services and primary income)",https://datahub.io/world-bank/dt.tds.dect.ex.zs/r/dt_tds_dect_ex_zs_zip.zip,0.00017,6/1/2018,zip,CC-BY-4.0,public,,,"Total debt service to exports of goods, services and primary income. Total debt service is the sum of principal repayments and interest actually paid in currency, goods, or services on long-term debt, interest paid on short-term debt, and repayments (repurchases and charges) to the IMF.",,Banking,debt,text,p3196,,,,,,a1fe09f3ecc285a774b3756310f0813f3adb01fa95e6504a1909cd4bec8badb2,,False
588,588,588,588,"Gross intake ratio in first grade of primary education, female",https://datahub.io/world-bank/se.prm.gint.fe.zs/r/se_prm_gint_fe_zs_zip.zip,0.000218,6/1/2018,zip,CC-BY-4.0,public,,,"Gross intake ratio in first grade of primary education is the number of new entrants in the first grade of primary education regardless of age, expressed as a percentage of the population of the official primary entrance age.",,Education,primary,text,p3196,Primary,,,,,79384821d2e102674e2c747d81e3ba774dd2f20c2064de74dfd3787e13b4f411,,False
589,589,589,589,"Life expectancy at birth, female (years)",https://datahub.io/world-bank/sp.dyn.le00.fe.in/r/sp_dyn_le00_fe_in_zip.zip,0.0003,6/1/2018,zip,CC-BY-4.0,public,,,Life expectancy at birth indicates the number of years a newborn infant would live if prevailing patterns of mortality at the time of its birth were to stay the same throughout its life.,,Health,"life expectancy, female",text,p3197,,,,,,1a805561dde378dbd5d9a79fe4859015805ba3da729c1fc1a75d280c953a2067,,False
590,590,590,590,High-technology exports (current US$),https://datahub.io/world-bank/tx.val.tech.cd/r/tx_val_tech_cd_zip.zip,0.00012,6/1/2018,zip,CC-BY-4.0,public,,,"High-technology exports are products with high R&D intensity, such as in aerospace, computers, pharmaceuticals, scientific instruments, and electrical machinery. Data are in current U.S. dollars.",,Consumer Products,"exports, technology",text,p3198,,,,,,590d26b0366a7303128120e27e3a2dc00f283fd85865fbefbe172867292dc1f9,,False
591,591,591,591,Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6016/soft/GDS6016_full.soft.gz,0.0091,12/23/2013,zip,,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,DataSet full SOFT file,P1098,,,,,,6f13db8d1b45ea5b1c0b2bca7d123211c7708c70a61d00b7c416a7ef3e5de6d5,,False
592,592,592,592,Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6016/soft/GDS6016.soft.gz,0.000729,12/23/2013,zip,,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,DataSet SOFT file,P1098,,,,,,f0251cee6cf83617e265b22b2ef36a2435318865723557c178f9d95c835e06bd,,False
593,593,593,593,Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE51nnn/GSE51612/soft/GSE51612_family.soft.gz,0.0089,12/23/2013,zip,,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,Series family SOFT file,P1098,,,,,,daeb533b33c709311fb00944d264c1990393d593c584b543e02615ac683e7534,,False
594,594,594,594,Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE51nnn/GSE51612/miniml/GSE51612_family.xml.tgz,0.009000000000000001,12/23/2013,zip,,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,Series family MINiML file,P1098,,,,,,50d503d1ebf1fc0a4cde20888c7e5ee02e19abecb61746337e7cec969bbfd2a5,,False
595,595,595,595,Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL7nnn/GPL7202/annot/GPL7202.annot.gz,0.0084,12/23/2013,zip,,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,Annotation SOFT file,P1098,,,,,,791ce1e7517b896704dad4d50614e9350d1efe692454c4af31037e3de92af05c,,False
596,596,596,596,Merchandise exports (current US$),https://datahub.io/world-bank/tx.val.mrch.cd.wt/r/tx_val_mrch_cd_wt_zip.zip,0.000306,6/15/2018,zip,CC-BY-4.0,public,,,Merchandise exports show the f.o.b. value of goods provided to the rest of the world valued in current U.S. dollars.,,Consumer Products,"exports, merchandise",text,p3199,,,,,,49f3a81456c9a1406efdf18e5061b21ebc3bae912121c492a77b9c9eaa272f9e,,False
597,597,597,597,Food production index,https://datahub.io/world-bank/ag.prd.food.xd/r/ag_prd_food_xd_zip.zip,0.00027,6/15/2018,zip,CC-BY-4.0,public,,,"Food production index covers food crops that are considered edible and that contain nutrients. Coffee and tea are excluded because, although edible, they have no nutritive value.",,Consumer Products,"food, health, production",text,p3200,,,,,,afab86363404c4d7e0d4ba2d2c604fbd25fb1eb2a6855b9088922edf9840e307,,False
598,598,598,598,"Net flows on external debt, total",https://datahub.io/world-bank/dt.nfl.dect.cd/r/dt_nfl_dect_cd_zip.zip,0.00013700000000000002,6/15/2018,zip,CC-BY-4.0,public,,,"Net flows on external debt are disbursements on long-term external debt and IMF purchases minus principal repayments on long-term external debt and IMF repurchases up to 1984. Beginning in 1985 this line includes the change in stock of short-term debt (including interest arrears for long-term debt). Thus, if the change in stock is positive, a disbursement is assumed to have taken place; if negative, a repayment is assumed to have taken place. Long-term external debt is defined as debt that has an original or extended maturity of more than one year and that is owed to nonresidents by residents of an economy and repayable in currency, goods, or services.",,Banking,"debt, NFL",text,p3201,,,,,,8ee79b1661776bf65e00349719b97202884d48fac0331f73dda1cd4300c75bcf,,False
599,599,599,599,Exports of goods and services (% of GDP),https://datahub.io/world-bank/ne.exp.gnfs.zs/r/ne_exp_gnfs_zs_zip.zip,0.0004,6/15/2018,zip,CC-BY-4.0,public,,,"Exports of goods and services represent the value of all goods and other market services provided to the rest of the world. They include the value of merchandise, freight, insurance, transport, travel, royalties, license fees, and other services, such as communication, construction, financial, information, business, personal, and government services. ",,Consumer Products,exports,text,p3202,,,,,,4f810e460ba3e991453159eca65b71174ab64a6c67a469715571f9123a53bc4c,,False
600,600,600,600,"School enrollment, secondary (% net)",https://datahub.io/world-bank/se.sec.nenr/r/se_sec_nenr_zip.zip,0.000143,6/15/2018,zip,CC-BY-4.0,public,,,"Net enrollment rate is the ratio of children of official school age who are enrolled in school to the population of the corresponding official school age. Secondary education completes the provision of basic education that began at the primary level, and aims at laying the foundations for lifelong learning and human development, by offering more subject- or skill-oriented instruction using more specialized teachers.",,Education,enrollment,text,p3203,,,,,,87f8b9918364d8e0e00af4163006fe6793a579a7cd7dd7225d65df229ac8d84e,,False
601,601,601,601,Energy use (kg of oil equivalent per capita),https://datahub.io/world-bank/eg.use.pcap.kg.oe/r/eg_use_pcap_kg_oe_zip.zip,0.000302,6/15/2018,zip,CC-BY-4.0,public,,,"Energy use refers to use of primary energy before transformation to other end-use fuels, which is equal to indigenous production plus imports and stock changes, minus exports and fuels supplied to ships and aircraft engaged in international transport.",,Energy and Utilities,energy,text,p3204,,,,,,f26240290abb956bf29c3603e7376f55b84adbfd597d6aeeae5ce3f9c9e0d29a,,False
602,602,602,602,	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6010/soft/GDS6010_full.soft.gz,0.0091,1/4/2016,zip,,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,DataSet full SOFT file,P1099,,,,,,9bdfdd598bf418409fd10b92014788baf40e466b134ad4ad3111b6a60a023a4b,,False
603,603,603,603,	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6010/soft/GDS6010.soft.gz,0.0027,1/4/2016,zip,,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,DataSet SOFT file,P1099,,,,,,e7d22823c139abe2f2dc14429688bb174c004825943b6037e84fc4c6c4205eb0,,False
604,604,604,604,	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE66nnn/GSE66597/soft/GSE66597_family.soft.gz,0.006999999999999999,1/4/2016,zip,,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,Series family SOFT file,P1099,,,,,,809405e5151a53a81874cf8ad09945c3f944e4a77f4a21f772a118d62afe37f9,,False
605,605,605,605,	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE66nnn/GSE66597/miniml/GSE66597_family.xml.tgz,0.0127,1/4/2016,zip,,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,Series family MINiML file,P1099,,,,,,38e9a7650c76da984c92fcc64ab1293b8352bc351b200dee1a73b7bbe92858a7,,False
606,606,606,606,	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6480/annot/GPL6480.annot.gz,0.006999999999999999,1/4/2016,zip,,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,Annotation SOFT file,P1099,,,,,,0e52e6b156c095397839aaf6eb62f7a182b8ff6257fff312de4580a4ebb7747c,,False
607,607,607,607,"GNI, Atlas method (current US$)",https://datahub.io/world-bank/ny.gnp.atls.cd/r/ny_gnp_atls_cd_zip.zip,0.000391,6/15/2018,zip,CC-BY-4.0,public,,,"GNI (formerly GNP) is the sum of value added by all resident producers plus any product taxes (less subsidies) not included in the valuation of output plus net receipts of primary income (compensation of employees and property income) from abroad. Data are in current U.S. dollars. GNI, calculated in national currency, is usually converted to U.S. dollars at official exchange rates for comparisons across economies, although an alternative rate is used when the official exchange rate is judged to diverge by an exceptionally large margin from the rate actually applied in international transactions. ",,Government,"tax, GNI",text,p3205,,,,,,3247591f10ae99fee15cec601922815186d688467ac17c79d7b8dc551e83122e,,False
608,608,608,608,High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6000/soft/GDS6000_full.soft.gz,0.012,1/8/2015,zip,,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,DataSet full SOFT file,P2000,,,,,,e7cae18df004337e2266d201d6f7a66e136faae17f36195300efc78d249851d7,,False
609,609,609,609,High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6000/soft/GDS6000.soft.gz,0.05,1/8/2015,zip,,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,DataSet SOFT file,P2000,,,,,,3b37b571da95e4e57364bbed3eba1bdd71479f379946dd09c37ac24ffdc49d79,,False
610,610,610,610,High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE64nnn/GSE64718/soft/GSE64718_family.soft.gz,0.25,1/8/2015,zip,,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,Series family SOFT file,P2000,,,,,,7a4f6b494e267a9e3877327722938579923e792d27a88458687fe9f6d9c998cd,,False
611,611,611,611,High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE64nnn/GSE64718/miniml/GSE64718_family.xml.tgz,0.25,1/8/2015,zip,,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,Series family MINiML file,P2000,,,,,,47a6bd4948e480759bc810b310f7987e8066ad5d3e67fc82f5cae1f065f91868,,False
612,612,612,612,Refugee population by country or territory of asylum,https://datahub.io/world-bank/sm.pop.refg/r/sm_pop_refg_zip.zip,0.000108,6/15/2018,zip,CC-BY-4.0,public,,,"Refugees are people who are recognized as refugees under the 1951 Convention Relating to the Status of Refugees or its 1967 Protocol, the 1969 Organization of African Unity Convention Governing the Specific Aspects of Refugee Problems in Africa, people recognized as refugees in accordance with the UNHCR statute, people granted refugee-like humanitarian status, and people provided temporary protection. Asylum seekers–people who have applied for asylum or refugee status and who have not yet received a decision or who are registered as asylum seekers–are excluded. Palestinian refugees are people (and their descendants) whose residence was Palestine between June 1946 and May 1948 and who lost their homes and means of livelihood as a result of the 1948 Arab-Israeli conflict. Country of asylum is the country where an asylum claim was filed and granted.",,Government,"refugees, humanitarian",text,p3206,,,,,,90c124867e82846a86b9d3d69286c28e4abffba7d7d8c0ccce2e1becf3fb1156,,False
613,613,613,613,Agricultural land (% of land area),https://datahub.io/world-bank/ag.lnd.agri.zs/r/ag_lnd_agri_zs_zip.zip,0.000373,6/15/2018,zip,CC-BY-4.0,public,,,"Agricultural land refers to the share of land area that is arable, under permanent crops, and under permanent pastures. Arable land includes land defined by the FAO as land under temporary crops (double-cropped areas are counted once), temporary meadows for mowing or for pasture, land under market or kitchen gardens, and land temporarily fallow.",,Government,"arable, agriculture",text,p3207,,,,,,fc77b593629afdb44536bbc2e729f7bf3b21f9d897e54de6f765d2a4f70c6b77,,False
614,614,614,614,Rural population,https://datahub.io/world-bank/sp.rur.totl/r/sp_rur_totl_zip.zip,336.0,6/15/2018,zip,CC-BY-4.0,public,,,Rural population refers to people living in rural areas as defined by national statistical offices. It is calculated as the difference between total population and urban population. Aggregation of urban and rural population may not add up to total population because of different country coverages.,,Government,"population, rural",text,p3208,,,,,,8b17d1c169ca4516eaeff78a778a9af689fe459acb3bb65f3c937460333eae5d,,False
615,615,615,615,Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5948/soft/GDS5948_full.soft.gz,0.0077,5/15/2015,zip,,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,DataSet full SOFT file,P2001,,,,,,bf0937f14cdb7930fe16f5ae4704ecdf7007430b113e8171ebc31bfca8b18ea9,,False
616,616,616,616,Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5948/soft/GDS5948.soft.gz,0.0009,5/15/2015,zip,,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,DataSet SOFT file,P2001,,,,,,1e3661d1e15c364d2e8734e67fb6d9f6fb689c0b88e8ddce1a25e7c0dbc3f009,,False
617,617,617,617,Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE56nnn/GSE56819/soft/GSE56819_family.soft.gz,0.013,5/15/2015,zip,,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,Series family SOFT file,P2001,,,,,,9c024e2f3bb4abb9e51ce882e2fd1c25c102f2228230a54b49f16a19d4e52526,,False
618,618,618,618,Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE56nnn/GSE56819/miniml/GSE56819_family.xml.tgz,0.013,5/15/2015,zip,,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,Series family MINiML file,P2001,,,,,,c766addddca090774e7063c3e3fa84f29b848b8c5e060b8617c8fccc824b2d43,,False
619,619,619,619,Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6244/annot/GPL6244.annot.gz,0.0069,5/15/2015,zip,,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,Annotation SOFT file,P2001,,,,,,e8e94cf986b2d3ed29708acb5bc41452b26c90e12dd583fed4693b4965dff2d6,,False
620,620,620,620,Net ODA received,https://datahub.io/world-bank/dt.oda.odat.gn.zs/r/dt_oda_odat_gn_zs_zip.zip,0.000327,6/15/2018,zip,CC-BY-4.0,public,,,"Net official development assistance (ODA) consists of disbursements of loans made on concessional terms (net of repayments of principal) and grants by official agencies of the members of the Development Assistance Committee (DAC), by multilateral institutions, and by non-DAC countries to promote economic development and welfare in countries and territories in the DAC list of ODA recipients.",,Government,"loans, banking, welfare",text,p3209,,,,,,22d4c9032a97fbef05f34962226380c08c78ea46f2b4a16a2c280f9141ccc57c,,False
621,621,621,621,"Birth rate, crude (per 1,000 people)",https://datahub.io/world-bank/sp.dyn.cbrt.in/r/sp_dyn_cbrt_in_zip.zip,0.0003,6/15/2018,zip,CC-BY-4.0,public,,,"Crude birth rate indicates the number of live births occurring during the year, per 1,000 population estimated at midyear.",,Government,birth rate,text,p3210,,,,,,3391f0125e960bc2f4347b6c3302a0dd34bc5e04e901ec96082b73981f711258,,False
622,622,622,622,"Gross intake ratio in first grade of primary education, male",https://datahub.io/world-bank/se.prm.gint.ma.zs/r/se_prm_gint_ma_zs_zip.zip,0.000217,6/15/2018,zip,CC-BY-4.0,public,,,"Gross intake ratio in first grade of primary education is the number of new entrants in the first grade of primary education regardless of age, expressed as a percentage of the population of the official primary entrance age.",,Education,primary,text,p3211,Primary,,,,,8cc05d174cd7e97a0a7c5aa9c4d2e07baabd8efc843d9ee565638e1027e74958,,False
623,623,623,623,Population ages 15-64 (% of total),https://datahub.io/world-bank/sp.pop.1564.to.zs/r/sp_pop_1564_to_zs_zip.zip,0.0005,6/15/2018,zip,CC-BY-4.0,public,,,"Total population between the ages 15 to 64 as a percentage of the total population. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship.",,Government,population,text,p3212,,,,,,db2a30021ea00ff9bb978894a91a986b96e306ffed6132cdd2c94ef18e8363a0,,False
624,624,624,624,High-technology exports (% of manufactured exports),https://datahub.io/world-bank/tx.val.tech.mf.zs/r/tx_val_tech_mf_zs_zip.zip,0.000179,6/15/2018,zip,CC-BY-4.0,public,,,"High-technology exports are products with high R&D intensity, such as in aerospace, computers, pharmaceuticals, scientific instruments, and electrical machinery. Data are in current U.S. dollars.",,Consumer Products,"technology, exports",text,p3213,,,,,,22294ae0349b2656cb646f272c95c2d08a17fa5fbc603f92b464da1f5bf5b955,,False
625,625,625,625,"Current account balance (BoP, current US$)",https://datahub.io/world-bank/bn.cab.xoka.cd/r/bn_cab_xoka_cd_zip.zip,0.000216,6/15/2018,zip,CC-BY-4.0,public,,,"Current account balance is the sum of net exports of goods and services, net primary income, and net secondary income. Data are in current U.S. dollars.",,Banking,account balance,text,p3214,,,,,,3e80ec1ec32e2ab42f7cd569909132e447067d2a75c6a99c191d87949111e307,,False
626,626,626,626,Fossil fuel energy consumption (% of total),https://datahub.io/world-bank/eg.use.comm.fo.zs/r/eg_use_comm_fo_zs_zip.zip,0.000292,6/15/2018,zip,CC-BY-4.0,public,,,"Fossil fuel comprises coal, oil, petroleum, and natural gas products.",,Energy and Utilities,,text,p3215,,,,,,4544d8f29129a77ed78eadc47a70dc607e94a954a5856ca41677b2e140182a80,,False
627,627,627,627,Military expenditure (% of GDP),https://datahub.io/world-bank/ms.mil.xpnd.gd.zs/r/ms_mil_xpnd_gd_zs_zip.zip,0.000317,6/15/2018,zip,CC-BY-4.0,public,,,"Military expenditures data from SIPRI are derived from the NATO definition, which includes all current and capital expenditures on the armed forces, including peacekeeping forces; defense ministries and other government agencies engaged in defense projects; paramilitary forces, if these are judged to be trained and equipped for military operations; and military space activities. Such expenditures include military and civil personnel, including retirement pensions of military personnel and social services for personnel; operation and maintenance; procurement; military research and development; and military aid (in the military expenditures of the donor country).",,Government,"expenditures, military, banking",text,p3216,Military,,,,,21fdbc8ba4494b36eafe1788400bb4416f87fc8f4942c8b085d9a0a950ddaeb0,,False
628,628,628,628,Imports of goods and services (% of GDP),https://datahub.io/world-bank/ne.imp.gnfs.zs/r/ne_imp_gnfs_zs_zip.zip,0.0004,6/15/2018,zip,CC-BY-4.0,public,,,"Imports of goods and services represent the value of all goods and other market services received from the rest of the world. They include the value of merchandise, freight, insurance, transport, travel, royalties, license fees, and other services, such as communication, construction, financial, information, business, personal, and government services. They exclude compensation of employees and investment income (formerly called factor services) and transfer payments.",,Consumer Products,"imports, services, goods",text,p3217,,,,,,4d89d60a90c91db101b9684e58d46dfadbc19aa7496833cb2c6e0769865012fd,,False
629,629,629,629,"Contributing family workers, male",https://datahub.io/world-bank/sl.fam.work.ma.zs/r/sl_fam_work_ma_zs_zip.zip,0.00019,6/15/2018,zip,CC-BY-4.0,public,,,Contributing family workers are those workers who hold “self-employment jobs” as own-account workers in a market-oriented establishment operated by a related person living in the same household.,,Government,household,text,p3218,,,,,,df59bb531efde095106eee5533e321942083a21b1e3adb5165d27018a9716451,,False
630,630,630,630,"Immunization, measles (% of children ages 12-23 months)",https://datahub.io/world-bank/sh.imm.meas/r/sh_imm_meas_zip.zip,0.00014199999999999998,6/15/2018,zip,CC-BY-4.0,public,,,"Child immunization, measles, measures the percentage of children ages 12-23 months who received the measles vaccination before 12 months or at any time before the survey. A child is considered adequately immunized against measles after receiving one dose of vaccine.",,Health,"immunization, measles",text,p3219,,,,,,d33a36314d671a3c18ae7911f252e74ab6b56125fdbee90cbc14b2410747f537,,False
631,631,631,631,"Unemployment, total (% of total labor force)",https://datahub.io/world-bank/sl.uem.totl.zs/r/sl_uem_totl_zs_zip.zip,0.000171,6/15/2018,zip,CC-BY-4.0,public,,,Unemployment refers to the share of the labor force that is without work but available for and seeking employment.,,Government,unemployment,text,p3220,,,,,,42627a2c859da35b2188a3bfe4832a1891f8d27c94888142dc98d6caa5b0643b,,False
632,632,632,632,"Employment in industry, male",https://datahub.io/world-bank/sl.ind.empl.ma.zs/r/sl_ind_empl_ma_zs_zip.zip,0.000179,6/15/2018,zip,CC-BY-4.0,public,,,"Employment is defined as persons of working age who were engaged in any activity to produce goods or provide services for pay or profit, whether at work during the reference period or not at work due to temporary absence from a job, or to working-time arrangement.",,Government,employment,text,p3221,,,,,,05f05811ddf6ff7d3d05d53cf55120d476f0deabd22bd342c62aa7723ec4a5cc,,False
633,633,633,633,"Foreign direct investment, net inflows",https://datahub.io/world-bank/bx.klt.dinv.cd.wd/r/bx_klt_dinv_cd_wd_zip.zip,0.000313,6/15/2018,zip,CC-BY-4.0,public,,,"Foreign direct investment refers to direct investment equity flows in the reporting economy. It is the sum of equity capital, reinvestment of earnings, and other capital. Direct investment is a category of cross-border investment associated with a resident in one economy having control or a significant degree of influence on the management of an enterprise that is resident in another economy. ",,Banking,"investment, foreign",text,p3222,,,,,,bad11f0521f070619f3b29c2339ceee2eb8ef73bab2604f0275eb0983c0da33d,,False
634,634,634,634,"Wage and salaried workers, male",https://datahub.io/world-bank/sl.emp.work.ma.zs/r/sl_emp_work_ma_zs_zip.zip,0.000179,6/15/2018,zip,CC-BY-4.0,public,,,"Wage and salaried workers (employees) are those workers who hold the type of jobs defined as “paid employment jobs,” where the incumbents hold explicit (written or oral) or implicit employment contracts that give them a basic remuneration that is not directly dependent upon the revenue of the unit for which they work.",,Government,salary,text,p3223,,,,,,49a80f3196b45e8a3b4930cbc0b58c646982a38ebe8b84bfea499ebaef6bbfe6,,False
635,635,635,635,"Immunization, DPT (% of children ages 12-23 months)",https://datahub.io/world-bank/sh.imm.idpt/r/sh_imm_idpt_zip.zip,0.000144,6/15/2018,zip,CC-BY-4.0,public,,,"Child immunization, DPT, measures the percentage of children ages 12-23 months who received DPT vaccinations before 12 months or at any time before the survey.",,Health,immunization,text,p3224,,,,,,fb5fddbad353ff6c96b08bc1f0157286ef0fcd9a3d7bb6548ebf95793c8fe885,,False
636,636,636,636,Technical cooperation grants ,https://datahub.io/world-bank/bx.grt.tech.cd.wd/r/bx_grt_tech_cd_wd_zip.zip,0.000147,6/15/2018,zip,CC-BY-4.0,public,,,"Technical cooperation grants include free-standing technical cooperation grants, which are intended to finance the transfer of technical and managerial skills or of technology for the purpose of building up general national capacity without reference to any specific investment projects; and investment-related technical cooperation grants, which are provided to strengthen the capacity to execute specific investment projects. Data are in current U.S. dollars.",,Banking,"grants, technical",text,p3225,,,,,,65efb2e2e5c42e4f2800c1e4a0c18e652a9aa758075f4a2196e6114ad58103b4,,False
637,637,637,637,Goose,https://datahub.io/five-thirty-eight/goose/r/goose_zip.zip,0.002,6/15/2018,zip,,public,,,"This folder contains data behind the stories: The Save Ruined Relief Pitching, The Goose Egg Can Fix It; Kenley Jansen Is The Model Of A Modern Reliever.",,Media and Entertainment,"news, stories",text,p3226,,,,,,030d6c7fa62d35f0ab2036c992376d6359de1bd951cecc7c663270cc02311901,,False
638,638,638,638,Congress Trump Score,https://datahub.io/five-thirty-eight/congress-trump-score/r/congress-trump-score_zip.zip,0.002,6/15/2018,zip,,public,,,This file contains links to the data behind Tracking Congress In The Age Of Trump. This dataset was scraped from FiveThirtyEight - congress-trump-score,,Media and Entertainment,"Trump, score",text,p3227,,,,,,243404a4f762435d98b365fc37e4607d445497af6e5eb18dbb4e1ad4a2335dec,,False
639,639,639,639,Reluctant Trump,https://datahub.io/five-thirty-eight/reluctant-trump/r/reluctant-trump_zip.zip,0.001,6/15/2018,zip,,public,,,Files in this directory contain data from a SurveyMonkey News Survey conducted in partnership with FiveThirtyEight and include crosstabs of Trump voters who were excited to vote for Trump versus those who were not.,,Media and Entertainment,"Trump, score",text,p3228,,,,,,31f8f11a7a309a7f3f36d7fd31bd6b7b9cc7591c1c4ef115e98d567730d5ac75,,False
640,640,640,640,Diagnosed Diabetes Prevalence 2004-2013,https://datahub.io/JohnSnowLabs/diagnosed-diabetes-prevalence-2004-2013/r/datapackage_zip.zip,0.002,1/1/2018,zip,,public,,,Dataset includes information about diabetes diagnoses between 2004 and 2013.,,Health,diabetes,text,p3229,,,,,,1b561532fb48c1da77adefbcf27d682f987e80ff467e8d34bdf2fda176285612,,False
641,641,641,641,City Population Annual Timeseries,https://datahub.io/JohnSnowLabs/city-population-annual-timeseries/r/city-population-annual-timeseries_zip.zip,0.002,1/1/2018,zip,proprietary,public,,,Timeseries of cities' populations. ,,Government,population,text,p3230,,,,,,5ebb3581ca0e72058f921f054f91debb8fc51505fbf45ce1898a52d98f782f0a,,False
642,642,642,642,Speed Dating,https://datahub.io/machine-learning/speed-dating/r/speed-dating_zip.zip,0.001,6/1/2018,zip,,public,,,"This data was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four-minute “first date” with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again.",,Social Sciences,dating,text,p3231,,,,,,fea8d2c9f152ef231958798668cc2424ae17fac6db63155277d4e0f2ef052b50,,False
643,643,643,643,GDP per capita (current US$),https://datahub.io/world-bank/ny.gdp.pcap.cd/r/ny_gdp_pcap_cd_zip.zip,0.00043,6/15/2018,zip,CC-BY-4.0,public,,,GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.,,Government,GDP,text,p3232,,,,,,2f9a84d260f8aea1df0941287919e7c386c365f9c3220dfa6d4e5e89e5424dae,,False
644,644,644,644,"Inflation, GDP deflator (annual %)",https://datahub.io/world-bank/ny.gdp.defl.kd.zg/r/ny_gdp_defl_kd_zg_zip.zip,0.00041,6/15/2018,zip,CC-BY-4.0,public,,,Inflation as measured by the annual growth rate of the GDP implicit deflator shows the rate of price change in the economy as a whole. The GDP implicit deflator is the ratio of GDP in current local currency to GDP in constant local currency.,,Government,"inflation, GDP",text,p3233,,,,,,bf4bc8c9399df7f5b4b1b26cccced8e3cacd5b34c7b004a81ad1ec68d0cf022b,,False
645,645,645,645,Nba Elo,https://datahub.io/five-thirty-eight/nba-elo/r/nba-elo_zip.zip,0.015,6/25/2018,zip,,public,periodically,,This directory contains the data behind the Complete History Of The NBA interactive.,,Media and Entertainment,NBA,text,p3234,,,,,,debcccea9d6a1c5560b794be1543ce6ac983442099cdc19c50bc8834ee15642a,,False
646,646,646,646,Nba Carmelo,https://datahub.io/five-thirty-eight/nba-carmelo/r/nba-carmelo_zip.zip,0.01,6/25/2018,zip,,public,,,This file contains links to the data behind The Complete History Of The NBA and our NBA Predictions.,,Media and Entertainment,NBA,text,p3235,,,,,,57af82439af24d5a1b6bbd06804699cd4fb74e6f3397542639a95c970add76e3,,False
647,647,647,647,Health Insurance Plans Unified Rate Review,https://datahub.io/JohnSnowLabs/health-insurance-plans-unified-rate-review-puf-2015/r/datapackage_zip.zip,0.06,1/1/2018,zip,,public,,,"Review of health insurance plans, 2015. ",,Health,insurance,text,p3236,,,,,,a2c037e758b48ddbb78e46a95b610e115ad7b3d6bd4188c9f7b7d1e31367a145,,False
648,648,648,648,Brooklyn Public Library Catalog,https://datahub.io/JohnSnowLabs/brooklyn-public-library-catalog/r/datapackage_zip.zip,0.157,1/1/2018,zip,,public,,,Public library catalogue (2018),,Consumer Products,library,text,p3237,,,,,,db8a94852699e317dd9ae4a0bd1ea9db488dd853e109d61f51f6c785eed1f277,,False
649,649,649,649,Euro 3 Cars Emissions Traded On UK Market 2000-2007,https://datahub.io/JohnSnowLabs/euro-3-cars-emissions-traded-on-uk-market-2000-2007/r/euro-3-cars-emissions-traded-on-uk-market-2000-2007_zip.zip,0.001,1/1/2018,zip,,public,,,UK traded car emissions,,Consumer Products,trade,text,p3238,,,,,,29e96366405b859056e31972a381b1a1f1a94f18e6b75c64b57e8f1238983ea0,,False
650,650,650,650,Mammography Data from Breast Cancer Surveillance Consortium,https://datahub.io/JohnSnowLabs/mammography-data-from-breast-cancer-surveillance-consortium/r/mammography-data-from-breast-cancer-surveillance-consortium_zip.zip,0.001,1/1/2018,zip,,public,,,Mammography Data,,Health,,text,p3239,,,,,,f553ac2a0055aae55889237f7bbe7e7e3f21d5743e48d48299dcbec264202c1b,,False
651,651,651,651,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_xyz.tgz,0.47,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/xyz; Duration: 30.09s, Length: 7.112m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,2e15b49eeeb53cf6e54993c78d787cbf4af2c64d713dea32d0ccd7feabf2a435,,False
652,652,652,652,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_rpy.tgz,0.42,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/rpy; Duration: 27.67s, Length: 1.664m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,243a56fc870df2ac8e7fbbbe97521d4948efc5ea5c5906a0f24c673b36adc10c,,False
653,653,653,653,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_xyz.tgz,2.39,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/xyz; Duration: 122.74s, Length: 7.029m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,de5294056ce20dbf13afddfa02c8db10c6895b88e7d8f3538eae5fe66138e837,,False
654,654,654,654,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_rpy.tgz,2.13,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/rpy; Duration: 109.97s, Length: 1.506m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,8822a392cbad6ea33cd80019d72046c92931d262dce9f40f8af70e274f586752,,False
655,655,655,655,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_360.tgz,0.45,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/360; Duration: 28.69s, Length: 5.818m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,d3bcb4fd00c0588707558420e898110ffe7422d990525182ac3bbb63bd5e5f41,,False
656,656,656,656,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_floor.tgz,0.82,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/floor; Duration: 49.87s, Length: 12.569m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,8881aad4811bcfd9244220f45e12ef4d60a85bb8d319142fe7fd65a320f7faa1,,False
657,657,657,657,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk.tgz,0.36,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/desk; Duration: 23.40s, Length: 9.263m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,1c6a561b79b47d967f9567e617317d1c19f786718bac1383165211789ec176a9,,False
658,658,658,658,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk2.tgz,0.37,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/desk2; Duration: 24.86s, Length: 10.161m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,7f0fac87a858d26450a6c6540e559213a75218b7be65ff8a2b82876ef1b25860,,False
659,659,659,659,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room.tgz,0.83,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/room; Duration: 48.90s, Length: 15.989m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,a6a1af56b1b1b42be89b81ec33ecb6c5efeb4bdb059c0a7af7b2451d52466f7b,,False
660,660,660,660,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_hemisphere.tgz,1.5,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/360_hemisphere; Duration: 91.48s, Length: 14.773m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,e13bc16454cb08f641fcf63fa4ec2e914c5b693d0d19704a81d35fd4f21beada,,False
661,661,661,661,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_kidnap.tgz,0.74,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/360_kidnap; Duration: 48.04s, Length: 14.286m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,79c93f6d45e32a21cbae6aeff941e8dc5256e5f3d62ea1643815b548c8474e6b,,False
662,662,662,662,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk.tgz,2.01,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/desk; Duration: 99.36s, Length: 18.880m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,1369866433318c9e791fe2d5f5b6b9e8127fa7f1810b8926dcb4a3ed9d05b21e,,False
663,663,663,663,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_large_no_loop.tgz,1.92,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/large_no_loop; Duration: 112.37s, Length: 26.086m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,1840ef8db7e25a4ebbdfd732d715536bb18ec767791e06e129b2a02ddd8acddc,,False
664,664,664,664,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_large_with_loop.tgz,2.83,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/large_with_loop; Duration: 173.19s, Length: 39.111m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,6846c8c2e5879085f9170f75c981e4f18ba07fdd22eb76873de8b29e43b740c9,,False
665,665,665,665,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_long_office_household.tgz,1.58,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/long_office_household; Duration: 87.09s, Length: 21.455m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,8652e71b23e145868e929689eb90cf49e37bc9edba232f84875af3847a18cc48,,False
666,666,666,666,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_360.tgz,0.73,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_360; Duration: 72.75s, Length: 16.118m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,4939200b55190f91fec64b3d8c2c33626e6547be7bad730fd044cdc2d03bf8fe,,False
667,667,667,667,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_slam.tgz,1.82,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_slam; Duration: 155.72s, Length: 40.380m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,26db5f255a67de969de5514942ac2b1acb510b72c15221e84153391a3371c395,,False
668,668,668,668,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_slam2.tgz,1.33,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_slam2; Duration: 115.63s, Length: 21.735m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,2332fee14aff18f52bbb7da904c2c556312c6aaf5354b2c00ee30c8cb2b5e127,,False
669,669,669,669,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_slam3.tgz,1.6,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_slam3; Duration: 111.91s, Length: 18.135m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,a835f1855f9f7b5dc76e1a5a951ef2ee1d31bc0e58571f567146c0273f137e2b,,False
670,670,670,670,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_far.tgz,0.21,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_notexture_far; Duration: 15.79s, Length: 2.897m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,dd9f12dfa1429ec0786922f99ce5918e801be7d547f3fd7def3d28ce4e394736,,False
671,671,671,671,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_near_withloop.tgz,0.53,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_notexture_near_withloop; Duration: 37.74s, Length: 11.739m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,a73831655e4cf0378a490e2e59fa7028c325073fe931348c625fe35f088ed67e,,False
672,672,672,672,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_far.tgz,0.2,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_texture_far; Duration: 15.53s, Length: 4.343m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,8d4badae3e2ca2451da821f96e508946a72f7e70eb744446d218c713793a2c25,,False
673,673,673,673,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_near_withloop.tgz,0.79,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_texture_near_withloop; Duration: 56.48s, Length: 13.456m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,2e2c830854946022550af4521136b604061be16aa98fdbe30a960cee5659d3fb,,False
674,674,674,674,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_far.tgz,0.31,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_notexture_far; Duration: 27.28s, Length: 4.353m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,ad0e42a1005070a1256e6447d66eb984b06c2e1df41599da0f79d4889d01b426,,False
675,675,675,675,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_near.tgz,0.43,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_notexture_near; Duration: 36.44s, Length: 3.872m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,fbc1f119142cc139a5b212d09b49a69e463984075c776a1c0be615d8817389e0,,False
676,676,676,676,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_far.tgz,0.55,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_texture_far; Duration: 31.55s, Length: 5.884m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,41010c1a60ac06acb2a9c11a34b3f0614e4ce6e95c3d8a44d2e68133259c7ea2,,False
677,677,677,677,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_near.tgz,0.6,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_texture_near; Duration: 36.91s, Length: 5.050m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,01a4b1223f9e5d2676705bafcfa95e7868aad5a894d97ea2836f4c09161c7921,,False
678,678,678,678,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk_with_person.tgz,2.71,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/desk_with_person; Duration: 142.08s, Length: 17.044m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,41fb29ead6efd0ed9b1eda23530743634705f6db5cd8c6a640e9341fa33fbae3,,False
679,679,679,679,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_static.tgz,0.44,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_static; Duration: 23.63s, Length: 0.259m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,04966ef3ec3a66667f708a782eeafaa185e2717a33f540d70a16998e18b42f49,,False
680,680,680,680,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_xyz.tgz,0.79,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_xyz; Duration: 42.50s, Length: 5.496m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,b93521646f9d1e1c696aaac907d0486741672019e5595ffb7d43a0400e49f9b6,,False
681,681,681,681,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_halfsphere.tgz,0.66,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_halfsphere; Duration: 37.15s, Length: 6.503m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,4f0abc30390336e301b63107283bfd6ae723361d16bdc6b989e497f509f4631a,,False
682,682,682,682,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_rpy.tgz,0.49,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_rpy; Duration: 27.48s, Length: 1.110m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,b659b35db344c613787f0ccc6ec70e97181d6c4f752c394a0784dad77259fd18,,False
683,683,683,683,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_static.tgz,0.48,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_static; Duration: 24.83s, Length: 0.282m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,f8e54287f494d485b971e4da61d9af15764eec0b4b32132edfea9bd15fc20585,,False
684,684,684,684,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_xyz.tgz,0.54,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_xyz; Duration: 28.83s, Length: 5.791m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,0f3773e1d6530e8837ca25dd77e5db0f2c13719b57d7d96e0c0972533365b616,,False
685,685,685,685,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_halfsphere.tgz,0.65,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_halfsphere; Duration: 35.81s, Length: 7.686m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,ab13feff42d116520287618358c3b961641a4982540a543f2b70855472a747e3,,False
686,686,686,686,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_rpy.tgz,0.54,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_rpy; Duration: 30.61s, Length: 2.698m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,f218414fad77ab5bc436930569a2ba1f1e958b91012e832a68f4014298c1ebba,,False
687,687,687,687,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_plant.tgz,0.74,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/plant; Duration: 41.53s, Length: 14.795m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,f5097ca9ecacdd1aeda54fdf9d1d47c338a349375ba9b5c1870e8d0a62164775,,False
688,688,688,688,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_teddy.tgz,0.93,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/teddy; Duration: 50.82s, Length: 15.709m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,add3cd3534329ae02b7eeacf6d2051473bd89b097ee1902f337a0a180f00645b,,False
689,689,689,689,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_coke.tgz,1.43,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/coke; Duration: 84.55s, Length: 11.681m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,2003ea6a00dd1c934273cc23cebe69c9e3929cb537156f3f9bc6d69a589e1dd3,,False
690,690,690,690,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_dishes.tgz,1.58,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/dishes; Duration: 100.55s, Length: 15.009m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,92340e2d97f0fd5ec7fa13bb27a3e47f83088da40e654fb30f2ce11176ea7f48,,False
691,691,691,691,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_flowerbouquet.tgz,1.77,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/flowerbouquet; Duration: 99.40s, Length: 10.758m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,731b34b6cd77e197c4bb9f1a754ead4dece9d5676a1b00d1bac679df9d6a7dfb,,False
692,692,692,692,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_flowerbouquet_brownbackground.tgz,1.25,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/flowerbouquet_brownbackground; Duration: 76.89s, Length: 11.924m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,da935e6e577bd8eeb5f7474dc7f47f6a84039f79e40e814afcd85047456643c4,,False
693,693,693,693,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_metallic_sphere.tgz,1.29,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/metallic_sphere; Duration: 75.60s, Length: 11.040m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,bf607ed09b71215c4cfbf9e70164c1c0e95da20946b80991d5181322bd5c10c2,,False
694,694,694,694,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_metallic_sphere2.tgz,1.03,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/metallic_sphere2; Duration: 62.33s, Length: 11.813m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,6e89eae955d572d798886509e48b6c13747fd5a9df1bc1b669b6da9ec40012ab,,False
695,695,695,695,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_cabinet.tgz,0.52,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/cabinet; Duration: 38.58s, Length: 8.111m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,2b8f8d7bb85382d7a871d2e68382b3ca5e4436fa1ad0a0e33204f62b05b15213,,False
696,696,696,696,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_large_cabinet.tgz,0.48,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/large_cabinet; Duration: 33.98s, Length: 11.954m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,0310c9215296dd934ffffceebe7d7c8e5c45a912f65503baf5bf40d05ddbb54c,,False
697,697,697,697,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_teddy.tgz,1.3,v1,tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/teddy; Duration: 80.79s, Length: 19.807m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,335d699ba90d785aeb818d0e5aa7f7cf38866d4a277119e140c07cd797e0266b,,False
698,698,698,698,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_xyz_validation.tgz,0.57,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,e35c8abf8ebf5af5dc0a28e12ebd35ec00b92fa3fca32e1df904b97d15450e6c,,False
699,699,699,699,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_rpy_validation.tgz,0.56,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,340c076f9569746d4a6aa1e611095eb68b8c6aa2afa80919f2c847b4723f812e,,False
700,700,700,700,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk_validation.tgz,0.66,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/desk_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,256c9547c9f90f1718b761fa8f64630097684e10651697406d644e34de3e8562,,False
701,701,701,701,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk2_validation.tgz,0.4,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/desk2_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,e0cd558859e9924ec7cfb7c939b1d9c553240863bde550841793ef9f64b40db9,,False
702,702,702,702,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_360_validation.tgz,0.53,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/360_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,f6e55fa1293107c4dbbe40129601b4a7a26a1b2d166d707048334f3d19a64db0,,False
703,703,703,703,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room_validation.tgz,1.4,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/room_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,802a9f9569b87b891104a00e2f582012dce3e63a6eddbbffb3618c91259de379,,False
704,704,704,704,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_plant_validation.tgz,0.8,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/plant_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,c66bf80d25f77d00cee0a8f2d7bf12e469714a84ec071547c2fbd33a8fcf76cb,,False
705,705,705,705,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_xyz_validation.tgz,2.4,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,ec76283cf083d7bcd7682edfa2b70049f6163e89dc1d4882ef175207a6f79085,,False
706,706,706,706,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_rpy_validation.tgz,2.3,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,4c7960c568bc8fce73020ad2903e883cbd740d6506bea52c16f2c5a756523637,,False
707,707,707,707,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_hemisphere_validation.tgz,1.14,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/360_hemisphere_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,6384c299779bbda60d923e7d05c8f2c6b269b60965892f386162067d687e2615,,False
708,708,708,708,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_kidnap_validation.tgz,0.89,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/360_kidnap_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,30aa640b1a520f44f99d8fd784313c1b5fc6f202c821fb1abd8a2c8f6d0c8076,,False
709,709,709,709,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk_validation.tgz,2.26,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/desk_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,c38f14f7d9da6b18ddc1ec9a2e77e25c96103709e7c1b9fb2f8c76c551ac628d,,False
710,710,710,710,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk_with_person_validation.tgz,2.6,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/desk_with_person_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,74060bd9682c193320cb0a42ffbf1b226fff58215e2251567b0b2f3df400e9e3,,False
711,711,711,711,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_360_validation.tgz,1.25,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/pioneer_360_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,e79ed40c9d245144fc233f2897dd403e1e9ffcca8318b40cac0cc3ee189fc00d,,False
712,712,712,712,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_cabinet_validation.tgz,0.44,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/cabinet_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,bdc9531f37de2caebad7019a23600d2e192181ab786d38941bbf8fff68abb2db,,False
713,713,713,713,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_large_cabinet_validation.tgz,0.5,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/large_cabinet_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,8af34aa94fc48b836c5954eb34b7ff4ad711295bd6c54ef58e26b7a7299d436e,,False
714,714,714,714,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_long_office_household_validation.tgz,1.64,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/long_office_household_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,b5371165223c529a8d68a4f6c277b6613b82fd98c2d6ecb11e70639d8fa4cf85,,False
715,715,715,715,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_far_validation.tgz,0.17,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_notexture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,1b4f29131e2851682e1a3c08e18443944859d249a1e4947e3528eaba7bfe6bd0,,False
716,716,716,716,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_near_withloop_validation.tgz,0.5,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_notexture_near_withloop_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,88949791476b78b6d8c224fa72cad894f78f0f27174aaa05f60d85af9dd15d94,,False
717,717,717,717,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_far_validation.tgz,0.21,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_texture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,4de9265e166029a982e565c25a636994eb5153f40e9e772ca3b8e23723ef11f8,,False
718,718,718,718,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_near_withloop_validation.tgz,0.88,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_texture_near_withloop_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,cf9061d1ed469cce26c486c6285c0d101d45d6150b5a84cf6e0fef381ae9decb,,False
719,719,719,719,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_far_validation.tgz,0.36,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_notexture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,7b0d8fc897eb7e232d982c0548b76c248a7b7cf973a717f8a1bfa3df937d6d98,,False
720,720,720,720,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_near_validation.tgz,0.46,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_notexture_near_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,b1fe67002565768c691f5c4cb7be8b254dfb8ff042472302d4a5eb31323548af,,False
721,721,721,721,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_far_validation.tgz,0.46,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_texture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,d0ff3149e5dc2084ac6a621c223f1b4ef7278941c09ab0ae9200b3a1039b93e9,,False
722,722,722,722,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_near_validation.tgz,0.58,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_texture_near_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,5ddf226a2671b805e1a385146b9a366bd0c4fc850658e948c968e1b06955bb50,,False
723,723,723,723,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_static_validation.tgz,0.43,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_static_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,c0b5c782353c762a8d4bf45e1d75b925ac831a53d02cb09826b55b33e06922d3,,False
724,724,724,724,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_xyz_validation.tgz,0.54,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,1fbbe28ed292460358f478583bde77e11d96088e6145fdbef5811e39723ed617,,False
725,725,725,725,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_halfsphere_validation.tgz,0.59,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_halfsphere_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,59bb5be5ab32e2c0525c969b445ab2b01d87109e79b597a9bff368026ec7a168,,False
726,726,726,726,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_rpy_validation.tgz,0.47,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,521caf3fe90fe106c9c9ae2aa34b88a89f0a1ea991da0a3e67f387a8707b92c2,,False
727,727,727,727,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_static_validation.tgz,0.51,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_static_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,05f6b3b241dce71953bceac9bfb80544e12c789056f7d95945da3fed52f49985,,False
728,728,728,728,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_xyz_validation.tgz,0.57,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,e6449700b3d38966cb3d8f305b498c5bc3ab4eb73464884927fce64a40353e2d,,False
729,729,729,729,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_halfsphere_validation.tgz,0.74,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_halfsphere_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,44a5b83af6fc10006c439b41adc37e9af9b6951e3878f67b7521a0038d13bb7c,,False
730,730,730,730,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_rpy_validation.tgz,0.5,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,20acb426d95c8ec136e5410cc1b6e7a57d4adb6ffa6a21a0100d92fcf2ba4267,,False
731,731,731,731,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_rgb_calibration.tgz,0.98,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/rgb_calibration; Duration: 50.27s, Length: 0.003m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,f619d78edac9fe9a74ecc72747aea2976f82d7a11573919b6c68e9bdcd9b7830,,False
732,732,732,732,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_ir_calibration.tgz,2.02,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/ir_calibration,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,22ff7b76cce8caece2f9a69c83109a89c5a17d5349dc032edf24defb91b246f2,,False
733,733,733,733,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_large_checkerboard_calibration.tgz,0.99,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/large_checkerboard_calibration; Duration: , Length:",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,dfdb88be4a33c108cc18dc5e1595a31fe6d9f74287b5084b87285bea9e81adb3,,False
734,734,734,734,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_rgb_calibration.tgz,1.15,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/rgb_calibration; Duration: 65.77s, Length: 0.017m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,96f995d0e5732b7abbd449ceecc7a2bbad4f09c4934001ee1e8b62a4d4bd9288,,False
735,735,735,735,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_ir_calibration.tgz,2.16,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/ir_calibration,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,b8b2d227549e856196ff4f75e13285aad3365deaaaffe96ab583e10869e46d3c,,False
736,736,736,736,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_large_checkerboard_calibration.tgz,1.55,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/large_checkerboard_calibration; Duration: 65.02s, Length: 20.052m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,59929ca9bf958ddfe3de670c1d1bddd7e23cbe806f1717ec1ec837ed59e824ca,,False
737,737,737,737,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_calibration_rgb_depth.tgz,0.64,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/calibration_rgb_depth; Duration: 44.45s, Length: 0.005m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,7f44f8d3118e97668b71d30ec8b91dee8aa4243baaa4441105e4f1caafff0ece,,False
738,738,738,738,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_calibration_ir.tgz,2.39,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/calibration_ir,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,32206152c3d14f2717279b49bbf6db6ec1b24e78948346f2b9e08c552a5a87e1,,False
739,739,739,739,RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_checkerboard_large.tgz,0.82,v1,tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/checkerboard_large; Duration: 53.41s, Length: 19.745m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License,594bb6205ef9568210cab281d89a099a071f3883ab23e9e3e7121649419b552e,,False
740,740,740,740,Drinking Water Quality Distribution Monitoring Data,https://datahub.io/JohnSnowLabs/drinking-water-quality-distribution-monitoring-data/r/drinking-water-quality-distribution-monitoring-data_zip.zip,0.003,1/1/2018,zip,proprietary,,,,Drinking water analysis,,Life Sciences,water,text,p3240,,,,,,25398c7df20319c6d89711d523ed66877afad8980e9deaea40a2fd24acab54b5,,False
741,741,741,741,JunIBIS Data Package,https://datahub.io/andrejjh/junibis_data/r/junibis_data_zip.zip,0.000773,6/15/2018,zip,Creative Commons 4.0 Attribution License,public,,,"The Campaign of Belgium in 1815 marked the end of the Napoleonic wars. This dataset lists the armies and units in presence, their commanders and their hourly positions during the six days of campaign between June 15 and June 20. ",,Government,campaign,text,p3241,,,,,,987a172c34f914e6cef6fdf654183b3c97dc857d3b8ecbaa723d38747be8cf16,,False
742,742,742,742,"Primary completion rate, female",https://datahub.io/world-bank/se.prm.cmpt.fe.zs/r/se_prm_cmpt_fe_zs_zip.zip,0.000208,6/15/2018,zip,CC-BY-4.0,,,,"Primary completion rate, or gross intake ratio to the last grade of primary education, is the number of new entrants (enrollments minus repeaters) in the last grade of primary education, regardless of age, divided by the population at the entrance age for the last grade of primary education. ",,Education,"primary, completion",text,p3242,,,,,,d960c460080ac295ff02a7f704c7e713bc87042a69c99bab34e2aee0ea143e03,,False
743,743,743,743,"School enrollment, preprimary",https://datahub.io/world-bank/se.pre.enrr/r/se_pre_enrr_zip.zip,0.00024700000000000004,6/15/2018,zip,CC-BY-4.0,,,,"Gross enrollment ratio is the ratio of total enrollment, regardless of age, to the population of the age group that officially corresponds to the level of education shown. Preprimary education refers to programs at the initial stage of organized instruction, designed primarily to introduce very young children to a school-type environment and to provide a bridge between home and school.",,Education,preprimary,text,p3243,,,,,,b491b0946d0611854b20f4fb90c332593dc41591ce56916d886db465167003bf,,False
744,744,744,744,Age dependency ratio,https://datahub.io/world-bank/sp.pop.dpnd/r/sp_pop_dpnd_zip.zip,0.0005,6/15/2018,zip,CC-BY-4.0,,,,Age dependency ratio is the ratio of dependents–people younger than 15 or older than 64–to the working-age population–those ages 15-64. Data are shown as the proportion of dependents per 100 working-age population.,,Government,"research, age",text,p3244,,,,,,21b12eee17059b59dac83c6f9dc106b4f1fba892c291430efda76fad558ee76e,,False
745,745,745,745,Total natural resources rents (% of GDP),https://datahub.io/world-bank/ny.gdp.totl.rt.zs/r/ny_gdp_totl_rt_zs_zip.zip,0.000368,6/15/2018,zip,CC-BY-4.0,,,,"Total natural resources rents are the sum of oil rents, natural gas rents, coal rents (hard and soft), mineral rents, and forest rents.",,Oil and Gas,resources,text,p3245,,,,,,7ac8a835eb2fb9780e8bc691ef3255ef41273175875309a285c5a045d252b2b0,,False
746,746,746,746,Police Deaths,https://datahub.io/five-thirty-eight/police-deaths/r/police-deaths_zip.zip,0.004,6/25/2018,zip,,,,,This directory contains the data and code behind the story The Dallas Shooting Was Among The Deadliest For Police In U.S. History.,,Government,"death, police",text,p3246,,,,,,4bace92cb5778cae2d960348c17e2ebda7a618d0e5f361970420463a6ac67611,,False
747,747,747,747,NYS Math Test Results By Grade 2006-2011,https://datahub.io/JohnSnowLabs/nys-math-test-results-by-grade-2006-2011/r/datapackage_zip.zip,0.003,1/1/2018,zip,,,,,Math test results by grade. ,,Education,"math, test",text,p3247,,,,,,8dfa7b3e90d67aa8da179f82e997eb5eff48133cca174451de07b67bf5b45e21,,False
748,748,748,748,School Attendance 4PM Report,https://datahub.io/JohnSnowLabs/school-attendance-4pm-report/r/school-attendance-4pm-report_zip.zip,0.01,1/1/2018,zip,proprietary,,,,School attendance report,,Education,attendance,text,p3248,,,,,,fcda95366b06733c85eb1b3789a9331872be6c6848ecb35b0e458f9fa01cc779,,False
749,749,749,749,YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5914/soft/GDS5914_full.soft.gz,0.0084,10/17/2014,zip,,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,DataSet full SOFT file,P2002,,,,,,ede82f95d4bb00f7c408adf91022f57b0784b0122ab06cc67af35bc8b025f826,,False
750,750,750,750,YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5914/soft/GDS5914.soft.gz,0.0014,10/17/2014,zip,,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,DataSet SOFT file,P2002,,,,,,4003f86140859a0399d76ae267b0f9a31cf4e482f993a2790094e535802e0114,,False
751,751,751,751,YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61989/soft/GSE61989_family.soft.gz,0.0137,10/17/2014,zip,,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,Series family SOFT file,P2002,,,,,,1ce1d5e4565aa895e7d037285b50c091f8e5ad60f5152448fbfd0974dde2d6be,,False
752,752,752,752,YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61989/miniml/GSE61989_family.xml.tgz,0.0137,10/17/2014,zip,,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,Series family MINiML file,P2002,,,,,,7cacce9b2d76bc6fc422a05880dff0f0b6f4e2c6569c4927f201b235d479a0c9,,False
753,753,753,753,SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5913/soft/GDS5913_full.soft.gz,0.0096,6/17/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,DataSet full SOFT file,P2003,,,,,,b75c227f63f4d62d38827d42679d2ffe163fa9f07c32923d4456c781f7761ee1,,False
754,754,754,754,SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5913/soft/GDS5913.soft.gz,0.0015,6/17/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,DataSet SOFT file,P2003,,,,,,ea72bea4c63b41897864534e51265c634bacafc8fa58ff8bdabb2e1fd3502f40,,False
755,755,755,755,SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62947/soft/GSE62947_family.soft.gz,0.0134,6/17/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,Series family SOFT file,P2003,,,,,,9b21379a619ad4f4f533ff7e913ccad7cefedf8a222298e2d07cd81a258b064e,,False
756,756,756,756,SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62947/miniml/GSE62947_family.xml.tgz,0.0134,6/17/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,Series family MINiML file,P2003,,,,,,73c09fbc312e6096dfc6010062c4a48bdd413041b1e9397342f757d912b99fe9,,False
757,757,757,757,Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5881/soft/GDS5881_full.soft.gz,0.0081,6/25/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,DataSet full SOFT file,P2004,,,,,,1cf10140f6de15e3bd2460b25d78d69aad4dce43481970278b3d7d8aa820a9ef,,False
758,758,758,758,Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5881/soft/GDS5881.soft.gz,0.0018,6/25/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,DataSet SOFT file,P2004,,,,,,e241ada02b85169ccc09ac27139c32a0e7b09adc18ece4139977fb3833e64f87,,False
759,759,759,759,Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70213/soft/GSE70213_family.soft.gz,0.0164,6/25/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,Series family SOFT file,P2004,,,,,,d3bdd22c73c4f1c72a068bc73a1403040cfa8cbb59e46fc440bcc8d890447b08,,False
760,760,760,760,Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70213/miniml/GSE70213_family.xml.tgz,0.0164,6/25/2015,zip,,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,Series family MINiML file,P2004,,,,,,f1db19bbd2aa015921014e5035fa79fce6ed89816969e82193ea12967eaac846,,False
761,761,761,761,Nebulin deficiency effect on the quadriceps,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5880/soft/GDS5880_full.soft.gz,0.0089,6/25/2015,zip,,public,,,Analysis of quadriceps muscles from 6 week old males with a conditional knockout of nebulin in their striated muscles. Nebulin is a giant filamentous protein that is coextensive with actin filaments of the skeletal muscle sarcomere. Results provide insight into the role of nebulin in adult muscles.,,Health,Mus Musculus,DataSet full SOFT file,P2005,,,,,,ff57f2e327cc683630c43ae4af72bd2917f134ea9ea0e886509f712f7eaca035,,False
762,762,762,762,Nebulin deficiency effect on the quadriceps,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5880/soft/GDS5880.soft.gz,0.0018,6/25/2015,zip,,public,,,Analysis of quadriceps muscles from 6 week old males with a conditional knockout of nebulin in their striated muscles. Nebulin is a giant filamentous protein that is coextensive with actin filaments of the skeletal muscle sarcomere. Results provide insight into the role of nebulin in adult muscles.,,Health,Mus Musculus,DataSet SOFT file,P2005,,,,,,286b607ed1073a551dc26265c58ac6ef8639a3ea0aa4aab0c47ced1c35d08c68,,False
763,763,763,763,Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5879/soft/GDS5879_full.soft.gz,0.0059,8/11/2015,zip,,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,DataSet full SOFT file,P2006,,,,,,0d78a76f6a400252995504bb42426a0a077f436e09c2bd2c63a9b8025221e3ec,,False
764,764,764,764,Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5879/soft/GDS5879.soft.gz,0.000857,8/11/2015,zip,,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,DataSet SOFT file,P2006,,,,,,faa50c006fc5ce47f4ca1572cb73d7c47de4843b8220f0a1c80bdaf1fadce52f,,False
765,765,765,765,Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71868/soft/GSE71868_family.soft.gz,0.0067,8/11/2015,zip,,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,Series family SOFT file,P2006,,,,,,7f50999db0a8b405a2c42d31b874139abc2c950840ada32f7e3664e53ecbbecf,,False
766,766,766,766,Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71868/miniml/GSE71868_family.xml.tgz,0.0064,8/11/2015,zip,,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,Series family MINiML file,P2006,,,,,,fc691630f9a108704c2d2a9e9aaa836be0f037a3b2d23fe96d449f0609178e44,,False
767,767,767,767,Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6885/annot/GPL6885.annot.gz,0.0047,8/11/2015,zip,,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,Annotation SOFT file,P2006,,,,,,07d0c86c06a17ee3dc29b02380d20f9a4ebf03eba113bf94c1c2c3d354e17128,,False
768,768,768,768,	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5826/soft/GDS5826_full.soft.gz,0.0113,7/9/2015,zip,,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,DataSet full SOFT file,P2007,,,,,,0afbccd967dffa4b9f917e7e2cd6bef0239a4f35ace44bb5ef34b06fa14412ed,,False
769,769,769,769,	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5826/soft/GDS5826.soft.gz,0.0029,7/9/2015,zip,,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,DataSet SOFT file,P2007,,,,,,93c79c587fafcdcabb3e794143c8ff02a91e8f3ceac36f4d57000c44903af132,,False
770,770,770,770,	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE69nnn/GSE69078/soft/GSE69078_family.soft.gz,0.016,7/9/2015,zip,,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,Series family SOFT file,P2007,,,,,,24272979c466a62ab07daece4b9d249893b0439e76120d3c0d2b37925a8b7ea8,,False
771,771,771,771,	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE69nnn/GSE69078/miniml/GSE69078_family.xml.tgz,0.016,7/9/2015,zip,,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,Series family MINiML file,P2007,,,,,,29407ce523b965a47190504c60f73f8f3fa65820cf6544b7c39aed62b07d9364,,False
772,772,772,772,Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5825/soft/GDS5825_full.soft.gz,0.0087,6/27/2015,zip,,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,DataSet full SOFT file,P2008,,,,,,6c253f329af27964a95ba93f4b0f90d4c29c7a1b7fb28964f0996b805801731c,,False
773,773,773,773,Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5825/soft/GDS5825.soft.gz,0.0016,6/27/2015,zip,,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,DataSet SOFT file,P2008,,,,,,450054253d0225ef51517ae42101b34842fa8a372dc167f48d329f566fac793b,,False
774,774,774,774,Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70302/soft/GSE70302_family.soft.gz,0.0133,6/27/2015,zip,,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,Series family SOFT file,P2008,,,,,,22bbaf5560145106062d857540aeb8895359bfef42926e73e06d5bf9eb3da0fa,,False
775,775,775,775,Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70302/miniml/GSE70302_family.xml.tgz,0.0133,6/27/2015,zip,,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,Series family MINiML file,P2008,,,,,,380d494beccb17cccb19be6071c2fba7dc4ea021b0c8876d0cc33763b3a1867c,,False
776,776,776,776,Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5824/soft/GDS5824_full.soft.gz,0.0084,2/4/2015,zip,,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,DataSet full SOFT file,P2009,,,,,,e9a2470c5a1038fb9eb3d73fa793c8f4a3510500903562e951ae42903f5974cc,,False
777,777,777,777,Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5824/soft/GDS5824.soft.gz,0.0025,2/4/2015,zip,,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,DataSet SOFT file,P2009,,,,,,a581fe13ca45ed5daac56b36bb8b503f5592b10dd4935d2e3152681199b7d611,,False
778,778,778,778,Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65557/soft/GSE65557_family.soft.gz,0.0156,2/4/2015,zip,,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Series family SOFT file,P2009,,,,,,432e89a1803bdcc120817b0a7b107469646a2fc4199e2857168883a1ba014473,,False
779,779,779,779,Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65557/miniml/GSE65557_family.xml.tgz,0.0156,2/4/2015,zip,,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Series family MINiML file,P2009,,,,,,8c51f9824ba85e415fa907f9afe1fd708a9c3a1126dd4025e2f5ed0849fa0267,,False
780,780,780,780,	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5822/soft/GDS5822_full.soft.gz,0.006999999999999999,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,DataSet full SOFT file,P2011,,,,,,e01e332d3b4ab4c222f270f792b95f22a82c53737480436de379c5d56fce8b6b,,False
781,781,781,781,	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5822/soft/GDS5822.soft.gz,0.0012,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,DataSet SOFT file,P2011,,,,,,685771dec6923e2841b5829fa25456ee42c054aa8130d4df627a5f2448914ae0,,False
782,782,782,782,	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/soft/GSE52812_family.soft.gz,0.0163,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,Series family SOFT file,P2011,,,,,,e7d8337cfdf05c6570fb309e694425e0e6c626455c22f6de2e4d838a21cba45c,,False
783,783,783,783,	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/miniml/GSE52812_family.xml.tgz,0.0163,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,Series family MINiML file,P2011,,,,,,3381d271962020df8b87fdbd324d3df76358bb47f952562f72d77e441f1309f8,,False
784,784,784,784,αSMA+ myofibroblast late depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5821/soft/GDS5821_full.soft.gz,0.0074,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in late-stage PDAC.,,Health,Mus Musculus,DataSet full SOFT file,P2012,,,,,,4e723b1f6b10cf50994cb92abf2dba8e8daf8ce38065f6677428e3a72f38a876,,False
785,785,785,785,αSMA+ myofibroblast late depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5821/soft/GDS5821.soft.gz,0.000924,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in late-stage PDAC.,,Health,Mus Musculus,DataSet SOFT file,P2012,,,,,,0deb29210180353e362bb0b107758eea3e730576bd6299e5f8b38af85dcdef63,,False
786,786,786,786,αSMA+ myofibroblast early depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5820/soft/GDS5820_full.soft.gz,0.0074,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 4 to 4.5 weeks of age for 14 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in early-stage PDAC.,,Health,Mus Musculus,DataSet full SOFT file,P2013,,,,,,20e0c3357970bf354855bc8ba60350612acb54cf9a639c7e7ce7d122eb84b34a,,False
787,787,787,787,αSMA+ myofibroblast early depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5820/soft/GDS5820.soft.gz,0.00093,1/20/2015,zip,,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 4 to 4.5 weeks of age for 14 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in early-stage PDAC.,,Health,Mus Musculus,DataSet SOFT file,P2013,,,,,,0103aefcda7eb14adf9a286df877cf823514b177ab07abf74f03c1eec5d5c282,,False
788,788,788,788,Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5819/soft/GDS5819_full.soft.gz,0.0086,2/3/2015,zip,,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,DataSet full SOFT file,P2014,,,,,,6880178d7e8c85c3cf4481d8e1517af80f603149fcc9862ee52b64c48468447f,,False
789,789,789,789,Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5819/soft/GDS5819.soft.gz,0.0015,2/3/2015,zip,,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,DataSet SOFT file,P2014,,,,,,d9cc41a463af7b76030ba941117fb59063fbf1a51af7ef75575cc0fe209b55ba,,False
790,790,790,790,Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65517/soft/GSE65517_family.soft.gz,0.0176,2/3/2015,zip,,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,Series family SOFT file,P2014,,,,,,c5b60c3400f8dfdce25384b30b482100ac79d3d0cbefee7854804dd349c82597,,False
791,791,791,791,Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65517/miniml/GSE65517_family.xml.tgz,0.0179,2/3/2015,zip,,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,Series family MINiML file,P2014,,,,,,ff48e630e2d7d6d39807cc1cf1bf94e7fcb71be388fc56f60cd8c7863f31dd35,,False
792,792,792,792,Net ODA received per capita (current US$),https://datahub.io/world-bank/dt.oda.odat.pc.zs/r/dt_oda_odat_pc_zs_zip.zip,0.000402,6/20/2018,zip,CC-BY-4.0,,,,"Net official development assistance (ODA) per capita consists of disbursements of loans made on concessional terms (net of repayments of principal) and grants by official agencies of the members of the Development Assistance Committee (DAC), by multilateral institutions, and by non-DAC countries to promote economic development and welfare in countries and territories in the DAC list of ODA recipients; and is calculated by dividing net ODA received by the midyear population estimate.",,Government,Loans,text,p3149,,,,,,d25eac3bb162b0ca981eb4fdd7dfdb7fbba3960925307ea440fde985d9390315,,False
793,793,793,793,GDP growth (annual %),https://datahub.io/world-bank/ny.gdp.mktp.kd.zg/r/ny_gdp_mktp_kd_zg_zip.zip,0.000418,6/20/2018,zip,CC-BY-4.0,,,,Annual percentage growth rate of GDP at market prices based on constant local currency. Aggregates are based on constant 2010 U.S. dollars. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products.,,Government,GDP,text,p3150,,,,,,1e17e2dc8f49b2ea0fe71dc58e745042ca4c63bc4b4c26a02e58dee85394f55e,,False
794,794,794,794,Inconvenient Sequel,https://datahub.io/five-thirty-eight/inconvenient-sequel/r/inconvenient-sequel_zip.zip,0.002,6/20/2018,zip,,,,,This folder contains data behind the story Al Gore’s New Movie Exposes The Big Flaw In Online Movie Ratings.,,Media and Entertainment,"movie, rating",text,p3151,,,,,,64b7c369e6f3aec7b60df6d162cecb70b210d03ae0252414f4a7e3c37a67436d,,False
795,795,795,795,MNST Training Images,http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz,0.0099,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples.",,AI,digits,image,p3152,,,,,,246042d2f8b6ea50c41b951a6a4a4f9bd26cd64cc9045d5bb4d6c7830f2ac9a8,,False
796,796,796,796,MNST Training Labels,http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz,2.8000000000000013e-05,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples.",,AI,digits,text,p3152,,,,,,c28b19604a65a0c628bb1e279bcec25f0475465f2a76dc46983807e3c2535738,,False
797,797,797,797,MNST Test Images,http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz,0.00165,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a test set of 10,000 examples.",,AI,digits,image,p3152,,,,,,569754756c220c424ae33fb9585c39fef3fa94465ac21ad492f14a18f4f191cc,,False
798,798,798,798,MNST Test Labels,http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz,4.5e-06,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a test set of 10,000 examples.",,AI,digits,text,p3152,,,,,,5b7c9ecc84c42dbc97342888606b20617343c31d7b33cd02d575f63b4b0b0453,,False
799,799,799,799,CIFAR-100 ,https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz,0.161,,gzip,,,,,"This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs).",,AI,"photo, label",image,p3153,Computer Vision,,,,,e1b9ee79703153702bcfeed8e158b803f5b9a32d49ad1111c883c462f0965f29,,False
800,800,800,800,Flickr Audio Corpus,https://groups.csail.mit.edu/sls/downloads/flickraudio/downloads/flickr_audio.tar.gz,4.2,,gzip,,,,,"The Flickr 8k Audio Caption Corpus contains 40,000 spoken captions of 8,000 natural images. It was collected in 2015 to investigate multimodal learning schemes for unsupervised speech pattern discovery.",,AI,audio caption,audio,p3154,,,,,,63187d889c0bb646081fe7e23a0f6827d29f9baee07818c375ddebe0bbe4aff9,,False
801,801,801,801,Golos,http://webstructor.net/data/golos_texts_all.zip,14.34,,zip,,,,,Webstructor text data,,AI,large file,text,p3155,,,,,,c803c653d6573e9b94a7443b8c2a46ab86de5ac9158f890b8cb284612a7bf546,,False
802,802,802,802,Steemit,http://webstructor.net/data/steemit_texts_all.zip,2.63,,zip,,,,,Webstructor text data,,AI,text data ,text,p3156,,,,,,c447f90ca64f9252647b732cc7816309763038d38e7c94b9a524ed5d5f634f76,,False
803,803,803,803,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam1_1024_16.tar,0.8540000000000001,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,81e7bd1fbcf4b322093a23b3f72f2b876ad6a565cf9aa1095a9449241f237350,,False
804,804,804,804,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam2_1024_16.tar,0.5529999999999999,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,3296b2fa891d904da12cd21bb3d684b377e90cfdf23bb108b2d02fba825b18e6,,False
805,805,805,805,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam3_1024_16.tar,0.6579999999999999,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,82940e7cac13fd15616e797643d0e009026479462683cb041bf3c9af2891ce07,,False
806,806,806,806,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam4_1024_16.tar,0.508,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,eca4443e329f7c5b9caa9f45678de5e849d8e5b7ef11c6367df4dba5aced621a,,False
807,807,807,807,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam5_1024_16.tar,0.737,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,1afdf57b8acf4668b4484bb02d41adb037d4fc66557ea49689106cfc26270349,,False
808,808,808,808,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam6_1024_16.tar,0.6809999999999999,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,c3e2b5696058e3961d3bf1f52e72a26616b29007dee72a354e40818137c6f3ca,,False
809,809,809,809,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam7_1024_16.tar,0.29,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,fb7db8fc74272ed898b2b920bfe7904dd0a070a87cc9607a4271e89654495bde,,False
810,810,810,810,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam8_1024_16.tar,0.393,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,aac0e0057d34eb7dd26a80351cfb86776e74a9993bbd7231842f7dc904525cba,,False
811,811,811,811,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu1_1024_16.tar,1.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,205d728dceda23a9b9c6c6d6846045cba2033807f2e466a6f4a9e8b93bc1b5ca,,False
812,812,812,812,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu2_1024_16.tar,1.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,48fe333ceee8e07e9ba70726f2bbbdc2ea258f7ac4b6dc341a1177a1799ac216,,False
813,813,813,813,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu3_1024_16.tar,1.6,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,b3dc683ab6bbb75c44109c0e38a61766d54facb1016da7c7e6a767616efdc93f,,False
814,814,814,814,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu4_1024_16.tar,1.4,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,8ce26ec39a83dfdbfed9c471df585ffc76f180119e6c44dff4aa205b71c250cd,,False
815,815,815,815,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-vignette2_1024_16.tar,3.9,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,ae07a5dacebd05a500478662fa65b5b68dcc568c88e26d7c13f5df3013f2f8c1,,False
816,816,816,816,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-vignette3_1024_16.tar,2.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,8eaf0922f09d4ef762de60357063aa38987b2a3ba812fc7207565cc3d3510cf5,,False
817,817,817,817,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor1_1024_16.tar,9.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,79968a6dd3797642c4eb7ac083609628a6d4464a201deee909bcfe11fafdcc41,,False
818,818,818,818,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor2_1024_16.tar,11.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,15d04494a04c834ec3af081d85012edce2f8c0d403d3c8044d299b9f952c9112,,False
819,819,819,819,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor3_1024_16.tar,9.4,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,6be54d87070f509b88685c2a19acc6595421fb529bd30d827fac3ae50737f0e9,,False
820,820,820,820,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor4_1024_16.tar,3.1,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,112b33f22d6797e555eebdf5394dd5b41f51793ee1c340ac6b9ca64fce5a030b,,False
821,821,821,821,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor5_1024_16.tar,9.6,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,64829fadf113a39bc9765fa93252eb321bb43b5031df45dbbdbc2fbbaf98d565,,False
822,822,822,822,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale1_1024_16.tar,28.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,25c9726aa123d7977f28f201fc8409448d520a8bc7a9be9ab2d7af82a77eeb94,,False
823,823,823,823,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale2_1024_16.tar,20.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,0a8c5d89810edd6afc8e19e18f4c5f6d9b95fa47c3ef7c63dda6b87a63995d61,,False
824,824,824,824,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale3_1024_16.tar,18.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,f680f948e677440841bb44d3bf2bdcdc6213df90b61b37834393a33abd16a4cf,,False
825,825,825,825,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale4_1024_16.tar,21.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,dccccbbb8b5cc096ab5b8435656b8f26d07a5c5c12a23b8a45384a3bdb7fc181,,False
826,826,826,826,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale5_1024_16.tar,16.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,379866aa2d2c749f8e2c1878d9a9b2672eca7ed5b955a5dc6b0a126694925138,,False
827,827,827,827,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale6_1024_16.tar,19.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,1f1f6f6cdd89bb41b124f4c82e63f92c23cb54e91cd36e6a412928b541036a25,,False
828,828,828,828,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors1_1024_16.tar,49.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,810b235141e1cf61917df68727642bbb32facc9597a7c2c853f26c63debb0d04,,False
829,829,829,829,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors2_1024_16.tar,33.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,289a82a0bd4f7900972790f2c9124c056c48ab90f6d6f6ab562ec57108730fac,,False
830,830,830,830,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors3_1024_16.tar,29.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,50554d7fdf3fb17d9f1087c8594a1bf14573254d84a36541af90a6cf663118ae,,False
831,831,831,831,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors4_1024_16.tar,25.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,a97ce9d441db5247f658b78a3d3d7763c2f27698ee6e4a4091bd226f9630e4ff,,False
832,832,832,832,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors5_1024_16.tar,30.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,8d2fe4ba8e42da86c2a0be8f69f9e729b1821907c6521fd81fe3085b12f39b96,,False
833,833,833,833,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors6_1024_16.tar,57.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,14a2e03e53bc4675c798e3bf427a4c7564b885c550e03390d7140fb4a9564918,,False
834,834,834,834,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors7_1024_16.tar,39.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,61559d2ecd6faa26fca984bbea2c6b2e1d7acf9d14e6e89f87238c3fb201d6a7,,False
835,835,835,835,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors8_1024_16.tar,28.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,9f1322401f5ab293714bbb8f63f7c52bbf7ab458134db3e9a60f0b84d1ff0d20,,False
836,836,836,836,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room1_1024_16.tar,4.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,3a20c149703b393e63dda43db4a73df7ac7f75260f4ce385b060576381c3634a,,False
837,837,837,837,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room2_1024_16.tar,4.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,627e4dd7dfc9bf3c448782468aa276f188a2f1f544096018cceb8105bb75db83,,False
838,838,838,838,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room3_1024_16.tar,4.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,d2c3bf92cfd1cc1e5135db08d6adc55617dedc4fc92c68e126150e6aa82b0a97,,False
839,839,839,839,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room4_1024_16.tar,3.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,f2f9fa59ba889d0f15fb681d207d222735950348101c2eb0879633747fae4cb9,,False
840,840,840,840,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room5_1024_16.tar,4.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,3a1cd8b63bf50e61ca230727d965d80ff94ac9d784890f3776166f1f929bd3d6,,False
841,841,841,841,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room6_1024_16.tar,4.4,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,15b5d27c45cc8c1ebb748c95e9b52fb340c9b1a39767c9dcb287c57e3abf7ea9,,False
842,842,842,842,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides1_1024_16.tar,9.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,c0cd694119d6bc256efc5ee33b6911182c5bba70605c86ffcebe04f94ba1cffa,,False
843,843,843,843,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides2_1024_16.tar,8.5,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,3247a2f32ee7e7fbd9d6a6ca1b2690e40f803b8ae5b3f281fd8f9142dda18122,,False
844,844,844,844,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides3_1024_16.tar,12.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,18aad7a5134feae3c533fe09a8a9241285ea8c21d55b9c2061680f64e9fc353f,,False
845,845,845,845,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam1_512_16.tar,0.8540000000000001,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,ab50e1f5d932780631cf1ccef0a2cf83b1fea65416a7aaab38872a3c196d17ac,,False
846,846,846,846,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam2_512_16.tar,0.5529999999999999,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x514,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,5a040d47f84958319787a8f0c350757572665ca3408bbbec33e203812beeb6cd,,False
847,847,847,847,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam3_512_16.tar,0.6579999999999999,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x516,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,a5567dc15718a4aa1ce2af10c0ca0e5e08ce2ff7965cba1c8a3f8b6e689cf04c,,False
848,848,848,848,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam4_512_16.tar,0.508,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x518,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,f8c7c9f383926769771ce4bc42e8053c1740c9a561ede7c0a25d95e87697e73b,,False
849,849,849,849,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam5_512_16.tar,0.737,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x520,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,9dfde0cdbc32f354be55978970e7f0653e127b8d1c2fcabd45e8efb70b97953a,,False
850,850,850,850,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam6_512_16.tar,0.6809999999999999,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x522,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,1b18cfed893fe3ab72ae8d16cb9c6397c7067759dacff9469f8b906f84d25d52,,False
851,851,851,851,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam7_512_16.tar,0.29,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x524,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,9fc56f30008306f0ae40d8d6ad7381650029258512c1b5ba26e23a6a5264da28,,False
852,852,852,852,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam8_512_16.tar,0.393,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x526,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,a71e9ba065be6c7967b1ed5f3b35891b9ce5a46af3917ad1925dea3c7b424604,,False
853,853,853,853,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu1_512_16.tar,1.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x528,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,7fcb07559017db7bf85cd4c454f3024c378c3e92dc240e2f76b83b96d83c17a6,,False
854,854,854,854,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu2_512_16.tar,1.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x530,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,3d9e4f11111fb129768885445f624dcfb55c82f719a47429fb90b3f496901e89,,False
855,855,855,855,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu3_512_16.tar,1.6,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x532,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,767187f8fe86336cc1122f2cce77e3943088d2ea7a8d5f4d9b72784e325db1bd,,False
856,856,856,856,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu4_512_16.tar,1.4,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x534,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,602a3ec26802a4990c0a77e1e845edf22043a02f1ac20afa777ae2d314dc0ee1,,False
857,857,857,857,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-vignette2_512_16.tar,3.9,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x536,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,b3478cd955bffd01d38f904f6053aa70721f190305cc584762aa4798648167bb,,False
858,858,858,858,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-vignette3_512_16.tar,2.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x538,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,459073881eac23e9700a2c66ef7004c42d08d062359a98abe636c3d9a7675ba2,,False
859,859,859,859,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor1_512_16.tar,9.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x540,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,4870c0cae1126a0bb69d099e439aa5d5883a51c5cd733b8554472ae7aa021cf3,,False
860,860,860,860,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor2_512_16.tar,11.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x542,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,4ea109add02766b494d3ae9fc6c3f0581f6c5290e806cfcdc2d5fba6621af0a9,,False
861,861,861,861,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor3_512_16.tar,9.4,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x544,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,a81525209604dc850032d4009c54927bb98c4fe1e6139cdda3e883be446d320c,,False
862,862,862,862,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor4_512_16.tar,3.1,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x546,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,abc591d85fae4c6efe7c617566c9ecacd9d647ca0f5ba8e05571f8b558a1d4ed,,False
863,863,863,863,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor5_512_16.tar,9.6,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x548,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,fade522d28335a50809c170f6f29b027429cb26792884073eda94f85bdb36799,,False
864,864,864,864,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale1_512_16.tar,28.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x550,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,8ca3e1d59a77e5f29e2d249c3b88099048479f399fd2542d678599e0b44befe6,,False
865,865,865,865,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale2_512_16.tar,20.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x552,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,cd3aed6c08217b4b12e1c795bee98818783ecf1627ead35c852056d0982c8438,,False
866,866,866,866,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale3_512_16.tar,18.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x554,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,0a962563c9fba54ab2448af3ac3ea310abb515ceaf9d8a98e9b109e1290a9971,,False
867,867,867,867,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale4_512_16.tar,21.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x556,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,7e964bddfe4ba00255024309f7c1ff5db437984d8ad773e155ba788663a7ae22,,False
868,868,868,868,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale5_512_16.tar,16.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x558,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,4172d01cdbf7e1085539c92a1031fcfcd014c891f8bccbb973be11dc8caeb5f9,,False
869,869,869,869,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale6_512_16.tar,19.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x560,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,5c04e3be53542754072237cfc15172a0869d6f8d6d0728bf7437344ebd5ee85d,,False
870,870,870,870,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors1_512_16.tar,49.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x562,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,d727c3ac800ef369a540a17502b6cbb77b263dc06f36bd589916725b8b3aea83,,False
871,871,871,871,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors2_512_16.tar,33.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x564,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,383dc273479c5d31af2202b30983d1b2da90494e2024a528c8137f4f48635b4d,,False
872,872,872,872,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors3_512_16.tar,29.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x566,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,755f0d1471ed1703453df74dd2f81513718bd2c4707733f9933f6c63f9671552,,False
873,873,873,873,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors4_512_16.tar,25.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x568,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,5776025a43515b90e25ba60ff7fa9ad50c19fccc79b573cbba3886940b4ab6b2,,False
874,874,874,874,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors5_512_16.tar,30.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x570,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,dee5c4662211c8bcf7220478864a82a27e01d45385fd38f2fb2d9c7191cbc151,,False
875,875,875,875,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors6_512_16.tar,57.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x572,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,c40c719392a6c90544238e3b5cbb58f76e07715c1b1e4efd0fc77782fe1be651,,False
876,876,876,876,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors7_512_16.tar,39.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x574,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,bc4d3f7245cd4d336962d6e68d62cadfbbd903eab579b1533c56443369932c8f,,False
877,877,877,877,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors8_512_16.tar,28.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x576,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,72f6ae21ffccbd69c615272aecca5444600563dc37bf96a0087cf0c97d0ee328,,False
878,878,878,878,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room1_512_16.tar,4.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x578,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,2c2af3f7d8335308a89477064721129c649095c68f91b8aee03f4eddaad009a6,,False
879,879,879,879,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room2_512_16.tar,4.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x580,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,ad754b60716df75f455c655d50cbe795060a9aac83166fa2569819c6b88ed4b4,,False
880,880,880,880,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room3_512_16.tar,4.8,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x582,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,43387a539780ad85d3e8a2ff017c28f063f7b4b374f3cd84245e11c66aeaed38,,False
881,881,881,881,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room4_512_16.tar,3.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x584,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,975f439b103152f71f7b2d519dcd63e14674d57d564dd2790ea84ed3f45bd7f4,,False
882,882,882,882,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room5_512_16.tar,4.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x586,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,106545dbe20d00d346fdb5313d30259b64697d69cd7fbb9fd5d4901d57116554,,False
883,883,883,883,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room6_512_16.tar,4.4,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x588,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,62470a28c165decdfc624af0fc4e012ce5829005fa3f3e8e7b512a22ecce40f7,,False
884,884,884,884,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides1_512_16.tar,9.7,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x590,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,4bcc5bb93a8289418e73b261350ca550994a8608655b3f21229a4b37d84a2a70,,False
885,885,885,885,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides2_512_16.tar,8.5,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x592,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,a1d800aca352049752f42def3dbc2f86777758b5eb201370ea6577878595fc04,,False
886,886,886,886,Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides3_512_16.tar,12.0,v1,tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x594,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,,c7743d2068a0e1b4b9b3b1f43bce22345baadbfeb58fbdd83e8729c1fe08d03b,,False
