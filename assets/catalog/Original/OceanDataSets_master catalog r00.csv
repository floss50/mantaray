RecordName,Download Link,SizeGB,Version,Format,License,Classification,UpdateFrequency,LifecycleStage,Description,Note,industry,keywords ,Type,P-ID,category1,category2,category3,category4,source code License
Mapillar Mapillary Vistas Dataset,https://s3-eu-west-1.amazonaws.com/static.mapillary.com/MVD_research_samples.zip,0.015,v1,zip,proprietary,public,static,initial,A diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world,,,,,,,,,Research Edition,sample
Mapillar Mapillary Vistas Dataset,https://s3-eu-west-1.amazonaws.com/static.mapillary.com/MVD_commercial_samples.zip,0.015,v1,zip,proprietary,public,static,initial,A diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world,,,,,,,,,Commercial Edition,sample
CH2_002,https://github.com/udacity/self-driving-car/blob/master/datasets/CH2/Ch2_002.tar.gz.torrent,4.4,11/18/2016,torrent,MIT License,public,static,initial,ROSBAG training data with very similar driving conditions to CH2_001,,,,,,,,,,
CH2_001,https://github.com/udacity/self-driving-car/blob/master/datasets/CH2/Ch2_001.tar.gz.torrent,0.456,11/18/2016,torrent,MIT License,public,static,initial,Final Round Test Data: JPG and Filtered ROSBAG,,,,,,,,,,
CH03_002,https://github.com/udacity/self-driving-car/blob/master/datasets/CH3/CH03_002.bag.tar.gz.torrent,61,2/12/2016,torrent,MIT License,public,static,initial,Continuous recording of a down/back trip on El Camino Real with two GPS fix / IMU sources.,,,,,,,,,,
CH03_001,https://github.com/udacity/self-driving-car/blob/master/datasets/CH3/CH3_001.tar.gz.torrent,14.4,11/18/2016,torrent,MIT License,public,static,initial,"el_camino_north: 2766 seconds, Northbound drive on El Camino with enhanced IMU-based positioning available in '/fix' topic (Does not make it all the way to SF)",,,,,,,,,,
CHX_001,https://github.com/udacity/self-driving-car/blob/master/datasets/CHX/CHX_001.tar.gz.torrent,1.4,11/18/2016,torrent,MIT License,public,static,initial,"Lap around block at Udacity office with new HDL-32E LIDAR from George Hotz. Can almost create a loop-closure using NDT mapping, more sophisticated methods of SLAM should be able to create a high-fidelity map",,,,,,,,,,
Udacity Self-Driving Car,http://academictorrents.com/userdetails.php?id=5125,420,v1,torrent,MIT License,public,static,initial,Udacity training and test datasets,,,,,,,,,,
GloVe,https://nlp.stanford.edu/software/GloVe-1.2.zip,0.000745,10/1/2015,zip,Apache License 2.0,public,,,"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.",source code,communications,"linguistics, word, vector",text,P3000,,,,,
Wikipedia + Gigaword 5,http://nlp.stanford.edu/data/glove.6B.zip,0.82,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"6B tokens, 400k vocab, uncased, 50d, 100d, & 300d vectors",Electronics,Machine Learning,text,P3000,,,,,
Common Crawl,http://nlp.stanford.edu/data/glove.42B.300d.zip,1.75,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"42B tokens, 1.9M vocab, uncased, 300d vectors",Electronics,Machine Learning,text,P3000,,,,,
Common Crawl,http://nlp.stanford.edu/data/glove.840B.300d.zip,2.03,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"840B tokens, 2.2M vocab, cased, 300d vectors",Electronics,Machine Learning,text,P3000,,,,,
Twitter,http://nlp.stanford.edu/data/glove.twitter.27B.zip,1.42,v1,zip,Public Domain Dedication and Licence v1.0,public,,,Pre-trained word vector,"2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors",Electronics,Machine Learning,text,P3000,,,,,
U.S. Department of Agriculture's Nutrient Database,https://www.ars.usda.gov/ARSUserFiles/80400525/Data/SR/SR28/dnload/sr28asc.zip,0.006,5/1/2016,zip,N/A,public,static,,"Food descriptions, nutrient information, weights, and other measurements.",,Life Sciences,"vitamins, food, health",text,P3001,Agriculture,,,,
U.S. Department of Agriculture's Plant Database,https://www.plants.usda.gov/java/downloadData?fileName=plantlst.txt&static=true,0.0068,v1,txt,N/A,public,static,,"Plant checklist including symbol, synonym symbol, scientific name with Authors, National common name, and Family. ",,Life Sciences,"plants, earth, agriculture",text,p3002,Agriculture,,,,
E-MTAB-6854,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6854/E-MTAB-6854.processed.1.zip,0.324,6/25/2018,zip,N/A,public,static,,Single-cell RNA-seq data of mammary gland epithelial cells from different gestational stages to detect and remove barcode swapping,,Life Sciences,"mus musculus, RNA, cells",text,p3003,Biology,,,,
E-MTAB-6051,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6051/E-MTAB-6051.processed.1.zip,0.056,6/25/2018,zip,N/A,public,static,,Single-cell RNA sequencing of OT-I CD8+ T cells after stimulation with different affinity ligands,,Life Sciences,"mus musculus, RNA",text,p3004,Biology,,,,
E-MTAB-6131,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6131/E-MTAB-6131.processed.1.zip,1.15,6/24/2018,zip,N/A,public,static,,Methylation array for Multi-omics molecular profiling of primary prostate adenocarcinoma,,Life Sciences,"Homo sapiens, Methylation",text,p3005,Biology,,,,
E-MTAB-6888,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6888/E-MTAB-6888.processed.1.zip,0.005,6/22/2018,zip,N/A,public,static,,Genome-wide maps of nuclear lamina interactions in HeLa cells identified using MethylAdenine IDentification (MadID),,Life Sciences,Homo sapiens,text,p3006,Biology,,,,
E-MTAB-6855,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6855/E-MTAB-6855.processed.1.zip,0.012,6/22/2018,zip,N/A,public,static,,"Gene expression profiles of human breast cancer cell lines MCF-7, T47D and MDA-MB-231 co-cultured with 3T3-L1 adipocytes",,Life Sciences,"Homo sapiens, profiling",text,p3007,Biology,,,,
E-MTAB-6843,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6843/E-MTAB-6843.processed.1.zip,0.075,6/21/2018,zip,N/A,public,static,,Single cell RNA-sequencing of T cells to examine barcode swapping,,Life Sciences,"Mus musculus, RNA, cells",text,p3008,Biology,,,,
E-MTAB-6063,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6063/E-MTAB-6063.processed.1.zip,0.000266,6/20/2018,zip,N/A,public,static,,microRNA array on plasma from two diffuse large B-cell lymphoma patients and one healthy donor,,Life Sciences,"Homo sapiens, microRNA",text,p3009,Biology,,,,
E-MTAB-6503,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6503/E-MTAB-6503.processed.1.zip,0.0001,6/17/2018,zip,N/A,public,static,,ChIP-seq of Mycobacterium smegmatis PafBC to identify in vivo binding sites,,Life Sciences,"Mycobacterium, ChIP-seq",text,p3010,Biology,,,,
E-MTAB-6497,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6497/E-MTAB-6497.processed.1.zip,0.006,6/17/2018,zip,N/A,public,static,,RNA-seq of Mycobacterium smegmatis mc2-155 SMR5 wild-type vs. a pafBC deletion strain under different growth/stress conditions,,Life Sciences,"Mycobacterium, RNA-seq",text,p3011,Biology,,,,
E-MTAB-6315,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6315/E-MTAB-6315.processed.1.zip,0.0015,6/8/2018,zip,N/A,public,static,,Methylation arrays (MethylationEPIC) reveal specific DNA hypermethylation of T cells during human hematopoietic differentiation,,Life Sciences,"Homo sapiens, methylation",text,p3012,Biology,,,,
E-MTAB-6311,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6311/E-MTAB-6311.processed.1.zip,0.0017,6/8/2018,zip,N/A,public,static,,Microarray analysis of transcription changes in zebrafish embryos of different developmental ages after exposure to cyanide,,Life Sciences,"Danio rerio, transcription profiling",text,p3013,Biology,,,,
Leishmania major,http://geneontology.org/gene-associations/gene_association.GeneDB_Lmajor.gz,0.000197,v1,gzip,N/A,public,static,,"L. major, is a protozoan parasite that causes cutaneous leishmaniasis. It is a representative genome for all the species of Leishmania. Leishmania, possesse a two-unit genome, a nuclear genome and a mitochondrial kinetoplast. The nuclear genome is divided into 36 chromosomes with a combined haploid size of 33.6Mb.  ",,Life Sciences,,text,p3014,Biology,,,,
Plasmodium falciparum,http://geneontology.org/gene-associations/gene_association.GeneDB_Pfalciparum.gz,0.00018,v1,gzip,N/A,public,static,,Plasmodium falciparum gene information,,Life Sciences,,text,p3015,Biology,,,,
Trypanosoma brucei,http://geneontology.org/gene-associations/gene_association.GeneDB_Tbrucei.gz,0.0005,v1,gzip,N/A,public,static,,"T. brucei, the protozoan responsible for African Sleeping Sickness, possesses a two-unit genome, a nuclear genome and a mitochondrial (kinetoplast) genome with a total estimated size of 35Mb/haploid genome. The nuclear genome is split into three classes of chromosomes according to their size on pulse filed gel electrophoresis, 11 megabase chromosomes (0.9-5.7 Mb), intermediate (300-900 kb) and minichromosomes (50-100 kb). ",,Life Sciences,"genome, chromosomes",text,p3016,Biology,,,,
Agrobacterium tumefaciens str. C58,http://geneontology.org/gene-associations/gene_association.PAMGO_Atumefaciens.gz,0.000005,v1,gzip,N/A,public,static,,This dataset contains the annotation of Agrobacterium tumefaciens C58 using GeneOntology (GO) terms.,,Life Sciences,"genome, evidence, annotation",text,p3017,Biology,,,,
Dickeya dadantii,http://geneontology.org/gene-associations/gene_association.PAMGO_Ddadantii.gz,0.000006,v1,gzip,N/A,public,static,,These annotations are the result of a directed literature search aimed at annotating known and putative virulence factors of Dickeya dadantii.  Annotations were done by Dr. Brad Anderson in the laboratory of Dr. Nicole Perna at the University of Wisconsin.,,Life Sciences,"genome, annotations",text,p3018,Biology,,,,
Magnaporthe grisea,http://geneontology.org/gene-associations/gene_association.PAMGO_Mgrisea.gz,0.000583,v1,gzip,N/A,public,static,,These annotations are based on both experimental data from scientific literature and orthologs identified by stringent computational approaches. Annotations were done by the Magnaporthe grisea genome annotation team at North Carolina State University.,,Life Sciences,"genome, annotations",text,p3019,Biology,,,,
Oomycetes,http://geneontology.org/gene-associations/gene_association.PAMGO_Oomycetes.gz,0.000001,v1,gzip,N/A,public,static,,These annotations are based solely on experimental data from scientific literature. Annotations are made by the Oomycete Genome Annotation team at the Virginia Bioinformatics Institute.,,Life Sciences,"genome, annotations",text,p3020,Biology,,,,
Aspergillus nidulans,http://geneontology.org/gene-associations/gene_association.aspgd.gz,0.006,v1,gzip,N/A,public,static,,Contains all GO annotations for Aspergillus nidulans genes (protein and RNA),,Life Sciences,"genome, genes",text,p3021,Biology,,,,
Candida albicans,http://geneontology.org/gene-associations/gene_association.cgd.gz,0.004,v1,gzip,N/A,public,static,,Contains all GO annotations for C. albicans genes (protein and RNA),,Life Sciences,genome,text,p3022,Biology,,,,
Dictyostelium discoideum,http://geneontology.org/gene-associations/gene_association.dictyBase.gz,0.002,v1,gzip,N/A,public,static,,This file contains all GO annotations for Dictyostelium discoideum gene products (protein and RNA) in dictyBase.,,Life Sciences,"ontology, genes",text,p3023,Biology,,,,
Escherichia coli,http://geneontology.org/gene-associations/gene_association.ecocyc.gz,0.000641,v1,gzip,N/A,public,static,,Contains all GO annotations for Escherichia coli K-12.,,Life Sciences,"E-coli, genes, ontology",text,p3024,Biology,,,,
Drosophila melanogaster,http://geneontology.org/gene-associations/gene_association.fb.gz,0.004,v1,gzip,N/A,public,static,,Currently all GO annotations in FlyBase are attributed to genes. The GO terms describe the attributes of the products (both RNA and protein) encoded by these Drosophila genes.,,Life Sciences,"genes, annotations",text,p3025,Biology,,,,
Gallus gallus,http://geneontology.org/gene-associations/goa_chicken.gaf.gz,0.004,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"chicken, genes, annotations",text,p3026,Biology,,,,
Gallus gallus (complex),http://geneontology.org/gene-associations/goa_chicken_complex.gaf.gz,0.000016,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"chicken, genes, annotations",text,p3027,Biology,,,,
Gallus gallus (rna),http://geneontology.org/gene-associations/goa_chicken_rna.gaf.gz,0.000032,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"chicken, genes, annotations",text,p3028,Biology,,,,
Bos taurus,http://geneontology.org/gene-associations/goa_cow.gaf.gz,0.002,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"cow, genes, annotations",text,p3029,Biology,,,,
Bos taurus (complex),http://geneontology.org/gene-associations/goa_cow_complex.gaf.gz,0.000004,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"cow, genes, annotations",text,p3030,Biology,,,,
Bos taurus (rna),http://geneontology.org/gene-associations/goa_cow_rna.gaf.gz,0.000362,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"cow, genes, annotations",text,p3031,Biology,,,,
Canis lupus familiaris,http://geneontology.org/gene-associations/goa_dog.gaf.gz,0.002,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"dog, genes, annotations",text,p3032,Biology,,,,
Canis lupus familiaris (complex),http://geneontology.org/gene-associations/goa_dog_complex.gaf.gz,0.000316,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"dog, genes, annotations",text,p3033,Biology,,,,
Canis lupus familiaris (rna),http://geneontology.org/gene-associations/goa_dog_rna.gaf.gz,0.00018,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"dog, genes, annotations",text,p3034,Biology,,,,
Homo sapiens,http://geneontology.org/gene-associations/goa_human.gaf.gz,0.009,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"human, genes, annotations",text,p3035,Biology,,,,
Homo sapiens (complex),http://geneontology.org/gene-associations/goa_human_complex.gaf.gz,0.000033,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"human, genes, annotations",text,p3036,Biology,,,,
Homo sapiens (rna),http://geneontology.org/gene-associations/goa_human_rna.gaf.gz,0.000378,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"human, genes, annotations",text,p3037,Biology,,,,
Sus scrofa,http://geneontology.org/gene-associations/goa_pig.gaf.gz,0.002,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"pig, genes, annotations",text,p3038,Biology,,,,
Sus scrofa (complex),http://geneontology.org/gene-associations/goa_pig_complex.gaf.gz,0.000008,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"Pig, genes, annotations",text,p3039,Biology,,,,
Sus scrofa (rna),http://geneontology.org/gene-associations/goa_pig_rna.gaf.gz,0.000114,v1,gzip,N/A,public,static,,"In the GOA project, this vocabulary is applied to all proteins described in the UniProt (Swiss-Prot and TrEMBL) Knowledgebase, and to RNAs and macromolecular complexes, identified by RNAcentral and Complex Portal identifiers, respectively, to create ""annotations"", which are provided in the UniProt annotation files.",,Life Sciences,"Pig, genes, annotations",text,p3040,Biology,,,,
Human Genome Diversity Project,http://www.hagsc.org/hgdp/data/hgdp.zip,2.08,v1,zip,N/A,public,static,,"The genotypes were generated on Illumina 650Y arrays, with a GenCall Score cutoff of 0.25. Samples with an overall call rate < 98.5% were removed. No filtering of SNPs was done, so be aware that low quality SNPs are included in these files.",,Life Sciences,"genome, chromosomes",text,p3041,Biology,,,,
Actuaries Climate Index,http://actuariesclimateindex.org/wp-content/uploads/2018/04/Actuaries-Climate-Index-Values-Through-August-2017_Summer-2017_English.xlsx,0.003,v1,xlsx,proprietary,public,static,,Monthly and seasonal climate index data.,,Earth Sciences,"climate, seasons",text,p3042,Climate,,,,
Berling Climate NetCDF ,https://www.beringclimate.noaa.gov/cache/5b322cddd36bc.zip,0.000217,v1,zip,proprietary,public,static,,"Climate indices, atmosphere, ocean, fishery, biology, and sea ice data files. ",,Earth Sciences,"climate, ocean, atmosphere, temperature",text,p3043,Climate,,,,
The World Bank Climate Change Data,http://databank.worldbank.org/data/download/catalog/climate_change_download_0.xls,0.0053,v1,xls,proprietary,public,static,,"Data from World Development Indicators and Climate Change Knowledge Portal on climate systems, exposure to climate impacts, resilience, greenhouse gas emissions, and energy use.","Temporal Coverage: 1990-2011, Time-Series",Earth Sciences,"climate change, location, temperature",text,p3044,Climate,,,,
CrossRef DOI URLs,https://archive.org/compress/doi-urls,0.5,1/1/2014,zip,proprietary,public,static,,DOI URLs from 2007-2013,,Electronics,URL,text,p3045,Networks,,,,
College Scorecard,https://ed-public-download.app.cloud.gov/downloads/CollegeScorecard_Raw_Data.zip,0.235,3/29/2018,zip,proprietary,public,,,"Data that appear on the College Scorecard, as well as supporting data on student completion, debt and repayment, earnings, and more. The files include data from 1996 through 2016 for all undergraduate degree-granting institutions of higher educatio",,Education,College,text,p3046,University,,,,
The Almanac of Minutely Power dataset ,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FIE0S4#,1,5/17/2016,zip,Creative Commons Attribution License,public,,,"This dataset contains electricity, water, and natural gas measurements at one minute intervals. This dataset contains a total of 1,051,200 readings for 2 years of monitoring (from April/2012 to March/2014) per meter. There are a total of 21 power meters, 2 water meters (with additional appliance usage annotations), and 2 natural gas meters.",,Energy and Utilities,Power,text,p3047,,,,,
Commercial Building Energy Dataset,http://combed.github.io/downloads/combed.zip,0.207,v1,zip,proprietary,public,,,COMBED contains a month of smart meter data collected from different sensing points in IIITD's academic building.,,Energy and Utilities,smart meter,text,p3048,,,,,
Global Power Plant Database,http://datasets.wri.org/dataset/540dcf46-f287-47ac-985d-269b04bea4c6/resource/27c271ef-63c3-49c5-a06a-f21bb7b96371/download/globalpowerplantdatabasev110,0.0077,6/1/2018,zip,proprietary,public,continuous,,"The Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights for one’s own analysis. Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. ",,Energy and Utilities,"plant capacity, generation",text,p3049,Power Plant,,,,
High Frequency EMI Data Set,https://goo.gl/NerkK1,0.014,v1,zip,N/A,public,,,"This work highlights an extensive empirical study of conducted EMI, performed on a set of 24 loads with 4 different test setups in lab settings and with one test setup in home settings.",Lab Setting,Energy and Utilities,"EMI, experimental",text,p3050,,,,,
PLAID:  Plug Load Appliance Identification Dataset,http://plaidplug.com/static/dataset/Plaid.tar.gz,0.031,v1,gzip,proprietary,public,,,"PLAID currently includes current and voltage measurements sampled at 30 kHz from 11 different appliance types present in more than 60 households in Pittsburgh, Pennsylvania, USA. Data collection took place during the summer of 2013, and winter of 2014. Each appliance type is represented by dozens of different instances of varying make/models. For each appliance, three to six measurements were collected for each state transition.",,Energy and Utilities,,text,p3051,,,,,
Tracebase,https://github.com/areinhardt/tracebase/archive/master.zip,2.7,v1,zip,Open Database License,public,,,"The tracebase data set is a collection of power consumption traces which can be used in energy analytics research. Traces have been collected from individual electrical appliances, at an average reporting rate of one sample per second.",,Energy and Utilities,,text,p3052,,,,,
Cricsheet,https://cricsheet.org/downloads/all.zip,0.466,v1,zip,N/A,public,,,"Ball-by-ball data for Men’s and Women’s Test Matches, One-day internationals, Twenty20 Internationals, some other international T20s, and all Indian Premier League seasons",,Media and Entertainment,cricket,text,p3053,sports,,,,
Ergast,http://ergast.com/downloads/f1db_csv.zip,0.0165,6/24/2018,zip,Creative Commons Attribution Licence,public,continuous,,"Race results, driver standings, lap times, pit stops, circuits, seasons, etc.",,Media and Entertainment,,"image, text",p3054,Sports,,,,
Pinhooker: Thoroughbread Bloodstock Sale Data,https://github.com/phillc73/pinhooker/archive/master.zip,0.0045,v1,zip,N/A,public,,,Historic thoroughbred bloodstock sales data.,,Media and Entertainment,horse,text,p3055,Sports,,,,
Retrosheet Baseball Statistics,http://www.retrosheet.org/events/allas.zip,0.002,v1,zip,N/A,public,,,Complete set of All-Star baseball games,,Media and Entertainment,baseball,text,p3056,Sports,,,,
Retrosheet Baseball Statistics,http://www.retrosheet.org/events/allpost.zip,0.011,v1,zip,N/A,public,,,Complete set of Post-Season baseball games,,Media and Entertainment,baseball,text,p3056,Sports,,,,
"ATP Tennis Rankings, Results, and Stats",https://github.com/JeffSackmann/tennis_atp/archive/master.zip,0.207,8/4/2015,zip,Creative Commons Attribution License,public,,,"ATP player information including id, name, hand, birth date, and country code. Rankings include date, ranking, player id, and ranking points. The ATP match results feature stats and additional player information.",,Media and Entertainment,tennis,text,p3057,Sports,,,,
"WTA Tennis Rankings, Results, and Stats",https://github.com/JeffSackmann/tennis_wta/archive/master.zip,0.12,5/28/2018,zip,Creative Commons Attribution License,public,,,"WTA player information including id, name, hand, birth date, and country code. Rankings include date, ranking, player id, and ranking points. The WTA match results feature stats and additional player information.",,Media and Entertainment,tennis,text,p3058,Sports,,,,
Airline On-Time Statistics and Delay Causes,https://www.transtats.bts.gov/OT_Delay/ot_delaycause1.asp?display=download&pn=0&month=4&year=2018,0.042,v1,zip,N/A,public,,,"Bureau of Transportation Statistics: On-Time Arrival Performance National (June, 2003 - April, 2018)",,Travel and Transportation,"airline, flight, statistics",text,p3059,Airline,,,,
201801 Ford GoBike ,https://s3.amazonaws.com/fordgobike-data/201801-fordgobike-tripdata.csv.zip,0.003,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,
201802 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201802-fordgobike-tripdata.csv.zip,0.0038,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,
201803 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201803-fordgobike-tripdata.csv.zip,0.004,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,
201804 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201804-fordgobike-tripdata.csv.zip,0.0048,5/16/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,
201805 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/201805-fordgobike-tripdata.csv.zip,0.0064,6/8/2018,zip,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,
2017 Ford GoBike,https://s3.amazonaws.com/fordgobike-data/2017-fordgobike-tripdata.csv,0.117,3/15/2018,csv,Ford GoBike Data License Agreement,public,continuous,,"Ford GoBike's anonymized trip data includes trip duration, bike id, location data, user type, and member information.",,Travel and Transportation,Bike,text,p3060,biking,,,,
Divvy 2013 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Stations_Trips_2013.zip,0.094,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2014 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Stations_Trips_2014_Q1Q2.zip,0.006,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2014 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Stations_Trips_2014_Q3Q4.zip,0.0064,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2015 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2015-Q1Q2.zip,0.0066,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2015 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2015_Q3Q4.zip,0.012,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2016 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2016_Q1Q2.zip,0.005,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2016 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2016_Q3Q4.zip,0.014,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2017 Q1 & Q2,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2017_Q1Q2.zip,0.006,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
Divvy 2017 Q3 & Q4,https://s3.amazonaws.com/divvy-data/tripdata/Divvy_Trips_2017_Q3Q4.zip,0.017,v1,zip,Divvy Data License Agreement,public,bi-yearly,,"Divvy's trip data for policy makers, transportation professionals, web developers, designers, and the general public. Each trip is anonymized and includes information about time, location, and rider type.",,Travel and Transportation,Bike-sharing,text,p3061,biking,,,,
2010 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2010-capitalbikeshare-tripdata.zip,0.0024,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2011 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2011-capitalbikeshare-tripdata.zip,0.025,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2012 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2012-capitalbikeshare-tripdata.zip,0.043,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2013 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2013-capitalbikeshare-tripdata.zip,0.056,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2014 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2014-capitalbikeshare-tripdata.zip,0.066,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2015 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2015-capitalbikeshare-tripdata.zip,0.073,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2016 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2016-capitalbikeshare-tripdata.zip,0.078,3/16/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2017 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/2017-capitalbikeshare-tripdata.zip,0.09,3/15/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
201801 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201801-capitalbikeshare-tripdata.zip,0.004,4/30/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
201802 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201802-capitalbikeshare-tripdata.zip,0.004,5/11/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
201803 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201803-capitalbikeshare-tripdata.zip,0.0055,5/11/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
201804 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201804-capitalbikeshare-tripdata.zip,0.0075,5/11/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
201805 Capital Bikeshare Trip Data,https://s3.amazonaws.com/capitalbikeshare-data/201805-capitalbikeshare-tripdata.zip,0.0085,6/8/2018,zip,Capital Bikeshare Data License Agreement,public,static,,"Bikeshare trip data for developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization or whatever else moves you",Washington DC Metro Area,Travel and Transportation,Bike-sharing,text,p3062,biking,,,,
2010 Nice Ride data ,https://niceridemn.egnyte.com/dl/byJLtGzvHM,0.0014,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
2011 Nice Ride data ,https://niceridemn.egnyte.com/dl/8xAYjDuS3L,0.0029,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
2012 Nice Ride data ,https://niceridemn.egnyte.com/dl/GlYmbU2Bh0,0.0038,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
2013 Nice Ride data ,https://niceridemn.egnyte.com/dl/kdJ4WP0mHC,0.0044,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
2014 Nice Ride data ,https://niceridemn.egnyte.com/dl/MZxvOEELWQ,0.0056,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
2015 Nice Ride data ,https://niceridemn.egnyte.com/dl/9nSKfEfxQ8,0.0059,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
2016 Nice Ride data ,https://niceridemn.egnyte.com/dl/gYLZtGrwEk,0.0055,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
2017 Nice Ride data ,https://niceridemn.egnyte.com/dl/QrR5Ih5Xeq,0.0058,v1,zip,Nice Ride Minnesota Data License Agreement,public,static,,"Minnesota Nice Ride bikeshare data for public use. Data includes station information, bike status, and system specifications. ",,Travel and Transportation,"Bike-sharing, Minnesota",text,p3063,biking,,,,
GeoLife GPS Trajectory,https://www.microsoft.com/en-us/download/confirmation.aspx?id=52367,1.71,8/9/2012,zip,N/A,public,static,,"A GPS trajectory of this dataset is represented by a sequence of time-stamped points, each of which contains the information of latitude, longitude and altitude. This dataset contains 17,621 trajectories with a total distance of about 1.2 million kilometers and a total duration of 48,000+ hours. These trajectories were recorded by different GPS loggers and GPS-phones, and have a variety of sampling rates. 91 percent of the trajectories are logged in a dense representation, e.g. every 1~5 seconds or every 5~10 meters per point.",,Travel and Transportation,GPS,text,p3064,,,,,
2014 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2014.zip,0.15,v1,zip,N/A,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,
2015 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2015.zip,0.173,v1,zip,N/A,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,
2016 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2016.zip,0.201,v1,zip,N/A,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,
2017 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2017.zip,0.238,v1,zip,N/A,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,
2018 Montreal MOXI Bike Share,https://montreal.bixi.com/c/bixi/file_db/data_all.file/BixiMontrealRentals2018.zip,0.052,v1,zip,N/A,public,static,,This dataset includes monthly trip information for MOXI riders. Timestamps and GPS information can be useful for analysis or further development. ,Montreal,Travel and Transportation,Bike-sharing,text,p3065,biking,,,,
NYC Taxi Trip 2013,https://archive.org/download/nycTaxiTripData2013/nycTaxiTripData2013_archive.torrent,0.0000012,6/18/2014,torrent,N/A,public,static,,FOIA/FOILed Taxi Trip Data from the NYC Taxi and Limousine Commission 2013.,NYC,Travel and Transportation,Taxi,text,p3066,Automotive,,,,
NYC Uber Trip Data,https://github.com/fivethirtyeight/uber-tlc-foil-response/archive/master.zip,0.4,v1,zip,N/A,public,static,,"This directory contains data on over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015.",,Travel and Transportation,Uber,text,p3067,Automotive,,,,
Open Flights Airport Database,https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports-extended.dat,0.0014,v1,dat,Open Database License,public,static,,"The OpenFlights Airports Database contains over 10,000 airports, train stations and ferry terminals spanning the globe. Each entry contains the following information: Airport ID, Name, City, Country, IATA, ICAO, Latitude, Longitude, Altitude, Timezone, DST, Tz database time zone, Type, and Source.",,Travel and Transportation,"Airline, airport",text,p3068,Airline,,,,
Open Flights Airline Database,https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat,0.0004,v1,dat,Open Database License,public,static,,"The OpenFlights Airlines Database contains 5888 airlines. Each entry contains the following information: Airline ID, Name, Alias, IATA, ICAO, Callsign, Country, and Active.",,Travel and Transportation,Airline,text,p3069,Airline,,,,
Open Flights Route Database,https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat,0.002,v1,dat,Open Database License,public,static,,"The OpenFlights/Airline Route Mapper Route Database contains 67663 routes between 3321 airports on 548 airlines spanning the globe. Each entry contains the following information: Airline, Airline ID, Source Airport, Source Airport ID, Destination airport, Destination airport ID, Codeshare, Stops, and Equipment",,Travel and Transportation,Route,text,p3070,Airline,,,,
Open Flights Plane Database,https://raw.githubusercontent.com/jpatokal/openflights/master/data/planes.dat,0.000005,v1,dat,Open Database License,public,static,,"The OpenFlights plane database contains a curated selection of 173 passenger aircraft with IATA and/or ICAO codes, covering the vast majority of flights operated today and commonly used in flight schedules and reservation systems.",,Travel and Transportation,Plane,text,p3071,Airline,,,,
2013 Housing Affordability Data System ,https://www.huduser.gov/portal/datasets/hads/hads2013n_ASCII.zip,0.018,v1,zip,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
2011 Housing Affordability Data System ,https://www.huduser.gov/portal/datasets/hads/hads2011(ASCII).zip,0.022,v1,zip,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
2009 Housing Affordability Data System ,https://www.huduser.gov/portal/datasets/hads/hads2009(ASCII)_v2.exe,0.01,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
2007 Housing Affordability Data System ,https://www.huduser.gov/portal/datasets/hads/hads2007(ASCII)_v2.exe,0.00819,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
2005 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads2005(ASCII)_v2.exe,0.00897,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
2003 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads2003(ASCII).exe,0.01,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
2001 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads2001(ASCII).exe,0.009,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
1999 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads99(ASCII).exe,0.0098,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
1997 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads97(ASCII).exe,0.00816,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
1995 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads95(ASCII).exe,0.00817,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
1993 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads93(ASCII).exe,0.00945,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
1991 Housing Affordability Data System,https://www.huduser.gov/portal/datasets/hads/hads91(ASCII).exe,0.00925,v1,exe,N/A,public,static,,"This system categorizes housing units by affordability and households by income, with respect to the Adjusted Median Income, Fair Market Rent (FMR), and poverty income. It also includes housing cost burden for owner and renter households.",ASCII version,Consumer Products,Housing,text,p3072,Housing,,,,
FDIC Failed Bank List,http://www.fdic.gov/bank/individual/failed/banklist.csv,0.000005,v1,csv,U.S. Government Work,public,,,"This list includes banks which have failed since October 1, 2000.",,Banking,failed banks,text,p3073,,,,,
Farmers Markets Directory and Geographic Data,https://apps.ams.usda.gov/FarmersMarketsExport/ExcelExport.aspx,0.0032,2/4/2018,csv,Creative Commons Attribution License,public,,,"This dataset includes longitude and latitude, state, address, name, and zip code of Farmers Markets in the United States.",,Consumer Products,Farmer's Markets,text,p3074,,,,,
Demographic Statistics,https://data.cityofnewyork.us/api/views/kku6-nxdu/rows.csv?accessType=DOWNLOAD,0.000029,4/4/2018,csv,N/A,public,static,,New York demographic statistics by zip code,,Government,"Demographic, New York",text,p3075,,,,,
2011 Zip Code Data,https://www.irs.gov/pub/irs-soi/2011zipcode.zip,0.235,v1,zip,N/A,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,
2012 Zip Code Data,https://www.irs.gov/pub/irs-soi/2012zipcode.zip,0.226,v1,zip,N/A,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,
2013 Zip Code Data,https://www.irs.gov/pub/irs-soi/zipcode2013.zip,0.51,v1,zip,N/A,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,
2014 Zip Code Data,https://www.irs.gov/pub/irs-soi/zipcode2014.zip,0.365,v1,zip,N/A,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,
2015 Zip Code Data,https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2015-zip-code-data-soi,0.379,v1,zip,N/A,public,static,,This study provides detailed tabulations of individual income tax return data at the state and ZIP code level.,,Government,"Treasury, tax",text,p3076,,,,,
Consumer Complaint Database,https://data.consumerfinance.gov/api/views/s6ew-h6mp/rows.csv?accessType=DOWNLOAD,0.574,v1,csv,U.S. Government Work,public,static,,Bureau of Consumer Financial Protection: consumer complaints about financial products and services.,,Government,"Consumer, complaint",text,p3078,,,,,
2010 Census Populations by Zip Code,https://data.lacity.org/api/views/nxs9-385f/rows.csv?accessType=DOWNLOAD,0.000012,2/3/2018,csv,N/A,public,static,,This data comes from the 2010 Census Profile of General Population and Housing Characteristics. Zip codes are limited to those that fall at least partially within LA city boundaries.,,Government,"Los Angeles,  population",text,p3079,population,,,,
Crimes - 2001 to present,https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD,0.01,6/6/2018,csv,N/A,public,continuous,,"This dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. ",,Government,Crime,text,p3080,,,,,
National Stock Number Extract,https://inventory.data.gov/dataset/67567804-073d-40ad-a710-2b0bed8b84e2/resource/360b0748-d161-4857-a7dc-dfccfaeea096/download/nsn-extract-4-5-17.xlsx,0.0009,4/19/2017,xlsx,Creative Commons Attribution License,public,static,,"National Stock Number extract includes the current listing of National Stock Numbers (NSNs) , NSN item name and descriptions, and current selling price of each product listed in GSA Advantage and managed by GSA.",General Services Administration,Consumer Products,,text,p3081,,,,,
Nottingham Music Database,http://ifdo.ca/~seymour/nottingham/nottingham_database.zip,0.0005,v1,zip,N/A,public,static,,This database contains over 1000 Folk Tunes converted to ABC format,,Media and Entertainment,Music,text,p3082,,,,,
Crime in the United States 1994-2013,https://ucr.fbi.gov/crime-in-the-u.s/2013/crime-in-the-u.s.-2013/tables/1tabledatadecoverviewpdf/table_1_crime_in_the_united_states_by_volume_and_rate_per_100000_inhabitants_1994-2013.xls/output.xls,0.000033,v1,xls,N/A,public,,,"Crime in the United States by Volume and Rate per 100,000 Inhabitants, 1994–2013",,Government,Crime,text,p3083,,,,,
Complication Rates by Hospital,https://data.medicare.gov/views/bg9k-emty/files/e0ad4b50-10b2-41e8-a9a7-85b705c75178?content_type=application%2Fzip%3B%20charset%3Dbinary&filename=Hospital_Revised_FlatFiles.zip,0.306,5/23/2018,zip,N/A,public,,,"These data allow you to compare the quality of care at over 4,000 Medicare-certified hospitals across the country.",,Healthcare,,text,p3084,,,,,
E-MTAB-6659,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6659/E-MTAB-6659.processed.1.zip,0.016,6/1/2018,zip,N/A,public,static,,"Transcription profiling by array of wild type pea seeds and seeds with embryo-specific expression of a bacterial trehalose 6-phosphate phosphatase (TPP), encoded by the otsB gene to investigate the influence of decreased T6P content on gene expression in developing cotyledons.",,Life Sciences,Pisum sativum,text,p3089,Biology,,,,
E-MTAB-5783,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-5783/E-MTAB-5783.processed.1.zip,0.038,5/22/2018,zip,N/A,public,static,,"RNA-seq of formalin-fixed, paraffin-embedded uninvolved terminal ileal tissue obtained from ileo-colic resection surgeries of Crohns disease and control patients.",,Life Sciences,Homo sapiens,text,p3090,Biology,,,,
E-MTAB-6406,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6406/E-MTAB-6406.processed.1.zip,0.0007,5/19/2018,zip,N/A,public,static,,MIRNA - Integrative multi-omics survey of effects carbon-based engineered nanomaterials on lung derived cell-lines.,,Life Sciences,Homo sapiens,text,p3091,Biology,,,,
E-MTAB-6397,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6397/E-MTAB-6397.processed.1.zip,0.764,5/19/2018,zip,N/A,public,static,,DNA-METHYLATION - Integrative multi-omics survey of effects carbon-based engineered nanomaterials on lung derived cell-lines.,,Life Sciences,Homo sapiens,text,p3092,Biology,,,,
E-MTAB-6396,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-6396/E-MTAB-6396.processed.1.zip,0.077,5/19/2018,zip,N/A,public,static,,MRNA-Integrative multi-omics survey of effects carbon-based engineered nanomaterials on lung derived cell-lines.,,Life Sciences,Homo sapiens,text,p3093,Biology,,,,
E-MTAB-5776,https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-5776/E-MTAB-5776.processed.1.zip,0.0089,5/16/2018,zip,N/A,public,static,,Microarray transcription profiling of healthy and PVYNTN infected potato tuber tissues (necrotic and non-necrotic).,,Life Sciences,Solanum tuberosum,text,p3094,Biology,,,,
Large Age Gap (LAG),http://www.ivl.disco.unimib.it/wp-content/uploads/2016/09/LAGdataset_100.zip,0.07,v1,zip,proprietary,public,static,,"Large Age-Gap (LAG) dataset is a dataset containing variations of age in the wild, with images ranging from child/young to adult/old. The dataset contains 3,828 images of 1,010 celebrities. For each identity at least one child/young image and one adult/old image are present.",,Life Sciences,Human,image,p3095,,,,,
DAVIS: Densely Annotated Video Segmentation 2016,https://graphics.ethz.ch/Downloads/Data/Davis/DAVIS-data.zip,2,v1,zip,proprietary,public,static,,"DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motion-blur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation.",,Electronics,"Research, motion",video,p3096,Computer Vision,,,,
DAVIS: Densely Annotated Video Segmentation 2017,https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-Full-Resolution.zip,3.2,v1,zip,proprietary,public,static,,"DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motion-blur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation.",,Electronics,"Research, motion",video,p3096,Computer Vision,,,,
LASIESTA Simple Sequences 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SI_01.rar,0.05,v1,rar,proprietary,public,static,,"Simple Sequences (SI): Three people cross a room, walking perpendicularly to the the optical axis of the camera. 300 frames, smooth shadows.","Sequences not containing camouflage, occlusions, illumination changes, modified background, camera motion, or bootstraping.",Electronics,"Research, motion",video,p3097,Computer Vision,,,,
LASIESTA Simple Sequences 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SI_02.rar,0.05,v1,rar,proprietary,public,static,,"Simple Sequences (SI): One person crosses a corridor going towards the camera. 300 frames, smooth shadows.","Sequences not containing camouflage, occlusions, illumination changes, modified background, camera motion, or bootstraping.",Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Camouflage 01 ,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_CA_01.rar,0.05,v1,rar,proprietary,public,static,,A person crosses a room and remains static for a few seconds in front of a door with similar color to his clothing. 350 frames.,Sequences with moving objects remaining temporally static on background regions with similar color.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Camouflage 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_CA_02.rar,0.05,v1,rar,proprietary,public,static,,A person appears and stands in front of a wall with similar color to his T-shirt. A plant is constantly moving in the background. 525 frames.,Sequences with moving objects remaining temporally static on background regions with similar color.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Occlusions 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_OC_01.rar,0.05,v1,rar,proprietary,public,,,A person crosses a room and passes behind a large column. 250 frames,Sequences containing totally or partially ocludded moving objects.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Occlusions 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_OC_02.rar,0.05,v1,rar,proprietary,public,,,One person goes down some stairs and walks behind a railing. 250 frames.,Sequences containing totally or partially ocludded moving objects.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Illumination Changes 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_IL_01.rar,0.05,v1,rar,proprietary,public,,,A person who is going through a room turns on the lights. 300 frames.,Sequences with global ilumination changes.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Illumination Changes 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_IL_02.rar,0.05,v1,rar,proprietary,public,,,One person walks towards a window and opens the blinds. Then the person walks out of the scene. 525 frames.,Sequences with global ilumination changes.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Modified Background 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_MB_01.rar,0.05,v1,rar,proprietary,public,,,"One person enters a room with a bag on his shoulder, leaves the bag on the ground, and goes out of the scene. 450 frames, smooth shadows, abandoned object.",Sequences showing situations in wich background elements are subtracted or where some objects are abandoned.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Modified Background 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_MB_02.rar,0.05,v1,rar,proprietary,public,,,"One person enters a room, picks up a bag that is on the ground, and goes out. 350 frames, moderate shadows, permanent changes in the background.",Sequences showing situations in wich background elements are subtracted or where some objects are abandoned.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Bootstrap 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_BS_01.rar,0.05,v1,rar,proprietary,public,,,"Two people shake their hands in a corridor and walk in opposite directions, entering two different rooms. 275 frames, bootstrap, moderate shadows.",Sequences containing moving objects from the first frame.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Bootstrap 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_BS_02.rar,0.05,v1,rar,proprietary,public,,,"A person stands shortly in front of a sign before walking out of the scene. 275 frames, bootstrap, moderate shadows.",Sequences containing moving objects from the first frame.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Moving Camera 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_MC_01.rar,0.05,v1,rar,proprietary,public,,,"One person climbs some steps in a hall with columns. The camera sweeps the scene. 300 frames, camera motion, smooth shadows.",Sequences recorded with non-completely static cameras (handy cameras or pan/tilt motion).,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Moving Camera 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_MC_02.rar,0.05,v1,rar,proprietary,public,,,"A person crosses a room. The camera is not completely static (soft jitter). 250 frames, camera motion, moderate shadows.",Sequences recorded with non-completely static cameras (handy cameras or pan/tilt motion).,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_01.rar,0.05,v1,rar,proprietary,public,,,"Pan motion, low intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_02.rar,0.05,v1,rar,proprietary,public,,,"Pan motion, medium intensity, 300 frames, 3 objects",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 03,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_03.rar,0.05,v1,rar,proprietary,public,,,"Pan motion, high intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 04,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_04.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, low/low intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 05,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_05.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, low/medium intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 06,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_06.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, low/high intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 07,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_07.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, medium/low intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 08,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_08.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, medium/medium intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 09,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_09.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, medium/high intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 10,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_10.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, high/low intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 11,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_11.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, high/medium intensity, 300 frames, 3 objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Simulated Motion 12,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/I_SM_12.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, high/high intensity, 300 frames, 3 objects. ",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, indoor",video,p3097,Computer Vision,,,,
LASIESTA Cloudy Conditions 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_CL_01.rar,0.05,v1,rar,proprietary,public,,,"A car crosses a parking and passes behind a lamppost. The wind moves the vegetation and clouds are reflected in parked cars' windows. 225 frames, dynamic background, smooth shadows, partial occlusion",Sequences that have cloudy conditions.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA Cloudy Conditions 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_CL_02.rar,0.05,v1,rar,proprietary,public,,,"Two people go up a street, one immediately after another. The wind moves the vegetation. 425 frames, dynamic background, smooth shadows.",Sequences that have cloudy conditions.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA Rainy Conditions 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_RA_01.rar,0.05,v1,rar,proprietary,public,,,"In rainy conditions, a person walks in a parking. After a while, a parked car starts moving and leaves the scene. 1400 frames, dynamic background, moderate shadows, rain, partial occlusion, permanent changes in the background. ",Sequences presenting dynamic background due to falling rain.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA Rainy Conditions 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_RA_02.rar,0.05,v1,rar,proprietary,public,,,"In rainy conditions, two people (one after another) go walking in front of a garden. 375 frames, dynamic frame, moderate shadows, rain.",Sequences presenting dynamic background due to falling rain.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA Snowy Conditions 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SN_01.rar,0.05,v1,rar,proprietary,public,,,"It's snowing. A car crosses a parking and disappears under a gate. 500 frames, dynamic background, snow, smooth shadows.",Sequences presenting dynamic background due to falling snow.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA Snowy Conditions 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SN_02.rar,0.05,v1,rar,proprietary,public,,,"It is snowing. A car traveling very slowly appears in the bottom right corner of the scene. 850 frames, dynamic background, snow, smooth shadows, partial occlusions.",Sequences presenting dynamic background due to falling snow.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA Sunny Conditions 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SU_01.rar,0.05,v1,rar,proprietary,public,,,"A person crosses a street in a sunny area and a car moves in the background of the scene. The silhouette of the person is reflected in a puddle. 250 frames, dynamic background, camouflage, hard shadows.",Sequences recorded in sunny scenarios and showing hard shadows of the moving objects.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA Sunny Conditions 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SU_02.rar,0.05,v1,rar,proprietary,public,,,"A person crosses a street in a sunny area. Later, another person crosses in opposite direction in a shaded area. 400 frames, dynamic background, hard shadows. ",Sequences recorded in sunny scenarios and showing hard shadows of the moving objects.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Moving Camera 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_MC_01.rar,0.05,v1,rar,proprietary,public,,,"A person crosses a parking and the camera sweeps the scene. 425 frames, smooth shadows, camera motion.",Sequences recorded with non-completely static cameras (handy cameras or pan/tilt motion).,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Moving Camera 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_MC_02.rar,0.05,v1,rar,proprietary,public,,,"A person crosses a parking. The camera is not completely static (soft jitter). 175 frames, dynamic background, smooth shadows, camera motion.",Sequences recorded with non-completely static cameras (handy cameras or pan/tilt motion).,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 01,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_01.rar,0.05,v1,rar,proprietary,public,,,"Pan and tilt motion, low intensity, 425 frames, 2 moving objects. ",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 02,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_02.rar,0.05,v1,rar,proprietary,public,,,"Pan and tilt motion, medium intensity, 425 frames, 2 moving objects",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 03,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_03.rar,0.05,v1,rar,proprietary,public,,,"Pan and tilt motion, high intensity, 425 frames, 2 moving objects. ",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 04,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_04.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, low/low intensity, 425 frames, 2 moving objects",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 05,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_05.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, low/medium intensity, 425 frames, 2 moving objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 06,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_06.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, low/high intensity, 425 frames, 2 moving objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 07,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_07.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, medium/low intensity, 425 frames,  2 moving objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 08,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_08.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, medium/medium intensity, 425 frames, 2 moving objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 09,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_09.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, medium/high intensity, 425 frames, 2 moving objects",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 10,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_10.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, high/low intensity, 425 frames, 2 moving objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 11,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_11.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, high/medium intensity, 425 frames, 2 moving objects.",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
LASIESTA (O) Simulation Motion 12,http://www.gti.ssr.upm.es/data/Data/Downloads/LASIESTA/O_SM_12.rar,0.05,v1,rar,proprietary,public,,,"Jitter/rotation motion, high/high intensity, 425 frames, 2 moving objects. ",Set of sequences simulating different types and intensities of camera motion.,Electronics,"Research, motion, outdoor",video,p3097,Computer Vision,,,,
FIRE: Fundus Image Registration Dataset,http://www.ics.forth.gr/cvrl/fire/FIRE.7z,0.264,v1,7-zip,proprietary,public,,,"The dataset consists of 129 retinal images forming 134 image pairs. These image pairs are split into 3 different categories depending on their characteristics. The images were acquired with a Nidek AFC-210 fundus camera, which acquires images with a resolution of 2912x2912 pixels and a FOV of 45° both in the x and y dimensions.",,Electronics,"Research, retina","image, text",p3098,Biology,,,,
Duke MTMC,http://vision.cs.duke.edu/DukeMTMC/data/demo_code/downloadDukeMTMC.m,N/A,v1,script,proprietary,public,,,"DukeMTMC aims to accelerate advances in multi-target multi-camera tracking. It provides a tracking system that works within and across cameras, a new large scale HD video data set recorded by 8 synchronized cameras with more than 7,000 single camera trajectories and over 2,000 unique identities, and a new performance evaluation method that measures how often a system is correct about who is where.",Annotated video,Electronics,"Research, camera","image, text",p3099,Computer Vision,,,,
NYU Symmetry Database,https://symmetry.cs.nyu.edu/Database.zip,0.011,v1,zip,proprietary,public,,,The mirror symmetry database contains 176 single-symmetry and 63 multiple-symmetry images (.png files) with accompanying ground-truth annotations (.mat files).,,Electronics,"Research, motion","image, text",p3100,Computer Vision,,,,
Mouse Embryo Tracking Database,http://celltracking.bio.nyu.edu/MouEmbTrkDtb.zip,3.58,v1,zip,proprietary,public,,,"The database contains, for each of the 100 examples: (1) the uncompressed frames, up to the 10th frame after the appearance of the 8th cell; (2) a text file with the trajectories of all the cells, from appearance to division (for cells of generations 1 to 3), where a trajectory is a sequence of pairs (center, radius); (3) a movie file showing the trajectories of the cells.", NYU Center for Genomics and Systems Biology,Electronics,"Research, biology","image, text",p3101,Computer Vision,,,,
Edge milling heads Dataset,http://pitia.unileon.es/varp/sites/default/files/MillingHeadDataSet.zip,0.118,v1,zip,proprietary,public,,,"This data set comprises 144 images of an edge profile cutting head of a milling machine. It contains a total of 30 cutting inserts. The cutting head is formed by 6 diagonals of inserts in radial direction along the tool perimeter, encompassing 5 inserts per diagonal in axial direction. Positions of the last and first inserts of consecutive diagonals are aligned in the same vertical. Therefore, even though we have 30 inserts, there are 24 equally spaced positions of inserts along the tool perimeter. Additionally, inserts are squared shape with four 90o indexable cutting edges. Inserts are fastened with a screw. Rake angle is 0.",,Electronics,"Consumer product, research, detection",image,p3102,Computer Vision,,,,
Georgia Tech Face Database,http://www.anefian.com/research/gt_db.zip,0.128,v1,zip,proprietary,public,,,"This dataset contains images of 50 people taken in two or three sessions between 06/01/99 and 11/15/99 at the Center for Signal and Image Processing at  Georgia Institute of Technology. All people in the database are represented by 15 color JPEG images with cluttered background taken at resolution 640x480 pixels. The average size of the faces in these images is 150x150 pixels. The pictures show frontal and/or tilted faces with different facial expressions, lighting conditions and scale.",,Electronics,"Research, facial recognition",image,p3103,Computer Vision,,,,
Monthly Excess Returns,https://vincentarelbundock.github.io/Rdatasets/csv/boot/acme.csv,N/A,v1,csv,proprietary,public,,,The excess return for the Acme Cleveland Corporation are recorded along with those for all stocks listed on the New York and American Stock Exchanges were recorded over a five year period. These excess returns are relative to the return on a risk-less investment such a U.S. Treasury bills. Contains 60 rows and 3 columns.,,Banking,,text,p3104,,,,,
Delay in AIDS Reporting in England and Wales,https://vincentarelbundock.github.io/Rdatasets/csv/boot/aids.csv,N/A,v1,csv,proprietary,public,,,"Although all cases of AIDS in England and Wales must be reported to the Communicable Disease Surveillance Centre, there is often a considerable delay between the time of diagnosis and the time that it is reported. In estimating the prevalence of AIDS, account must be taken of the unknown number of cases which have been diagnosed but not reported. The data set here records the reported cases of AIDS diagnosed from July 1983 and until the end of 1992. The data are cross-classified by the date of diagnosis and the time delay in the reporting of the cases. Contains 570 rows and 6 columns.",,Healthcare,,text,p3105,,,,,
Failures of Air-conditioning Equipment,https://vincentarelbundock.github.io/Rdatasets/csv/boot/aircondit.csv,N/A,v1,csv,proprietary,public,,,Proschan (1963) reported on the times between failures of the air-conditioning equipment in 10 Boeing 720 aircraft. The aircondit data frame contains the intervals for the ninth aircraft while aircondit7 contains those for the seventh aircraft.,,Travel and Transportation,Aircraft,text,p3106,,,,,
Failures of Air-conditioning Equipment,https://vincentarelbundock.github.io/Rdatasets/csv/boot/aircondit7.csv,N/A,v1,csv,proprietary,public,,,Proschan (1963) reported on the times between failures of the air-conditioning equipment in 10 Boeing 720 aircraft. The aircondit data frame contains the intervals for the ninth aircraft while aircondit7 contains those for the seventh aircraft.,,Travel and Transportation,Aircraft,text,p3106,,,,,
Car Speeding and Warning Signs	,https://vincentarelbundock.github.io/Rdatasets/csv/boot/amis.csv,N/A,v1,csv,proprietary,public,,,"In a study into the effect that warning signs have on speeding patterns, Cambridgeshire County Council considered 14 pairs of locations. The locations were paired to account for factors such as traffic volume and type of road. One site in each pair had a sign erected warning of the dangers of speeding and asking drivers to slow down. No action was taken at the second site. Three sets of measurements were taken at each site. Each set of measurements was nominally of the speeds of 100 cars but not all sites have exactly 100 measurements. These speed measurements were taken before the erection of the sign, shortly after the erection of the sign, and again after the sign had been in place for some time.",,Travel and Transportation,Speeding,text,p3107,,,,,
Remission Times for Acute Myelogenous Leukaemia,https://vincentarelbundock.github.io/Rdatasets/csv/boot/aml.csv,N/A,v1,csv,N/A,public,,,"A clinical trial to evaluate the efficacy of maintenance chemotherapy for acute myelogenous leukaemia was conducted by Embury et al. (1977) at Stanford University. After reaching a stage of remission through treatment by chemotherapy, patients were randomized into two groups. The first group received maintenance chemotherapy and the second group did not. The aim of the study was to see if maintenance chemotherapy increased the length of the remission. The data here formed a preliminary analysis which was conducted in October 1974.",,Healthcare,"Medicine, chemotherapy, cancer, research",text,p3108,,,,,
Beaver Body Temperature Data,https://vincentarelbundock.github.io/Rdatasets/csv/boot/beaver.csv,N/A,v1,csv,proprietary,public,,,This data set is part of a long study into body temperature regulation in beavers. Four adult female beavers were live-trapped and had a temperature-sensitive radio transmitter surgically implanted. Readings were taken every 10 minutes. The location of the beaver was also recorded and her activity level was dichotomized by whether she was in the retreat or outside of it since high-intensity activities only occur outside of the retreat.,,Life Sciences,"Beaver, temperature",text,p3109,Biology,,,,
Population of U.S. Cities,https://vincentarelbundock.github.io/Rdatasets/csv/boot/bigcity.csv,N/A,v1,csv,proprietary,public,,,The measurements are the population (in 1000's) of 49 U.S. cities in 1920 and 1930. The 49 cities are a random sample taken from the 196 largest cities in 1920. The city data frame consists of the first 10 observations in bigcity.,,Government,Population,text,p3110,,,,,
Spatial Location of Bramble Canes,https://vincentarelbundock.github.io/Rdatasets/csv/boot/brambles.csv,N/A,v1,csv,proprietary,public,,,The location of living bramble canes in a 9m square plot was recorded. We take 9m to be the unit of distance so that the plot can be thought of as a unit square. The bramble canes were also classified by their age.,,Life Sciences,"Environment, canes",text,p3111,,,,,
Smoking Deaths Among Doctors,https://vincentarelbundock.github.io/Rdatasets/csv/boot/breslow.csv,N/A,v1,csv,proprietary,public,,,In 1961 Doll and Hill sent out a questionnaire to all men on the British Medical Register enquiring about their smoking habits. Almost 70% of such men replied. Death certificates were obtained for medical practitioners and causes of death were assigned on the basis of these certificates. The breslow data set contains the person-years of observations and deaths from coronary artery disease accumulated during the first ten years of the study.,,Healthcare,,text,p3112,,,,,
Calcium Uptake Data,https://vincentarelbundock.github.io/Rdatasets/csv/boot/calcium.csv,N/A,v1,csv,proprietary,public,,,"Howard Grimes from the Botany Department, North Carolina State University, conducted an experiment for biochemical analysis of intracellular storage and transport of calcium across plasma membrane. Cells were suspended in a solution of radioactive calcium for a certain length of time and then the amount of radioactive calcium that was absorbed by the cells was measured. The experiment was repeated independently with 9 different times of suspension each replicated 3 times.",,Life Sciences,Calcium,text,p3113,Biology,,,,
Sugar-cane Disease Data,https://vincentarelbundock.github.io/Rdatasets/csv/boot/cane.csv,N/A,v1,csv,proprietary,public,,,"The cane data frame has 180 rows and 5 columns. The data frame represents a randomized block design with 45 varieties of sugar-cane and 4 blocks. The aim of the experiment was to classify the varieties into resistant, intermediate and susceptible to a disease called ""coal of sugar-cane"" (carvao da cana-de-acucar). This is a disease that is common in sugar-cane plantations in certain areas of Brazil.",,Life Sciences,"sugar, biology, agriculture",text,p3114,agriculture,,,,
Weight Data for Domestic Cats,https://vincentarelbundock.github.io/Rdatasets/csv/boot/catsM.csv,N/A,v1,csv,proprietary,public,,,144 adult (over 2kg in weight) cats used for experiments with the drug digitalis had their heart and body weight recorded. 47 of the cats were female and 97 were male. The catsM data frame consists of the data for the male cats.,,Life Sciences,"cats, weight, measurement",text,p3115,,,,,
Position of Muscle Caveolae,https://vincentarelbundock.github.io/Rdatasets/csv/boot/cav.csv,N/A,v1,csv,proprietary,public,,,The data gives the positions of the individual caveolae in a square region with sides of length 500 units. This grid was originally on a 2.65mum square of muscle fibre.,,Life Sciences,muscle,text,p3116,Biology,,,,
CD4 Counts for HIV-Positive Patients,https://vincentarelbundock.github.io/Rdatasets/csv/boot/cd4.csv,N/A,v1,csv,proprietary,public,,,"CD4 cells are carried in the blood as part of the human immune system. One of the effects of the HIV virus is that these cells die. The count of CD4 cells is used in determining the onset of full-blown AIDS in a patient. In this study of the effectiveness of a new anti-viral drug on HIV, 20 HIV-positive patients had their CD4 counts recorded and then were put on a course of treatment with this drug. After using the drug for one year, their CD4 counts were again recorded. The aim of the experiment was to show that patients taking the drug had increased CD4 counts which is not generally seen in HIV-positive patients.",,Life Sciences,"cells, HIV",text,p3117,Biology,,,,
Channing House Data,https://vincentarelbundock.github.io/Rdatasets/csv/boot/channing.csv,N/A,v1,csv,proprietary,public,,,"Channing House is a retirement centre in Palo Alto, California. These data were collected between the opening of the house in 1964 until July 1, 1975. In that time 97 men and 365 women passed through the centre. For each of these, their age on entry and also on leaving or death was recorded. A large number of the observations were censored mainly due to the resident being alive on July 1, 1975 when the data was collected. Over the time of the study 130 women and 46 men died at Channing House. Differences between the survival of the sexes, taking age into account, was one of the primary concerns of this study.",,Healthcare,retirement,text,p3118,,,,,
Genetic Links to Left-handedness,https://vincentarelbundock.github.io/Rdatasets/csv/boot/claridge.csv,N/A,v1,csv,proprietary,public,,,"The data are from an experiment which was designed to look for a relationship between a certain genetic characteristic and handedness. The 37 subjects were women who had a son with mental retardation due to inheriting a defective X-chromosome. For each such mother a genetic measurement of their DNA was made. Larger values of this measurement are known to be linked to the defective gene and it was hypothesized that larger values might also be linked to a progressive shift away from right-handednesss. Each woman also filled in a questionnaire regarding which hand they used for various tasks. From these questionnaires a measure of hand preference was found for each mother. The scale of this measure goes from 1, indicating someone who always favours their right hand, to 8, indicating someone who always favours their left hand. Between these two extremes are people who favour one hand for some tasks and the other for other tasks.",,Life Sciences,"Research, biology",text,p3119,,,,,
Dates of Coal Mining Disasters,https://vincentarelbundock.github.io/Rdatasets/csv/boot/coal.csv,N/A,v1,csv,proprietary,public,,,"This data frame gives the dates of 191 explosions in coal mines which resulted in 10 or more fatalities. The time span of the data is from March 15, 1851 until March 22 1962.
",,Metals and Mining,"Disaster, coal",text,p3120,,,,,
Average Heights of the Rio Negro river at Manaus,https://vincentarelbundock.github.io/Rdatasets/csv/boot/manaus.csv,N/A,v1,csv,proprietary,public,,,"The data values are monthly averages of the daily stages (heights) of the Rio Negro at Manaus. Manaus is 18km upstream from the confluence of the Rio Negro with the Amazon but because of the tiny slope of the water surface and the lower courses of its flatland affluents, they may be regarded as a good approximation of the water level in the Amazon at the confluence. The data here cover 90 years from January 1903 until December 1992.",,Earth Sciences,River,text,p3121,,,,,
Neurophysiological Point Process Data,https://vincentarelbundock.github.io/Rdatasets/csv/boot/neuro.csv,N/A,v1,csv,proprietary,public,,,neuro is a matrix containing times of observed firing of a neuron in windows of 250ms either side of the application of a stimulus to a human subject. Each row of the matrix is a replication of the experiment and there were a total of 469 replicates.,,Life Sciences,"Human, neuro, reflex",text,p3122,Biology,,,,
Survival from Malignant Melanoma,https://vincentarelbundock.github.io/Rdatasets/csv/boot/melanoma.csv,N/A,v1,csv,proprietary,public,,,"The data consist of measurements made on patients with malignant melanoma. Each patient had their tumour removed by surgery at the Department of Plastic Surgery, University Hospital of Odense, Denmark during the period 1962 to 1977. The surgery consisted of complete removal of the tumour together with about 2.5cm of the surrounding skin. Among the measurements taken were the thickness of the tumour and whether it was ulcerated or not. These are thought to be important prognostic variables in that patients with a thick and/or ulcerated tumour have an increased chance of death from melanoma. Patients were followed until the end of 1977.",,Healthcare,"Melanoma, cancer",text,p3123,Cancer,,,,
Labeled Faces in the Wild,http://vis-www.cs.umass.edu/lfw/lfw.tgz,0.173,v1,tgz,proprietary,public,,,"The data set contains more than 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. 1680 of the people pictured have two or more distinct photos in the data set.",University of Massachussettes,Electronics,facial recognition,image,p3124,,,,,
One Million Audio Cover Images for Research,https://archive.org/download/audio-covers/audio-covers_archive.torrent,148,v1,torrent,proprietary,public,,,"At 148gb, the collection is large but not unmanageable (there is a torrent available) and allows a developer or artist to work with the material through various means. The differences in resolution, filename structure and arrangement encourage machine learning or visual recognition algorithms to be used. ",,Electronics,"Machine learning, music",image,p3125,,,,,
Complete Public Reddit Comments Corpus,https://archive.org/download/2015_reddit_comments_corpus/2015_reddit_comments_corpus_archive.torrent,0.008,v1,torrent,proprietary,public,,,This is an archive of Reddit comments from October of 2007 until May of 2015 (complete month). This reflects 14 months of work and a lot of API calls. This dataset includes nearly every publicly available Reddit comment.,,Electronics,"Machine Learning, music",text,p3126,,,,,
Microsoft Marco Training,https://msmarco.blob.core.windows.net/msmarco/train_v2.1.json.gz,3.83,v2.1,gzip,MIT License,public,,,"MS MARCO(Microsoft Machine Reading Comprehension) is a large scale dataset focused on machine reading comprehension and question answering. In MS MARCO, all question have been generated from real anonymized Bing user queries which grounds the dataset in a real world problem and can provide researchers real contrainsts their models might be used in.The context passages, from which the answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated.",Training Set,Electronics,Machine Learning,text,p3127,,,,,
Microsoft Marco Dev,https://msmarco.blob.core.windows.net/msmarco/dev_v2.1.json.gz,0.47,v2.1,gzip,MIT License,public,,,"MS MARCO(Microsoft Machine Reading Comprehension) is a large scale dataset focused on machine reading comprehension and question answering. In MS MARCO, all question have been generated from real anonymized Bing user queries which grounds the dataset in a real world problem and can provide researchers real contrainsts their models might be used in.The context passages, from which the answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated.",Dev Set,Electronics,Machine Learning,text,p3127,,,,,
Microsoft Marco Evaluation,https://msmarco.blob.core.windows.net/msmarco/eval_v2.1_public.json.gz,0.47,v2.1,gzip,MIT License,public,,,"MS MARCO(Microsoft Machine Reading Comprehension) is a large scale dataset focused on machine reading comprehension and question answering. In MS MARCO, all question have been generated from real anonymized Bing user queries which grounds the dataset in a real world problem and can provide researchers real contrainsts their models might be used in.The context passages, from which the answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated.",Evaluation Set,Electronics,Machine Learning,text,p3127,,,,,
MNIST,https://pjreddie.com/media/files/mnist_train.csv,0.109,v1,csv,N/A,public,,,"Dataset of 25x25, centered, B&W handwritten digits.",,Electronics,Machine Learning,image,p3128,Computer Vision,,,,
LSUN Scene Classification,https://github.com/fyu/lsun/blob/master/download.py,N/A,v1,script,N/A,public,,,"In this task, an algorithm needs to report the top 1 most likely scene categories for each image.",,Electronics,Object recognition,image,p3129,Machine Learning,,,,
LSUN Saliency Prediction (iSun),http://lsun.cs.princeton.edu/challenge/2015/eyetracking/data/image.zip,2,v1,zip,N/A,public,,,"The data is collected by gaze tracking from Amazon Mechanical Turk using a web-cam. All our images are from the SUN database. For each image, we provide the image content in JPG, image resolution, scene category, and ground truth (including gaze trajectory, fixation points, and saliency mask, for training and validation sets only).",images,Electronics,Object recognition,image,p3129,Machine Learning,,,,
LSUN Saliency Prediction (Salicon),http://lsun.cs.princeton.edu/challenge/2015/eyetracking_salicon/data/image.zip,3,v1,zip,N/A,public,,,"The data is collected via mouse cursor tracking in a new psychophysical paradigm from Amazon Mechanical Turk by NUS VIP Lab. All the images are from MS COCO dataset. For each image, we provide the image content in JPG, image resolution and ground truth (including mouse trajectory, fixation points, and saliency mask, for training and validation sets only).",images,Electronics,Object recognition,image,p3129,Machine Learning,,,,
LSUN Saliency Prediction (iSun) Ground Truth,http://lsun.cs.princeton.edu/challenge/2015/eyetracking/data/saliency.zip,12,v1,zip,N/A,public,,,"The data is collected by gaze tracking from Amazon Mechanical Turk using a web-cam. All our images are from the SUN database. For each image, we provide the image content in JPG, image resolution, scene category, and ground truth (including gaze trajectory, fixation points, and saliency mask, for training and validation sets only).",Saliency Map Ground Truth,Electronics,Object recognition,image,p3129,Machine Learning,,,,
LSUN Saliency Prediction (Salicon) Ground Truth,http://lsun.cs.princeton.edu/challenge/2015/eyetracking_salicon/data/saliency.zip,19,v1,zip,N/A,public,,,"The data is collected via mouse cursor tracking in a new psychophysical paradigm from Amazon Mechanical Turk by NUS VIP Lab. All the images are from MS COCO dataset. For each image, we provide the image content in JPG, image resolution and ground truth (including mouse trajectory, fixation points, and saliency mask, for training and validation sets only).",Saliency Map Ground Truth,Electronics,Object recognition,image,p3129,Machine Learning,,,,
LSUN Room Layout Estimation,http://lsun.cs.princeton.edu/challenge/2015/roomlayout/data/image.zip,2,v1,zip,N/A,public,,,"In this task, an algorithm needs to estimate the room layout from a single indoor scene image. All the images are indoor. They are from the SUN database and our LSUN scene classification database. We assume that a room showed in an image can be represented by a part of a 3D box. Therefore, the room layout estimation is formulated as a way to predict the positions of intersection between planar walls, ceiling and floors. There are 4000 images for training, 394 images for validation and 1000 images for testing. All the images have valid room layout that can be clearly annotated by human.",images,Electronics,Object recognition,image,p3129,Machine Learning,,,,
The Visual Genome Objects,http://visualgenome.org/static/data/dataset/objects.json.zip,0.35,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Relationships,http://visualgenome.org/static/data/dataset/relationships.json.zip,0.7,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Object Aliases,http://visualgenome.org/static/data/dataset/object_alias.txt,0.00006,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Relationship Aliases,http://visualgenome.org/static/data/dataset/relationship_alias.txt,0.000122,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Synsets for Objects,http://visualgenome.org/static/data/dataset/object_synsets.json.zip,0.000357,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Synsets for Attributes,http://visualgenome.org/static/data/dataset/attribute_synsets.json.zip,0.000171,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Synsets for Relationships,http://visualgenome.org/static/data/dataset/relationship_synsets.json.zip,0.000098,7/12/2017,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Images 01,https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip,9.2,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",image,p3130,Machine Learning,,,,
The Visual Genome Images 02,https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip,5.47,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",image,p3130,Machine Learning,,,,
The Visual Genome Image Metadata,http://visualgenome.org/static/data/dataset/image_data.json.zip,17.6,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Region Descriptions,http://visualgenome.org/static/data/dataset/region_descriptions.json.zip,0.712,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Attributes,http://visualgenome.org/static/data/dataset/attributes.json.zip,0.462,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
The Visual Genome Question Answers,http://visualgenome.org/static/data/dataset/question_answers.json.zip,0.803,8/29/2016,zip,proprietary,public,,,Very detailed visual knowledge base with deep captioning of ~100K images.,,Electronics,"Visual learning, research",text,p3130,Machine Learning,,,,
Quora Question Pairs,http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv,0.058,v1,tsv,proprietary,public,,,"Our dataset consists of over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair.",,Electronics,"Machine Learning, text",text,p3131,Machine Learning,,,,
SQuAD2.0 Training Set,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json,0.04,v1,json,Creative Commons Attribution License,public,,,"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.",,Electronics,"Machine Learning, text",text,p3132,Computer Vision,,,,
SQuAD2.0 Dev Set,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json,0.004,v1,json,Creative Commons Attribution License,public,,,"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.",,Electronics,"Machine Learning, text",text,p3132,Computer Vision,,,,
Billion Words,http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz,,v1,gzip,N/A,public,,,The purpose of the project is to make available a standard training and test setup for language modeling experiments.,,Electronics,"Machine Learning, text",text,p3133,Computer Vision,,,,
The Children's Book Test,http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz,1.7,v1,tgz,N/A,public,,,"Baseline of (Question + context, Answer) pairs extracted from Children’s books available through Project Gutenberg. Useful for question-answering, reading comprehension, and factoid look-up.",,Education,"Machine learning, question",text,p3134,Machine Learning,,,,
Stanford Sentiment Analysis,http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip,0.006,v1,zip,N/A,public,,,A standard sentiment dataset with fine-grained sentiment annotations at every node of each sentence’s parse tree.,,Electronics,"Sentiment, analysis",text,p3135,Machine Learning,,,,
20 Newsgroups,http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz,0.09,v1,gzip,N/A,public,,,"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It is one of the classic datasets for text classification, usually useful as a benchmark for either pure classification or as a validation of any IR / indexing algorithm.",,Electronics,Machine Learning,text,p3136,Machine Learning,,,,
Libri Speech 100 hours,http://www.openslr.org/resources/12/train-clean-100.tar.gz,6.3,v1,gzip,proprietary,public,,,Audio books data set of text and speech.,,Electronics,"Machine Learning, speech",audio,p3137,Machine Learning,,,,
Libri Speech 360 hours,http://www.openslr.org/resources/12/train-clean-360.tar.gz,23,v1,gzip,proprietary,public,,,Audio books data set of text and speech.,,Electronics,"Machine Learning, speech",audio,p3137,Machine Learning,,,,
Libri Speech 500 hours,http://www.openslr.org/resources/12/train-other-500.tar.gz,30,v1,gzip,proprietary,public,,,Audio books data set of text and speech.,,Electronics,"Machine Learning, speech",audio,p3137,Machine Learning,,,,
MovieLens 20M Dataset,http://files.grouplens.org/datasets/movielens/ml-20m.zip,0.19,v1,zip,proprietary,public,,,"Stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags.",,Media and Entertainment,"Machine Learning, review, ratings",text,p3138,Machine Learning,,,,
MovieLens Latest Dataset,http://files.grouplens.org/datasets/movielens/ml-latest.zip,0.224,v1,zip,proprietary,public,,,"26,000,000 ratings and 750,000 tag applications applied to 45,000 movies by 270,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags.",,Media and Entertainment,"Machine Learning, review, ratings",text,p3139,Machine Learning,,,,
WikiLens,http://files.grouplens.org/datasets/wikilens/wikilens.20080202.tar.gz,0.0034,2/1/2008,gzip,proprietary,public,,,"WikiLens was a generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items.",,Electronics,"Machine Learning, review, ratings",text,p3140,Machine Learning,,,,
Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_stereo_centre_01.tar,2.83,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,
Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_stereo_left_01.tar,2.77,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,
Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_stereo_right_01.tar,2.86,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,
Bumblebee XB3,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_vo.tar,0.44,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"front window view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1000,Self-driving cars,,,,
Grasshopper 2 Left,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_vo.tar,2.07,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"left side view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1001,Self-driving cars,,,,
Grasshopper 2 right,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_mono_right_01.tar,2.03,5/6/2014,tar,Creative Commons Attribution License,private (academia only),,initial,"right side view, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1002,Self-driving cars,,,,
LMS front,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_lms_front_01.tar,SIGN UP,5/6/2014,tar,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,private (academia only),,initial,"LMS front, streets, low traffic",Need to sign up,Automotive,"alternate, sun, cloud",image,P1003,Self-driving cars,,,,
LMS rear,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_lms_rear_01.tar,SIGN UP,5/6/2014,tar,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,private (academia only),,initial,"LMS rear, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1004,Self-driving cars,,,,
LDMRS,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_ldmrs_01.tar,SIGN UP,5/6/2014,tar,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,private (academia only),,initial,"LDMRS, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1005,Self-driving cars,,,,
GPS/IN,http://mrgdatashare.robots.ox.ac.uk/accounts/login/?next=/download/%3Ffilename%3Ddatasets/2014-05-06-12-54-54/2014-05-06-12-54-54_gps.tar,SIGN UP,5/6/2014,tar,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,private (academia only),,initial,"GPS, streets, low traffic",,Automotive,"alternate, sun, cloud",image,P1006,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"114 frames (00:11 minutes), 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc",,Automotive,city,image,P1007,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"114 frames (00:11 minutes), 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc",,Automotive,city,image,P1007,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"114 frames (00:11 minutes), 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc",,Automotive,city,"text, video",P1007,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_tracklets.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"114 frames (00:11 minutes), 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc",,Automotive,city,text,P1007,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_extract.zip,0.3,9/26/2011,zip,proprietary,public,,initial,"83 frames (00:08 minutes), 1 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1008,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0002/2011_09_26_drive_0002_extract.zip,0.6,9/26/2011,zip,N/A,public,,initial,"160 frames (00:16 minutes), 9 Cars, 3 Vans, 0 Trucks, 2 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1009,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0009/2011_09_26_drive_0009_extract.zip,1.8,9/26/2011,zip,proprietary,public,,initial,"453 frames (00:45 minutes), 89 Cars, 3 Vans, 2 Trucks, 3 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 1 Misc",,Automotive,city,image,P1010,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0011/2011_09_26_drive_0011_extract.zip,0.9,9/26/2011,zip,proprietary,public,,initial,"238 frames (00:23 minutes), 15 Cars, 1 Vans, 1 Trucks, 1 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",,Automotive,city,image,P1011,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0014/2011_09_26_drive_0014_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"320 frames (00:32 minutes), 8 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1012,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0017/2011_09_26_drive_0017_extract.zip,0.5,9/26/2011,zip,proprietary,public,,initial,"120 frames (00:12 minutes), 4 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1013,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0018/2011_09_26_drive_0018_extract.zip,1.1,9/26/2011,zip,proprietary,public,,initial,"276 frames (00:27 minutes), 11 Cars, 2 Vans, 2 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1014,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0048/2011_09_26_drive_0048_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"444 frames (00:44 minutes), 26 Cars, 15 Vans, 1 Trucks, 3 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 2 Misc",,Automotive,city,image,P1015,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0056/2011_09_26_drive_0056_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"300 frames (00:30 minutes), 13 Cars, 3 Vans, 1 Trucks, 2 Pedestrians, 0 Sitters, 1 Cyclists, 6 Trams, 2 Misc",,Automotive,city,image,P1016,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0057/2011_09_26_drive_0057_extract.zip,1.4,9/26/2011,zip,proprietary,public,,initial,"367 frames (00:36 minutes), 22 Cars, 4 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1017,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0059/2011_09_26_drive_0059_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"52 Cars, 3 Vans, 0 Trucks, 5 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",,Automotive,city,image,P1018,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0060/2011_09_26_drive_0060_extract.zip,0.3,9/26/2011,zip,proprietary,public,,initial,"84 frames (00:08 minutes), 2 Cars, 0 Vans, 0 Trucks, 1 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1019,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0084/2011_09_26_drive_0084_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"389 frames (00:38 minutes), 49 Cars, 1 Vans, 0 Trucks, 3 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 2 Misc",Imag resolution: 1392 x 512 pixels,Automotive,city,image,P1020,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0091/2011_09_26_drive_0091_extract.zip,1.3,9/26/2011,zip,proprietary,public,,initial,"346 frames (00:34 minutes), 2 Cars, 1 Vans, 0 Trucks, 42 Pedestrians, 14 Sitters, 8 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1021,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0093/2011_09_26_drive_0093_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"439 frames (00:43 minutes), 54 Cars, 2 Vans, 0 Trucks, 4 Pedestrians, 2 Sitters, 2 Cyclists, 0 Trams, 2 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1022,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0095/2011_09_26_drive_0095_extract.zip,1.1,9/26/2011,zip,proprietary,public,,initial,"274 frames (00:27 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1023,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0096/2011_09_26_drive_0096_extract.zip,1.9,9/26/2011,zip,proprietary,public,,initial,"481 frames (00:48 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1024,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0104/2011_09_26_drive_0104_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"318 frames (00:31 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1025,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0106/2011_09_26_drive_0106_extract.zip,0.9,9/26/2011,zip,proprietary,public,,initial,"233 frames (00:23 minutes), 0 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,city,image,P1026,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0117/2011_09_26_drive_0117_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,93 frames (00:09 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1027,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0117/2011_09_26_drive_0117_extract.zip,2.6,9/26/2011,zip,proprietary,public,,initial,666 frames (01:06 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1028,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0001/2011_09_28_drive_0001_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,111 frames (00:11 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1029,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_28_drive_0002/2011_09_28_drive_0002_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,382 frames (00:38 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1030,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_29_drive_0026/2011_09_29_drive_0026_extract.zip,0.6,9/26/2011,zip,proprietary,public,,initial,164 frames (00:16 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1031,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_29_drive_0071/2011_09_29_drive_0071_extract.zip,4.1,9/26/2011,zip,proprietary,public,,initial,1065 frames (01:46 minutes),Image resolution: 1392 x 512 pixels,Automotive,city,image,P1032,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0019/2011_09_26_drive_0019_extract.zip,1.9,9/26/2011,zip,proprietary,public,,initial,"487 frames (00:48 minutes), 13 Cars, 2 Vans, 0 Trucks, 3 Pedestrians, 0 Sitters, 7 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1033,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0020/2011_09_26_drive_0020_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"92 frames (00:09 minutes), 5 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1034,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0022/2011_09_26_drive_0022_extract.zip,3.1,9/26/2011,zip,proprietary,public,,initial,"806 frames (01:20 minutes), 53 Cars, 4 Vans, 1 Trucks, 2 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 3 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1035,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0023/2011_09_26_drive_0023_extract.zip,1.9,9/26/2011,zip,proprietary,public,,initial,"480 frames (00:48 minutes), 150 Cars, 6 Vans, 1 Trucks, 1 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 8 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1036,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0035/2011_09_26_drive_0035_extract.zip,0.5,9/26/2011,zip,proprietary,public,,initial,"137 frames (00:13 minutes), 23 Cars, 6 Vans, 2 Trucks, 2 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 3 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1037,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0036/2011_09_26_drive_0036_extract.zip,3.1,9/26/2011,zip,proprietary,public,,initial,"809 frames (01:20 minutes), 80 Cars, 7 Vans, 1 Trucks, 1 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1038,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0039/2011_09_26_drive_0039_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"401 frames (00:40 minutes), 35 Cars, 4 Vans, 1 Trucks, 2 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1039,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0046/2011_09_26_drive_0046_extract.zip,0.5,9/26/2011,zip,proprietary,public,,initial,"131 frames (00:13 minutes), 8 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1040,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0061/2011_09_26_drive_0061_extract.zip,2.7,9/26/2011,zip,proprietary,public,,initial,"709 frames (01:10 minutes), 39 Cars, 0 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 3 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1041,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0064/2011_09_26_drive_0064_extract.zip,2.2,9/26/2011,zip,proprietary,public,,initial,"576 frames (00:57 minutes), 38 Cars, 8 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1042,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0079/2011_09_26_drive_0079_extract.zip,0.4,9/26/2011,zip,proprietary,public,,initial,"107 frames (00:10 minutes), 2 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1043,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0086/2011_09_26_drive_0086_extract.zip,2.7,9/26/2011,zip,proprietary,public,,initial,"711 frames (01:11 minutes), 3 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1044,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0087/2011_09_26_drive_0087_extract.zip,2.8,9/26/2011,zip,proprietary,public,,initial,"735 frames (01:13 minutes), 6 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1045,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0018/2011_09_30_drive_0018_extract.zip,10.7,9/26/2011,zip,proprietary,public,,initial,2768 frames (04:36 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1046,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0020/2011_09_30_drive_0020_extract.zip,4.3,9/26/2011,zip,proprietary,public,,initial,1111 frames (01:51 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1047,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0027/2011_09_30_drive_0027_extract.zip,4.3,9/26/2011,zip,proprietary,public,,initial,1112 frames (01:51 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1048,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0028/2011_09_30_drive_0028_extract.zip,20,9/26/2011,zip,proprietary,public,,initial,5183 frames (08:38 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1049,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0033/2011_09_30_drive_0033_extract.zip,6.2,9/26/2011,zip,proprietary,public,,initial,1600 frames (02:40 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1050,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0034/2011_09_30_drive_0034_extract.zip,4.8,9/26/2011,zip,proprietary,public,,initial,1230 frames (02:03 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1051,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0027/2011_10_03_drive_0027_extract.zip,17.6,9/26/2011,zip,proprietary,public,,initial,4550 frames (07:35 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1052,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0034/2011_10_03_drive_0034_extract.zip,18,9/26/2011,zip,proprietary,public,,initial,4669 frames (07:46 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1053,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0015/2011_09_26_drive_0015_extract.zip,1.2,9/26/2011,zip,proprietary,public,,initial,"303 frames (00:30 minutes), 33 Cars, 1 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1054,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0027/2011_09_26_drive_0027_extract.zip,0.7,9/26/2011,zip,proprietary,public,,initial,"194 frames (00:19 minutes), 3 Cars, 1 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1055,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0028/2011_09_26_drive_0028_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"435 frames (00:43 minutes), 9 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 1 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1056,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0029/2011_09_26_drive_0029_extract.zip,1.7,9/26/2011,zip,proprietary,public,,initial,"3 Cars, 0 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1057,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0032/2011_09_26_drive_0032_extract.zip,1.5,9/26/2011,zip,proprietary,public,,initial,"396 frames (00:39 minutes), 21 Cars, 4 Vans, 2 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 1 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1058,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0052/2011_09_26_drive_0052_extract.zip,0.3,9/26/2011,zip,proprietary,public,,initial,"84 frames (00:08 minutes), 4 Cars, 4 Vans, 1 Trucks, 0 Pedestrians, 0 Sitters, 0 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1059,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0070/2011_09_26_drive_0070_extract.zip,1.6,9/26/2011,zip,proprietary,public,,initial,"426 frames (00:42 minutes), 2 Cars, 2 Vans, 0 Trucks, 2 Pedestrians, 0 Sitters, 2 Cyclists, 0 Trams, 0 Misc",Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1060,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0101/2011_09_26_drive_0101_extract.zip,3.6,9/26/2011,zip,proprietary,public,,initial,941 frames (01:34 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1061,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_29_drive_0004/2011_09_29_drive_0004_extract.zip,1.3,9/26/2011,zip,proprietary,public,,initial,345 frames (00:34 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1062,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_30_drive_0016/2011_09_30_drive_0016_extract.zip,1.1,9/26/2011,zip,proprietary,public,,initial,285 frames (00:28 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1063,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0042/2011_10_03_drive_0042_extract.zip,4.5,9/26/2011,zip,proprietary,public,,initial,1176 frames (01:57 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1064,Self-driving cars,,,,
Raw Data Sets KITTI Benchmark Solution Self-driving cars,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_extract.zip,3.3,9/26/2011,zip,proprietary,public,,initial,844 frames (01:24 minutes),Image resolution: 1392 x 512 pixels,Automotive,residential,image,P1065,Self-driving cars,,,,
Ford Campus Vision and Lidar Data Set,robots.engin.umich.edu/uploads/SoftwareData/Ford/dataset-1.tar.gz,78,11/1/2009,tar,N/A,public,,initial,"large and small-scale loop closures, Ford Research campus and downtown Dearborn, Michigan","professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system",Automotive,"GPS, campus, ford research, downtown",image,P1066,Self-driving cars,,,,
Ford Campus Vision and Lidar Data Set,robots.engin.umich.edu/uploads/SoftwareData/Ford/dataset-2.tar.gz,119,11/1/2009,tar,N/A,public,,initial,a loop inside the Ford campus in Dearborn Michigan,"professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system",Automotive,"GPS, campus, ford research, downtown",image,P1067,Self-driving cars,,,,
Ford Campus Vision and Lidar Data Set,robots.engin.umich.edu/uploads/SoftwareData/Ford/dataset-1-subset.tgz,6,11/1/2009,tar,N/A,public,,initial,"large and small-scale loop closures, Ford Research campus and downtown Dearborn, Michigan","professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system",Automotive,"GPS, campus, ford research, downtown",image,P1068,Self-driving cars,,,,
The CCSAD dataset,https://www.dropbox.com/s/uhl5j354ncfesdu/S1A-20140527_151946.zip?dl=0,6.2,5/27/2014,zip,N/A,public,,Sequence S1A,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/v2f3yugihwaaxyt/S1B-20140527_153617.zip?dl=0,4.5,5/27/2014,zip,N/A,public,,Sequence S1B,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/6e55lfkeqsx3zm4/S1D-20140527_160412.zip?dl=0,4.9,5/27/2014,zip,N/A,public,,Sequence S1D,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/xb0lvr4bo19kddx/S1E-20140527_162403.zip?dl=0,4.5,5/27/2014,zip,N/A,public,,Sequence S1E,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/om4v4kdxpdu9oh0/S1F-20140527_163102.zip?dl=0,5.6,5/27/2014,zip,N/A,public,,Sequence S1F,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/4is8rx0tbfb8z0w/S1G-20140527_163836.zip?dl=0,4.6,5/27/2014,zip,N/A,public,,Sequence S1G,Urban Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1069,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/rrd1f3cksj7ow5t/S1C-20140527_154854.zip?dl=0,4.5,5/27/2014,zip,N/A,public,,Sequence S1C,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/ydjoj2gfi8qwawc/S1H-20140527_164804.zip?dl=0,5.6,5/27/2014,zip,N/A,public,,Sequence S1H,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/7f18payobkg7b4w/S1I-20140527_165728.zip?dl=0,4.1,5/27/2014,zip,N/A,public,,Sequence S1I,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/xrdvlk2jwmeyvlv/S1J-20140527_170958.zip?dl=0,6.9,5/27/2014,zip,N/A,public,,Sequence S1J,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/mfz20gazymkh5bt/S1K-20140527_172001.zip?dl=0,5.7,5/27/2014,zip,N/A,public,,Sequence S1K,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/3fu1xlq3dkke6hk/S1L-20140527_172711.zip?dl=0,4.1,5/27/2014,zip,N/A,public,,Sequence S1L,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/otpii6h5zd63qk0/S2C-20140604_152758.zip?dl=0,7.2,5/27/2014,zip,N/A,public,,Sequence S2C,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/q98vplqlmjnzurx/S2D-20140604_153453.zip?dl=0,7.4,5/27/2014,zip,N/A,public,,Sequence S2D,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/7rveus8mxaxayjc/S2E-20140604_155133.zip?dl=0,8.5,5/27/2014,zip,N/A,public,,Sequence S2E,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/zysmz33yl4vme3v/S2H-20140604_164515.zip?dl=0,9.3,5/27/2014,zip,N/A,public,,Sequence S2H,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/1dqgc3zgppry7hx/S3F-20140704_190936.zip?dl=0,7.2,5/27/2014,zip,N/A,public,,Sequence S3F,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/ksga8hh1fz7hwgn/S3G-20140704_191748.zip?dl=0,6.6,5/27/2014,zip,N/A,public,,Sequence S3G,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/0q4hspny9uvcksd/S3S-20140705_012726.zip?dl=0,6.8,5/27/2014,zip,N/A,public,,Sequence S3S,Avenues and Small Roads,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1070,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/3m9k8slmm7t4ktm/S2A-20140604_144706.zip?dl=0,6,5/27/2014,zip,N/A,public,,Sequence S2A,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/y026offhq7zckcb/S2B-20140604_151558.zip?dl=0,7.5,5/27/2014,zip,N/A,public,,Sequence S2B,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/8q2h6lg5nrnfwld/S2F-20140604_161757.zip?dl=0,8,5/27/2014,zip,N/A,public,,Sequence S2F,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/4eopiqjvyhjsxkl/S2G-20140604_162539.zip?dl=0,7.7,5/27/2014,zip,N/A,public,,Sequence S2G,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/fndyv96tsptnzoh/S2I-20140604_165737.zip?dl=0,7.1,5/27/2014,zip,N/A,public,,Sequence S2I,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/w6gksoajngmxucn/S2J-20140604_170400.zip?dl=0,6.7,5/27/2014,zip,N/A,public,,Sequence S2J,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/caigrl4vogx7b23/S3A-20140704_181506.zip?dl=0,5.6,5/27/2014,zip,N/A,public,,Sequence S3A,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/v1rizjc66okhqke/S3B-20140704_182151.zip?dl=0,5.6,5/27/2014,zip,N/A,public,,Sequence S3B,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/4gjx6gni91qzlyj/S3C-20140704_182720.zip?dl=0,6,5/27/2014,zip,N/A,public,,Sequence S3C,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/0fbchwr930a3f7v/S3D-20140704_183242.zip?dl=0,6.3,5/27/2014,zip,N/A,public,,Sequence S3D,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/t8f7mowvutvcu9i/S3E-20140704_190046.zip?dl=0,6.6,5/27/2014,zip,N/A,public,,Sequence S3E,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/suhkhcpoj8sb9ir/S3H-20140704_192518.zip?dl=0,7.6,5/27/2014,zip,N/A,public,,Sequence S3H,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/jnw6r52cxnf3b9b/S3I-20140704_195540.zip?dl=0,7.9,5/27/2014,zip,N/A,public,,Sequence S3I,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/snx75w43cxiziwm/S3J-20140704_200203.zip?dl=0,7,5/27/2014,zip,N/A,public,,Sequence S3J,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/g4aj1chwab21a40/S3K-20140704_201624.zip?dl=0,3.8,5/27/2014,zip,N/A,public,,Sequence S3K,Colonial Town Streets,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1071,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/hgav8b09kegp60y/S3L-20140705_000919.zip?dl=0,6.8,5/27/2014,zip,N/A,public,,Sequence S3L,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/0lw0a0l3ianbgbc/S3M-20140705_001513.zip?dl=0,4.8,5/27/2014,zip,N/A,public,,Sequence S3M,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/vmgdzborashkxat/S3N-20140705_002134.zip?dl=0,6.2,5/27/2014,zip,N/A,public,,Sequence S3N,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/oo9vp8vim9275fc/S3O-20140705_002829.zip?dl=0,5.9,5/27/2014,zip,N/A,public,,Sequence S3O,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/gzeed4kj790irwx/S3P-20140705_003440.zip?dl=0,5.2,5/27/2014,zip,N/A,public,,Sequence S3P,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/nm66meabt8xrr0f/S3Q-20140705_003929.zip?dl=0,7.2,5/27/2014,zip,N/A,public,,Sequence S3Q,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,
The CCSAD dataset,https://www.dropbox.com/s/z9nif9jxpan7xai/S3R-20140705_011358.zip?dl=0,6.8,5/27/2014,zip,N/A,public,,Sequence S3R,Tunnel Network,,Automotive,"abundant potholes, speed bumpers and peculiar flows of pedestrians","image, text",P1072,Self-driving cars,,developing countries,,
MIT Age Labe dataset,https://www.dropbox.com/s/6h6bojpurnzus53/mit-agelab-sync-sample-car-dataset.zip?dl=1,2,1/1/2016,zip,MIT License,public,,initial,"contain the video for front, dash, and face videos,","supposedly MIT license, as it's directed by MIT AGE LAB",Automotive,"1,000+ hours of multi-sensor driving datasets",video,P1073,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/DefinedTS.tar.gz,0.0013,1/1/2011,tar,proprietary,public,,initial,Defined Traffic Signs,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,defined traffic signs,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera00.tar,0.22,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera01.tar,0.98,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera02.tar,0.36,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera03.tar,0.28,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera04.tar,0.73,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera05.tar,0.63,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera06.tar,0.18,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/camera07.tar,0.23,1/1/2011,tar,proprietary,public,,initial,annotations,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Annotations/annotations.tar,0.0017,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar00,1,1/1/2011,tar,N/A,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar01,1,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar02,1,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar03,1,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/NonTSImages/NonTS_TrainingBG.tar03,1,1/1/2011,tar,proprietary,public,,initial,background images,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_00.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_01.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_02.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_03.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_04.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_05.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_06.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq01_07.tar,0.866,1/1/2011,tar,proprietary,public,,Sequence 01,"annotations, 3001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_00.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_00.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_01.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_01.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_02.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_02.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_03.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_03.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_04.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_04.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_05.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_05.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_06.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_06.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_07.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq02_07.tar01,0.79,1/1/2011,tar,proprietary,public,,Sequence 02,"annotations, 6001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_00.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_01.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_02.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_03.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_04.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_05.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_06.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq03_07.tar,0.6,1/1/2011,tar,proprietary,public,,Sequence 03,"annotations, 2001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_00.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_00.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_01.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_01.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_02.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_02.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_03.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_03.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_04.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_04.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_05.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_05.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_06.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_06.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_07.tar00,1,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
BelgiumTS Dataset,https://btsd.ethz.ch/shareddata/BelgiumTS/Seqs/Seq04_07.tar01,0.16,1/1/2011,tar,proprietary,public,,Sequence 04,"annotations, 4001 frames","If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,annotations,image,P1074,Self-driving cars,,,,
hciLab driving dataset,https://www.hcilab.org/wp-content/uploads/hcilab_driving_dataset.zip,0.036,10/30/2013,zip,Open Database License,public,,initial,Instance-level & Pixel-level labels,"If you use the material from this dataset, please cite one of the following publications, accordingly",Automotive,"road types, exits, on-ramps, workload",image,P1075,Self-driving cars,,,,
Joint Attention for Autonomous Driving (JAAD) Dataset,data.nvision2.eecs.yorku.ca/JAAD_dataset/data/JAAD_clips.zip,3.1,3/6/2018,zip,proprietary,public,,4,"videos 1-60: GoPro Hero+ (1920x1980), videos 61-70: Highscreen Box Connect (1280x720), videos 71-346: Garmin GDR-35 (1920x1980)",Annotations for the dataset can be downloaded from our GitHub repo (https://github.com/ykotseruba/JAAD_dataset),Automotive,"pedestrians, behaviour",video,P1076,Self-driving cars,,,,
Predicting Driver Intent from Models of Naturalistic Driving in Intelligent Transportation Systems,its.acfr.usyd.edu.au/wordpress/wp-content/uploads/2014/05/20150423T032510_ivssg-2_EKF_ALL_GNSS.csv.gz,0.0092,1/1/2015,zip,N/A,public,,initial,"This is a rich dataset derived from a vehicle driving around urban streets around the Australian Centre for Field Robotics in Sydney. The data were collected by a system fusing GPS and dead reckoning information from gyroscopes and odometry, at a resolution of 10 hertz.",,Automotive,"urban streets, Australia",GPS,P1077,Self-driving cars,,,,
Predicting Driver Intent from Models of Naturalistic Driving in Intelligent Transportation Systems,its.acfr.usyd.edu.au/wordpress/wp-content/uploads/2014/05/20150414T041122_ivssg-2_EKF_All_GNSS.csv.gz,0.026,1/1/2015,zip,N/A,public,,initial,"This is a rich dataset derived from a vehicle driving around urban streets around the Australian Centre for Field Robotics in Sydney. The data were collected by a system fusing GPS and dead reckoning information from gyroscopes and odometry, at a resolution of 10 hertz.",,Automotive,"urban streets, Australia",GPS,P1077,Self-driving cars,,,,
Predicting Driver Intent from Models of Naturalistic Driving in Intelligent Transportation Systems,its.acfr.usyd.edu.au/wordpress/wp-content/uploads/2014/05/20150323T041228_ivssg-2_EKF_All_GNSS.csv.gz,0.02,1/1/2015,zip,N/A,public,,initial,"This is a rich dataset derived from a vehicle driving around urban streets around the Australian Centre for Field Robotics in Sydney. The data were collected by a system fusing GPS and dead reckoning information from gyroscopes and odometry, at a resolution of 10 hertz.",,Automotive,"urban streets, Australia",GPS,P1077,Self-driving cars,,,,
DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving,deepdriving.cs.princeton.edu/DeepDrivingCode_v2.zip,1.2,1/1/2015,zip,N/A,public,,2,"Source Code and Setup File, map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving",,Automotive,"Convolutional Neural Network, direct perception based approach",image,P1078,Self-driving cars,,,,
DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving,deepdriving.cs.princeton.edu/TORCS_trainset.zip,50.2,1/1/2015,zip,N/A,public,,inital,"the images are in the integer data field, while the corresponding labels are in the float data field",,Automotive,"Convolutional Neural Network, direct perception based approach",image,P1078,Self-driving cars,,,,
DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving,deepdriving.cs.princeton.edu/TORCS_baseline_testset.zip,8.2,1/1/2015,zip,N/A,public,,inital,A fixed testset to compare with the baselines,Our real testing is to let the ConvNet to drive the car in the game. So the testing images are generated on-the-fly,Automotive,"Convolutional Neural Network, direct perception based approach",GIST,P1078,Self-driving cars,,,,
"Autonomous Vehicle Survey of Bicyclists and Pedestrians in Pittsburgh, 2017",https://data.wprdc.org/dataset/0b584c84-7e35-4f4d-a5a2-b01697470c0f/resource/6d29ac78-12b8-4e1d-b325-6edeef59b593/download/bikepghmembers.csv,N/A,5/5/2017,csv,Creative Commons Attribution License,public,,inital,"Our survey asked participants how they feel about being a fellow road user with AVs, either walking or biking, 1119 responses",,Automotive,"survey, pittsburgh",text,P1079,Self-driving cars,,,,
"Autonomous Vehicle Survey of Bicyclists and Pedestrians in Pittsburgh, 2017",https://data.wprdc.org/dataset/0b584c84-7e35-4f4d-a5a2-b01697470c0f/resource/e95dd941-8e47-4460-9bd8-1e51c194370b/download/bikepghpublic.csv,N/A,5/5/2017,csv,Creative Commons Attribution License,public,,inital,"Our survey asked participants how they feel about being a fellow road user with AVs, either walking or biking, 1119 responses",,Automotive,"survey, pittsburgh",text,P1079,Self-driving cars,,,,
"Autonomous Vehicle Survey of Bicyclists and Pedestrians in Pittsburgh, 2017",https://data.wprdc.org/dataset/0b584c84-7e35-4f4d-a5a2-b01697470c0f/resource/960fb9fb-371b-421a-b3e3-78dec5b3e6bd/download/bikepghdatadictionary.csv,N/A,5/5/2017,csv,Creative Commons Attribution License,public,,inital,"Our survey asked participants how they feel about being a fellow road user with AVs, either walking or biking, 1119 responses",,Automotive,"survey, pittsburgh",text,P1079,Self-driving cars,,,,
the comma.ai driving dataset,https://archive.org/download/comma-dataset/comma-dataset.zip,45,8/2/2016,zip,Creative Commons Attribution License,public,,inital,7 and a quarter hours of largely highway driving. Enough to train what we had in Bloomberg.,,Automotive,"speed, acceleration, steering angle, GPS coordinates, gyroscope angles",video,P1080,Self-driving cars,,,,
1 iROADS Dataset (Intercity Roads and Adverse Driving Scenarios),https://www.cs.auckland.ac.nz/~m.rezaei/Publications/iROADS%20Dataset-Full%20Set.zip,1.5,v1,zip,N/A,public,,initial,"Images: Daylight 903, Night 1050 , Rainy day 1049, Rainy night 431, Snowy 569, Sun strokes 307, Tunnel 347","Sequence Length (Frames): 4656, Colour / Grey Colour, Colour Depth: 24‐bit, Resolution (pixels): 640 x 360, Stereo Rectified: NA, Ego‐motion data: NA",Automotive,"various daytimes, different weather conditions, light, sun, rain, clouds",image,P1081,Self-driving cars,,,,
Places,http://places.csail.mit.edu/model/placesCNN_upgraded.tar.gz,0.234,v1,gzip,proprietary,public,,,"205 scene categories and 2.5 millions of images with a category label. Using convolutional neural network (CNN), we learn deep scene features for scene recognition tasks, and establish new state-of-the-art performances on scene-centric benchmarks.",,Electronics,Machine learning,image,p3141,Computer Vision,,,,
AN4,http://www.speech.cs.cmu.edu/databases/an4/an4_raw.bigendian.tar.gz,0.095,v1,gzip,proprietary,public,,,948 training and 130 test utterances,,Electronics,Machine learning,audio,p3142,Computer Vision,,,,
BSDS images,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz,0.022,v1,tar,proprietary,public,,,"12,000 hand-labeled segmentations of 1,000 Corel dataset images from 30 human subjects. Half of the segmentations were obtained from presenting the subject with a color image; the other half from presenting a grayscale image. The public benchmark based on this data consists of all of the grayscale and color segmentations for 300 images. The images are divided into a training set of 200 images, and a test set of 100 images.",,Electronics,Machine learning,image,p3143,Computer Vision,,,,
BSDS segmentations,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-human.tgz,0.098,v1,tar,proprietary,public,,,"12,000 hand-labeled segmentations of 1,000 Corel dataset images from 30 human subjects. Half of the segmentations were obtained from presenting the subject with a color image; the other half from presenting a grayscale image. The public benchmark based on this data consists of all of the grayscale and color segmentations for 300 images. The images are divided into a training set of 200 images, and a test set of 100 images.",,Electronics,Machine learning,text,p3143,Computer Vision,,,,
CIFAR-10,https://www.cs.toronto.edu/~kriz/cifar.html,0.186,v1,gzip,proprietary,public,,,"The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.",,Electronics,Machine learning,image,p3144,Computer Vision,,,,
Oxford 102 Flowers 01,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz,0.363,v1,gzip,proprietary,public,,,This set contains images of flowers belonging to 102 different categories. The images were acquired by searching the web and taking pictures. There are a minimum of 40 images for each category.,images,Electronics,Machine learning,image,p3145,Computer Vision,,,,
Oxford 102 Flowers 02,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102segmentations.tgz,0.25,v1,gzip,proprietary,public,,,This set contains images of flowers belonging to 102 different categories. The images were acquired by searching the web and taking pictures. There are a minimum of 40 images for each category.,segmentations,Electronics,Machine learning,text,p3145,Computer Vision,,,,
Oxford 102 Flowers 03,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/distancematrices102.mat,2,v1,mat,proprietary,public,,,This set contains images of flowers belonging to 102 different categories. The images were acquired by searching the web and taking pictures. There are a minimum of 40 images for each category.,distances,Electronics,Machine learning,text,p3145,Computer Vision,,,,
Oxford 102 Flowers 04,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat,0.000004,v1,mat,proprietary,public,,,This set contains images of flowers belonging to 102 different categories. The images were acquired by searching the web and taking pictures. There are a minimum of 40 images for each category.,image labels,Electronics,Machine learning,text,p3145,Computer Vision,,,,
Oxford 102 Flowers 05,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat,0.000016,v1,mat,proprietary,public,,,This set contains images of flowers belonging to 102 different categories. The images were acquired by searching the web and taking pictures. There are a minimum of 40 images for each category.,data splits,Electronics,Machine learning,text,p3145,Computer Vision,,,,
Annotated Driving Dataset,bit.ly/udacity-annoations-crowdai,1.5,v1,tar,N/A,public,,,"Mountain View California and neighboring cities during daylight conditions. 65,000 labels across 9,423 frames collected from a Point Grey research cameras running at full resolution of 1920x1200 at 2hz.",,automotive ,"Car, Truck, Pedestrian",text,p1083,,,,,
Annotated Driving Dataset,bit.ly/udacity-annotations-autti,3.3,v1,tar,N/A,public,,," Mountain View California and neighboring cities during daylight conditions. 15,000 frames collected from a Point Grey research cameras running at full resolution of 1920x1200 at 2hz.",,automotive ,"Car, Truck, Pedestrian",text,p1084,,,,,
Velodyne SLAM - Dataset,www.mrt.kit.edu/z/publ/download/velodyneslam/data/result_scenario1.zip,0.236,v1,zip,N/A,public,,,"recorded with the Velodyne HDL64E-S2 scanner in the city of Karlsruhe, Germany",scenario 1,automotive ,,p3d,p1085,,,,,
Velodyne SLAM - Dataset,www.mrt.kit.edu/z/publ/download/velodyneslam/data/result_scenario2.zip,0.18,v1,zip,N/A,public,,,"recorded with the Velodyne HDL64E-S2 scanner in the city of Karlsruhe, Germany",scenario 2,automotive ,,p3d,p1086,,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-01.zip,0.883,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-02.zip,2,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-03.zip,0.796,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-04.zip,0.653,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-05.zip,4.4,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-06.zip,4.4,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-07.zip,2.2,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-08.zip,10,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-09.zip,0.968,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-10.zip,17,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-11.zip,2.3,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-12.zip,8.5,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-13.zip,33,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-14.zip,1.9,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
The Málaga Stereo and Laser Urban Data Set,http://ingmec.ual.es/~jlblanco/malaga-urban-dataset/extracts/malaga-urban-dataset-extract-15.zip,1.3,v3,zip,N/A,,,,"This dataset was gathered entirely in urban scenarios with a car equipped with several sensors, including one stereo camera (Bumblebee2) and five laser scanners. One distinctive feature of the present dataset is the existence of high-resolution stereo images grabbed at high rate (20fps) during a 36.8km trajectory, turning the dataset into a suitable benchmark for a variety of computer vision techniques. Both plain text and binary files are provided, as well as open source tools for working with the binary versions.",,Travel and transportation,"urban, stereo camera, high-resolution stero images","image, .rawlog",P5001,Robotics,,,,
Dataset for Mobile Robotics Olfaction,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/DataSet_for_Odor_Classification.tar.gz/download,0.0107,v1,gzip,N/A,,,,A collectionof 3 data-sets for Odor Classification with a Mobile Robot: Classification_DataSet_1 --> Controlled gas pulses. Classification_DataSet_2 --> Classification with different gas sensors (MCE-nose). Classification_DataSet_3 --> Classification in turbulent environments.,,Electronics,"eNose, gas, odor","text, .rawlog",P5002,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_360.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_desk2.tgz,0.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_desk.tgz,0.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_floor.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_ir_calibration.tgz,0.6,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_plant.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_rgb_calibration.tgz,0.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_room.tgz,0.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_rpy.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_teddy.tgz,0.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg1_xyz.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_360_hemisphere.tgz,1.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_360_kidnap.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_coke.tgz,1.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_desk.tgz,1.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_desk_with_person.tgz,2.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_dishes.tgz,1.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_flowerbouquet_brownbackground.tgz,1.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_flowerbouquet.tgz,1.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_ir_calibration.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_large_checkerboard_calibration.tgz,1.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_large_no_loop.tgz,1.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_large_with_loop.tgz,2.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_metallic_sphere2.tgz,1.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_metallic_sphere.tgz,1,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_360.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_slam2.tgz,1.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_slam3.tgz,0.9,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_pioneer_slam.tgz,1.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_rgb_calibration.tgz,1.1,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_rpy.tgz,2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_teddy.tgz,1.1,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg2_xyz.tgz,2.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_texture_near.tgz,0.6,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_texture_far.tgz,0.6,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_notexture_near.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_structure_notexture_far.tgz,0.4,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_nostructure_texture_far.tgz,0.3,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_nostructure_notexture_far.tgz,0.2,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_walking_xyz.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_walking_static.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_sitting_halfsphere.tgz,0.7,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_sitting_xyz.tgz,0.8,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Collection of Kinect (RGB+D) datasets with 6D ground truth,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/RGBD-datasets/rawlog_rgbd_dataset_freiburg3_sitting_static.tgz,0.5,v1,tgz,Creative Commons Attribution License,,,,"This dataset is a derived work from the collection [1] published by the CVPR team in the TUM University. According to the original ""Creative Commons Attribution"" license, this derived work is also released under identical terms. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].",,Travel and transportation,"Kinect, 6D, ground truth","image, .rawlog",P5003,Robotics,,,,
Málaga dataset 2009 – Campus 0L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_parking_2L.zip/download,2.1,v1,zip,N/A,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5004,Robotics,,,,
Málaga dataset 2009 – Campus 2L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_campus_2L.zip/download,3.6,v1,zip,N/A,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5005,Robotics,,,,
Málaga dataset 2009 – Campus RT,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_campus_RT.zip/download,1.9,v1,zip,N/A,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5006,Robotics,,,,
Málaga dataset 2009 – Parking 0L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_parking_0L.zip/download,1.3,v1,zip,N/A,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5007,Robotics,,,,
Málaga dataset 2009 – Parking 2L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_parking_2L.zip/download,2.1,v1,zip,N/A,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5008,Robotics,,,,
Málaga dataset 2009 – Parking 6L,http://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/malaga2009_parking_6L.zip/download,2.3,v1,zip,N/A,,,,"The lack of publicly accessible datasets with a reliable ground truth has prevented in the past a fair and coherent comparison of different methods proposed in the mobile robot Simultaneous Localization and Mapping (SLAM) literature. Providing such a ground truth renders specially challenging in the case of visual SLAM, where the world model is 3-dimensional and the robot path is 6-dimensional. This work addresses both the practical and theoretical issues found while building a collection of six outdoor datasets. It is discussed how to estimate the 6-d vehicle path from readings of a set of three Real Time Kinematics (RTK) GPS receivers, as well as the associated uncertainty bounds that can be employed to evaluate the performance of Visual SLAM methods. The vehicle was also equipped with several laser scanners, from which reference point clouds are built as a test-bed for other algorithms such as segmentation or surface fitting. All the datasets, calibration information and associated software tools are available for download.","2xSICK LMS, 2xHokuyo scanners, 2xCameras, 3xRTK GPS, IMU",Travel and transportation,"SLAM, 6D ground truth, RTK GPS","image, .rawlog",P5009,Robotics,,,,
Málaga 2006 campus dataset,http://downloads.sourceforge.net/mrpt/dataset_malaga20060121.tgz,0.0046,v1,tgz,N/A,,,,"Most successful works in Simultaneous Localization and Mapping (SLAM) aim to build a metric map under a probabilistic viewpoint employing Bayesian filtering techniques. This work introduces a new hybrid metrictopological approach, where the aim is to reconstruct the path of the robot in a hybrid continuous-discrete state space which naturally combines metric and topological maps. Our fundamental contributions are: (i) the estimation of the topological path, an improvement similar to that of RaoBlackwellized Particle Filters (RBPF) and FastSLAM in the field of metric map building; and (ii) the application of grounded methods to the abstraction of topology (including loop closure) from raw sensor readings. It is remarkable that our approach could be still represented as a Bayesian inference problem, becoming an extension of purely metric SLAM.","Odometry, SICK LMS",Travel and transportation,"SLAM, RBPF, FastSLAM, maps, mobile robots","image, .rawlog",P5010,Robotics,,,,
3 horizontal lasers,http://downloads.sourceforge.net/mrpt/dataset_threelasers_20070517.tgz,0.00056,v1,tgz,N/A,,,,"Sensors: Odometry, SICK LMS, 2x Hokuyo LMS",Robot: SENA PATH LENGTH: 20m RAWLOG ENTRIES: 505,Travel and transportation,"Odometry, SICK LMS, 2x Hokuyo LMS","image, .rawlog",P5011,Robotics,,,,
"With eNose, at Malaga Office 2.2.30",http://downloads.sourceforge.net/mrpt/dataset_laser_eNoses_20061218.tgz,0.0013,v1,tgz,N/A,,,,"Odometry, SICK LMS, Hokuyo, 2x eNoses",PATH LENGTH: 21m RAWLOG ENTRIES: 1768.,Electronics,"eNose, SICK, LMS, odometry","image, .rawlog",P5012,Robotics,,,,
"Málaga, corridor 2.3",http://downloads.sourceforge.net/mrpt/dataset_malaga_floor2.3_2lasers_stereo.tgz,0.086,v1,tgz,N/A,,,,"Odometry, SICK LMS, Hokuyo, Stereo",PATH LENGTH: 175m. RAWLOG ENTRIES: 3768.,Electronics,"Odometry, SICK LMS, Hokuyo, Stereo","image, .rawlog",P5013,Robotics,,,,
"Málaga, corridor 2.3 (vertical laser)",http://downloads.sourceforge.net/mrpt/dataset_malaga_corridor2.3_vertical_laser_20060120.tgz,0.01,v1,tgz,N/A,,,,"Odometry, SICK LMS (horz), Hokuyo (vert.)",PATH LENGTH: 385m. RAWLOG ENTRIES: 2190.,Electronics,"Odometry, SICK LMS, Hokuyo, Stereo","image, .rawlog",P5014,Robotics,,,,
Long walk in corridor 2.3,http://downloads.sourceforge.net/mrpt/dataset_malaga_large_floor2.3_1laser_20070517.tgz,0.0021,v1,tgz,N/A,,,,"Odometry, SICK LMS, Hokuyo",PATH LENGTH: 290m. RAWLOG ENTRIES: 2444.,Electronics,"Odometry, SICK LMS, Hokuyo, Stereo","image, .rawlog",P5015,Robotics,,,,
Kenmore,http://downloads.sourceforge.net/mrpt/dataset_kenmore_pradoroof_20061120.tar.gz,0.0925,v1,gzip,N/A,,,,"Robot/Vehicle: A Toyota Prado with two SICK LMS on its roof. PATH LENGTH: 18 Km approx. RAWLOG ENTRIES: 262850. See also the next rawlog for artificially added odometry from scan matching. AUTHORS: Michael Bosse, from Australia's Commonwealth Scientific and Industrial Research Organisation (CSIRO)","2xSICK LMS; Suburban streets in Kenmore, QLD, Australia (2006-NOV-20).",Travel and transportation,"SICK LMS, Rawlog","image, .rawlog",P5016,Robotics,,,,
Kenmore (decimated + estimated odometry),http://downloads.sourceforge.net/mrpt/dataset_kenmore_pradoroof_20061120_decimated+odometry.tgz,0.0103,v1,tgz,N/A,,,,"Robot/Vehicle: A Toyota Prado with two SICK LMS on its roof. PATH LENGTH: 18 Km approx. RAWLOG ENTRIES: 26283. AUTHORS OF THE ORIGINAL DATASET: Michael Bosse, from Australia's Commonwealth Scientific and Industrial Research Organisation (CSIRO).","2xSICK LMS; Suburban streets in Kenmore, QLD, Australia (2006-NOV-20).",Travel and transportation,"SICK LMS, Rawlog","image, .rawlog",P5017,Robotics,,,,
Edmonton 2002,http://downloads.sourceforge.net/mrpt/dataset_edmonton_20020728.tar.gz,0.002,v1,gzip,N/A,,,,"Odometry, SICK LMS","PATH LENGTH: 327m approx. RAWLOG ENTRIES: 6010. AUTHORS: Nick Roy, MIT",Travel and transportation,"SICK LMS, Rawlog, Odometry","image, .rawlog",P5018,Robotics,,,,
The Victoria Park,http://downloads.sourceforge.net/mrpt/dataset_victoria_park.tar.gz,0.002,v1,gzip,N/A,,,,"Odometry, SICK LMS, GPS","PATH LENGTH: 4Km approx. RAWLOG ENTRIES: 4832. AUTHORS: Dr. Jose Guivant, from the University of Sydney",Travel and transportation,"SICK LMS, Rawlog, GPS","image, .rawlog",P5019,Robotics,,,,
A. Davison’s Monoslam test video,http://downloads.sourceforge.net/mrpt/dataset_monoslam_davison.tar.gz,0.032,v1,gzip,N/A,,,,"Monoslam test video recorded with camera in an indoor, office scenario","PATH LENGTH: A few meters. RAWLOG ENTRIES: 1000. AUTHORS: Dr. Andrew Davison, from the Imperial College London",Electronics,"Monoslam, Camera, Office, Rawlog","image, .rawlog",P5020,Robotics,,,,
Intel (2003),http://downloads.sourceforge.net/mrpt/dataset_intel.tar.gz,0.0018,v1,gzip,N/A,,,,Interior of the Intel Research Lab in Seattle (2003),"PATH LENGTH: 506m approx. RAWLOG ENTRIES: 5452. AUTHORS: Dieter Fox, University of Washington",Travel and transportation,"Odometry, SICK LMS","image, .rawlog",P5021,Robotics,,,,
fr079,https://sourceforge.net/projects/mrpt/files/Datasets%20%28Rawlogs%29/Datasets/dataset_fr079.tar.gz/download,0.0068,v1,gzip,N/A,,,,"This dataset was recorded by Cyrill Stachniss in Building 079 at the University of Freiburg. This is a converted version from CARMEN logs, which were downloaded from: - D. Holz & S. Behnke: http://www.ais.uni-bonn.de/~holz/spmicp/ - G.D. Tipaldi : http://www.openslam.org/flirtlib.html Note: This dataset package includes an already built simplemap and gridmap (as binary object and as image), and two rawlogs with original and with ""corrected"" odometry.",,Travel and transportation,"Odometry, SICK LMS","image, .rawlog",P5022,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2009_09_08_drive_0010.zip,0.7,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2009_09_08_drive_0012.zip,1.5,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2009_09_08_drive_0015.zip,0.5,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2009_09_08_drive_0016.zip,0.7,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2009_09_08_drive_0019.zip,0.7,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2009_09_08_drive_0021.zip,0.9,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2009_12_14_drive_0051.zip,0.5,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_04_drive_0032.zip,0.3,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_04_drive_0033.zip,0.2,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_04_drive_0041.zip,0.2,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_04_drive_0042.zip,0.2,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_05_drive_0017.zip,0.4,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_05_drive_0023.zip,0.4,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_09_drive_0019.zip,0.2,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_09_drive_0020.zip,0.3,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_09_drive_0023.zip,0.061,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_09_drive_0051.zip,0.3,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_09_drive_0081.zip,0.2,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_09_drive_0082.zip,0.3,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Karlsruhe Dataset: Stereo Video Sequences,http://downloads.sourceforge.net/project/mrpt/Datasets%20%28Rawlogs%29/Datasets/Karlsruhe_sequences/2010_03_17_drive_0046.zip,0.5,v1,.rawlog,N/A,,,,"This page contains high-quality stereo sequences recorded from a moving vehicle in Karlsruhe. The sequences, which are captured by Pointgrey Flea2 firewire cameras, are saved as rectified images in *.png format, ground truth odometry from an OXTS RT 3000 GPS/IMU system is provided in a separate text file. This dataset is a derived work from the collection [1] published by Andreas Geiger et al. The dataset has been converted into the Rawlog format [2] and published in the aim of it being useful to MRPT users. It can be visualized and manipulated with MRPT tools, available for download at [3].;",Sensor: stereo camera,Travel and transportation,"Odometry, moving vehicle","image, .rawlog",P5023,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/teach1/ABL_Dataset_teach1_Header.zip,0.000155,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","teach 1, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/teach1/ABL_Dataset_teach1_Imagestack.zip,10,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","teach 1, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/teach1/ABL_Dataset_teach1_SURF.zip,1.3,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","teach 1, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run1/ABL_Dataset_run1_Header.zip,0.000159,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 1, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run1/ABL_Dataset_run1_Imagestack.zip,10.2,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 1, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run1/ABL_Dataset_run1_SURF.zip,1.3,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 1, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run4/ABL_Dataset_run4_Header.zip,0.000119,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 4, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run4/ABL_Dataset_run4_Imagestack.zip,9.7,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 4, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run4/ABL_Dataset_run4_SURF.zip,1.3,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 4, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run5/ABL_Dataset_run5_Header.zip,0.000159,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 5, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run5/ABL_Dataset_run5_Imagestack.zip,12.4,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 5, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run5/ABL_Dataset_run5_SURF.zip,1.5,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 5, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run6/ABL_Dataset_run6_Header.zip,0.000209,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 6, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run6/ABL_Dataset_run6_Imagestack.zip,13.6,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 6, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run6/ABL_Dataset_run6_SURF.zip,1.8,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 6, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run7/ABL_Dataset_run7_Header.zip,0.000206,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 7, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run7/ABL_Dataset_run7_Imagestack.zip,13.9,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 7, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run7/ABL_Dataset_run7_SURF.zip,1.6,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 7, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run8/ABL_Dataset_run8_Header.zip,0.000162,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 8, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run8/ABL_Dataset_run8_Imagestack.zip,12.4,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 8, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run8/ABL_Dataset_run8_SURF.zip,1.6,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 8, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run9/ABL_Dataset_run9_Header.zip,0.000114,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 9, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run9/ABL_Dataset_run9_Imagestack.zip,9.4,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 9, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run9/ABL_Dataset_run9_SURF.zip,1.2,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 9, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run10/ABL_Dataset_run10_Header.zip,0.000175,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 10, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run10/ABL_Dataset_run10_Imagestack.zip,11.2,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 10, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run10/ABL_Dataset_run10_SURF.zip,1.5,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 10, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run11/ABL_Dataset_run11_Header.zip,0.000129,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 11, header; General dataset information, GPS and alignment matrices.",Electronics,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run11/ABL_Dataset_run11_Imagestack.zip,8.7,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 11, imagestacks; Full set of lidar scans, containing intensity, azimuth, elevation, range and timing data.",,"SLAM, odometry",image,P5024,Robotics,,,,
Gravel Pit Lidar-Intensity Imagery Dataset,ftp://asrl3.utias.utoronto.ca/abl-sudbury/run11/ABL_Dataset_run11_SURF.zip,1.1,v1,.zip,Free; cite where possible,,,,"The Gravel Pit Lidar Intensity Imagery Dataset is a collection of 77,754 high-framerate laser range and intensity images gathered at a suitable planetary analogue environment in Sudbury, Ontario, Canada. The data were collected during a visual teach and repeat experiment in which a 1.1km route was taught and then autonomously re-traversed (i.e., the robot drove in its own tracks) every 2-3 hours for 25 hours. The dataset is subdivided into the individual 1.1km traversals of the same route, at varying times of day (ranging from full sunlight to full darkness). This data should be of interest to researchers who develop algorithms for visual odometry, simultaneous localization and mapping (SLAM) or place recognition in three-dimensional, unstructured and natural environments. In concert with state-of-the-art techniques, this dataset creates ample opportunity for loop closure; in addition to having multiple traversals of the same path, the trajectory was specifically chosen to include both small- and large-scale loops. The lidar scans were taken with a $480 \times 360$ resolution at 2Hz, while driving roughly 0.3-0.4 meters per second; therefore, one of the challenges in using this dataset is to compensate for the motion distortion in the data. All of the data are presented in either human-readable text files or images, and are accompanied by Matlab parsing scripts for ease of use.","run 11, SURF and Matches; SURF features and frame-to-frame matches extracted from the Imagestacks.",,"SLAM, odometry",image,P5024,Robotics,,,,
Canadian Planetary Emulation Terrain 3D Mapping Dataset,ftp://asrl3.utias.utoronto.ca/3dmap_dataset/a100_dome/a100_dome.zip,0.928,v1,.zip,N/A,,,,"This dataset consists of 95 laser scans obtained using a Clearpath Husky A100 at the University of Toronto Institute for Aerospace Studies (UTIAS) indoor rover test facility, located in Toronto, Ontario, Canada. This dataset was collected January 2011. The UTIAS indoor rover test facility consists of a large dome structure, which covers a circular workspace area 40m in diameter. In this workspace, gravel was distributed to emulate scaled planetary hills and ridges, providing characteristic natural, unstructured terrain. Four large retroreflective sheets were placed outside the workspace to serve as easily-identifiable landmarks for ground truth localization.",,Electronics,"6D ground truth, laser scans, rover",image,P5025,Robotics,,,,
Canadian Planetary Emulation Terrain 3D Mapping Dataset,ftp://asrl3.utias.utoronto.ca/3dmap_dataset/a100_dome_vo/a100_dome_vo.zip,0.433,v1,.zip,N/A,,,,"This dataset consists of 50 laser scans obtained using a Clearpath Husky A100 at the University of Toronto Institute for Aerospace Studies (UTIAS) indoor rover test facility, located in Toronto, Ontario, Canada. In addition to the laser scans, this dataset also contains pose-to-pose transformation estimates between consecutive scan stops based on stereo visual odometry. This dataset was collected November 2010. The UTIAS indoor rover test facility consists of a large dome structure, which covers a circular workspace area 40m in diameter. In this workspace, gravel was distributed to emulate scaled planetary hills and ridges, providing characteristic natural, unstructured terrain. Four large retroreflective sheets were placed outside the workspace to serve as easily-identifiable landmarks for ground truth localization.",,Electronics,"6D ground truth, laser scans, rover",image,P5026,Robotics,,,,
Canadian Planetary Emulation Terrain 3D Mapping Dataset,ftp://asrl3.utias.utoronto.ca/3dmap_dataset/p2at_met/p2at_met.zip,0.413,v1,.zip,N/A,,,,"This dataset consists of 102 laser scans obtained using a modified MobileRobots Pioneer P2AT at the Canadian Space Agency's (CSA) Mars Emulation Terrain (MET), located near Montreal, Quebec, Canada. This dataset was collected for mapping purposes during October 2010. The CSA MET is an outdoor test facility with workspace dimensions of 120m x 60m. The terrain consists of scattered rocks on sand, along with some large ridge, crater, and outcrop features.",,Electronics,"Ground truth, mars, lasers",image,P5027,Robotics,,,,
Canadian Planetary Emulation Terrain 3D Mapping Dataset,ftp://asrl3.utias.utoronto.ca/3dmap_dataset/a200_met/a200_met.zip,0.208,v1,.zip,N/A,,,,"This dataset consists of 25 laser scans obtained using a Clearpath Husky A200 at the Canadian Space Agency's (CSA) Mars Emulation Terrain (MET), located near Montreal, Quebec, Canada. In addition to the laser scans, this dataset also contains pose-to-pose transformation estimates between consecutive scan stops based on stereo visual odometry. This dataset was collected during a field demonstration conducted July 2011. The CSA MET is an outdoor test facility with workspace dimensions of 120m x 60m. The terrain consists of scattered rocks on sand, along with some large ridge, crater, and outcrop features.",,Electronics,"Ground truth, mars, lasers, odometry",image,P5028,Robotics,,,,
Canadian Planetary Emulation Terrain 3D Mapping Dataset,ftp://asrl3.utias.utoronto.ca/3dmap_dataset/box_met/box_met.zip,2.1,v1,.zip,N/A,,,,"This dataset consists of 112 laser scans obtained using a custom-built data collection platform at the Canadian Space Agency's (CSA) Mars Emulation Terrain (MET), located near Montreal, Quebec, Canada. This dataset was collected for mapping purposes during November 2011. The CSA MET is an outdoor test facility with workspace dimensions of 120m x 60m. The terrain consists of scattered rocks on sand, along with some large ridge, crater, and outcrop features.",,Electronics,"Ground truth, mars, lasers, mapping",image,P5029,Robotics,,,,
Devon Island Rover Navigation Dataset,ftp://asrl3.utias.utoronto.ca/Devon-Island-Rover-Navigation/rover-traverse/rover-traverse-logs.zip,0.0029,v1,.zip,N/A,,,,"Stereo imagery, sun vectors, and inclinometer data were collected over a ten-kilometer traverse starting and ending at the Haughton-Mars Research Station on Devon Island. The data collection platform was a pushcart outfitted with typical rover engineering sensors---a Point Grey Research Bumblebee XB3 stereo camera, a Honeywell HMR-3000 inclinometer, and a Sinclair Interplanetary SS-411 digital sun sensor. For ground-truth position, a pair of Magellan ProMark3 GPS units were used to produce post-processed differential GPS data for the whole traverse. The data is partitioned into 23 sections. At the start of each section, a batch of sun vectors and inclinometer readings, together with global position from GPS, were used to estimate the rotation between the sun sensor frame and the topocentric frame. This estimate provides platform orientation at the start of each subsequence, allowing motion estimates to be easily compared to ground-truth data provided by the GPS.",Size is for ZIP log file zip,Electronics,"Mars, Rover",image,P5030,Robotics,,,,
Devon Island Rover Navigation Dataset,ftp://asrl3.utias.utoronto.ca/Devon-Island-Rover-Navigation/long-range-localization/Lidar-Scans.zip,4.7,v1,.zip,N/A,,,,"The long-range localization dataset consists of 3D lidar scans, 3D orbital maps, and ground-truth rover positions acquired from field tests at a Mars/Moon analogue site on Devon Island, Nunavut (75°22'N and 89°41'W).",Lidar Scans,Electronics,"Mars, lidar scans, orbiatl maps, ground truth, 3D",image,P5031,Robotics,,,,
Devon Island Rover Navigation Dataset,ftp://asrl3.utias.utoronto.ca/Devon-Island-Rover-Navigation/long-range-localization/Orbital-Maps.zip,4.7,v1,.zip,N/A,,,,"The long-range localization dataset consists of 3D lidar scans, 3D orbital maps, and ground-truth rover positions acquired from field tests at a Mars/Moon analogue site on Devon Island, Nunavut (75°22'N and 89°41'W).",Orbital Maps,Electronics,"Mars, lidar scans, orbiatl maps, ground truth, 3D",image,P5031,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM1.zip,0.0058,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM1,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM2.zip,0.0073,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM2,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM3.zip,0.0077,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM3,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM4.zip,0.006,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM4,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM5.zip,0.0098,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM5,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM6.zip,0.0038,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM6,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM7.zip,0.0165,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM7,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM8.zip,0.0108,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM8,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",Text,P5032,Robotics,,,,
UTIAS Multi-Robot Cooperative Localization and Mapping Dataset,ftp://asrl3.utias.utoronto.ca/MRCLAM/MRCLAM9.zip,22,v1,.zip,N/A,,,,"This 2d indoor dataset collection consists of 9 individual datasets. Each dataset contains odometry and (range and bearing) measurement data from 5 robots, as well as accurate groundtruth data for all robot poses and (15) landmark positions. The dataset is intended for studying the problems of cooperative localization (with only a team robots), cooperative localization with a known map, and cooperative simultaneous localization and mapping (SLAM).",MRCLAM9,Electronics,"Cooperative localization, SLAM, odometry, occlusions, groundtruth",image,P5032,Robotics,,,,
Bike Video Dataset,https://storage.googleapis.com/brain-robotics-data/bike/BikeVideoDataset.tar,32,v1,tar,Creative Commons Attribution License,,,,"The Bike Video Dataset is created by recording videos using a hand-held phone camera while riding a bicycle. This particular camera offers no stabilization. The image frame rate is 30fps, with a resolution of 720×1280.",Google Brain Robotics Data,Electronics,"Google, video, brain robotics",Video,P5033,Robotics,,,,
Grasping Dataset,https://sites.google.com/site/brainrobotdata/home/grasping-dataset/download_listing.sh?revision=2,886,v2,.sh,Creative Commons Attribution License,,,,"This data set contains roughly 650,000 examples of robot grasping attempts. The examples are grouped into batches that have identical feature keys. The set of features present in each batch is described in a CSV file.",Google Brain Robotics Data,Electronics,"Google, grasping, hand-eye coordination, deep learning",script,P5034,Robotics,,,,
Multiview Pouring Dataset,https://sites.google.com/site/brainrobotdata/home/multiview-pouring/download.sh?attredirects=0&d=1,9.5,v2,.sh,Creative Commons Attribution License,,,,Multiview pouring dataset—pouring liquid into a cup from 1st-person viewpoint and 3rd-persion viewpoint fo rthe paper Time-Contrastive Networks: Self-Supervised Learning from Video,Google Brain Robotics Data,Electronics,"Google, pouring, self-supervised learning",script,P5035,Robotics,,,,
Pouring Dataset,https://sites.google.com/site/brainrobotdata/home/pouring-dataset/download.sh?attredirects=0&d=1,7,v1,.sh,Creative Commons Attribution License,,,,"Pouring dataset created for the publication ""Unsupervised Perceptual Rewards for Imitation Learning""",Google Brain Robotics Data,Electronics,"Google, pouring, imitation learning, perception",script,P5036,Robotics,,,,
Push Dataset,https://sites.google.com/site/brainrobotdata/home/push-dataset/download_listing.sh?attredirects=0&d=1,137.5,v3,.sh,Creative Commons Attribution License,,,,"This data set contains roughly 59,000 examples of robot pushing motions, including one training set (train) and two test sets of previously seen (testseen) and unseen (testnovel) objects.",Google Brain Robotics Data,Electronics,"Google, unsupervised learning, Tensorflow RecordWriter",script,P5037,Robotics,,,,
Apartment — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/apartment_03-Dec-2011-18_13_33/csv_local/local_frame.tar.gz,0.362,v1,.tar.gz,proprietary,,,,"This dataset was recorded with the intention to test registration algorithm robustness against outliers created by dynamic elements. The dynamic elements were moved in between scans (as opposed to during a scan). The complexity of the environment is relatively low given the high level of structure (walls, celling and floor). The opposite of a high structure level would be fond in a low-constrained environment, such as in a mountain plain. Multiple reflective surfaces can be found in this data set. Most of them are highlighted in the section Environment Topology.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Gravity","image, text",P5038,Robotics,,,,
Apartment — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/apartment_03-Dec-2011-18_13_33/csv_global/global_frame.tar.gz,0.291,v1,.tar.gz,proprietary,,,,"This dataset was recorded with the intention to test registration algorithm robustness against outliers created by dynamic elements. The dynamic elements were moved in between scans (as opposed to during a scan). The complexity of the environment is relatively low given the high level of structure (walls, celling and floor). The opposite of a high structure level would be fond in a low-constrained environment, such as in a mountain plain. Multiple reflective surfaces can be found in this data set. Most of them are highlighted in the section Environment Topology.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Gravity","image, text",P5038,Robotics,,,,
Apartment — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/apartment_03-Dec-2011-18_13_33/vtk/vtk_global.tar.gz,0.27,v1,.tar.gz,proprietary,,,,"This dataset was recorded with the intention to test registration algorithm robustness against outliers created by dynamic elements. The dynamic elements were moved in between scans (as opposed to during a scan). The complexity of the environment is relatively low given the high level of structure (walls, celling and floor). The opposite of a high structure level would be fond in a low-constrained environment, such as in a mountain plain. Multiple reflective surfaces can be found in this data set. Most of them are highlighted in the section Environment Topology.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Gravity","image, text",P5038,Robotics,,,,
Apartment — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/apartment_03-Dec-2011-18_13_33/rosbags/rosbags.tar.gz,0.0395,v1,.tar.gz,proprietary,,,,"This dataset was recorded with the intention to test registration algorithm robustness against outliers created by dynamic elements. The dynamic elements were moved in between scans (as opposed to during a scan). The complexity of the environment is relatively low given the high level of structure (walls, celling and floor). The opposite of a high structure level would be fond in a low-constrained environment, such as in a mountain plain. Multiple reflective surfaces can be found in this data set. Most of them are highlighted in the section Environment Topology.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Gravity","image, text",P5038,Robotics,,,,
ETH Hauptgebaude — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/ETH_hauptgebaude_23-Aug-2011-18_43_49/csv_local/local_frame.tar.gz,0.151,v1,.tar.gz,proprietary,,,,"This dataset was recorded in the main building (Hauptgebaude) of ETH Zurich. The scanner moved mainly along a straight line in a long hallway. This hallway is located at the second floor and surrounds the central place, which is open over several floors. The hallway has a curved ceiling and a wall on one side. On the other side, there are pillars, aches and ramps. Those elements are the main interest of this dataset. The dataset is considered mainly static but at few occasion people were walking in the hallway while recording (seeContextual Photographs). The intention behind the recording is to evaluate robustness of local registration to repetitive elements. It could also be used for environment representation since same objects reappear often in the scene.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5039,Robotics,,,,
ETH Hauptgebaude — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/ETH_hauptgebaude_23-Aug-2011-18_43_49/csv_global/global_frame.tar.gz,0.122,v1,.tar.gz,proprietary,,,,"This dataset was recorded in the main building (Hauptgebaude) of ETH Zurich. The scanner moved mainly along a straight line in a long hallway. This hallway is located at the second floor and surrounds the central place, which is open over several floors. The hallway has a curved ceiling and a wall on one side. On the other side, there are pillars, aches and ramps. Those elements are the main interest of this dataset. The dataset is considered mainly static but at few occasion people were walking in the hallway while recording (seeContextual Photographs). The intention behind the recording is to evaluate robustness of local registration to repetitive elements. It could also be used for environment representation since same objects reappear often in the scene.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5039,Robotics,,,,
ETH Hauptgebaude — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/ETH_hauptgebaude_23-Aug-2011-18_43_49/vtk/vtk_global.tar.gz,0.121,v1,.tar.gz,proprietary,,,,"This dataset was recorded in the main building (Hauptgebaude) of ETH Zurich. The scanner moved mainly along a straight line in a long hallway. This hallway is located at the second floor and surrounds the central place, which is open over several floors. The hallway has a curved ceiling and a wall on one side. On the other side, there are pillars, aches and ramps. Those elements are the main interest of this dataset. The dataset is considered mainly static but at few occasion people were walking in the hallway while recording (seeContextual Photographs). The intention behind the recording is to evaluate robustness of local registration to repetitive elements. It could also be used for environment representation since same objects reappear often in the scene.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5039,Robotics,,,,
ETH Hauptgebaude — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/ETH_hauptgebaude_23-Aug-2011-18_43_49/rosbags/rosbags.tar.gz,0.0188,v1,.tar.gz,proprietary,,,,"This dataset was recorded in the main building (Hauptgebaude) of ETH Zurich. The scanner moved mainly along a straight line in a long hallway. This hallway is located at the second floor and surrounds the central place, which is open over several floors. The hallway has a curved ceiling and a wall on one side. On the other side, there are pillars, aches and ramps. Those elements are the main interest of this dataset. The dataset is considered mainly static but at few occasion people were walking in the hallway while recording (seeContextual Photographs). The intention behind the recording is to evaluate robustness of local registration to repetitive elements. It could also be used for environment representation since same objects reappear often in the scene.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5039,Robotics,,,,
Mountain Plain — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/plain_01-Sep-2011-16_39_18/csv_local/local_frame.tar.gz,0.068,v1,.tar.gz,proprietary,,,,"This dataset was recorded at 1920 m above the sea level nearby a small lake, Lake Cadagno, which is located in a small concave basin in an alpine plain. Recordings were made while the scanner was moving down a small slope towards the lake. The majority of the outer part of the basin is 3-4 meter higher than the middle section. There is no major vertical structure in the environment and the main elements that can be found on the ground are dry vegetation (around 50 cm height) (see Contextual Photographs). The motivation for this dataset is to evaluate robustness of registration algorithms against low-constrained environment. The opposite of a low-constrained environment would be an apartment where ceilings and walls are large enough to fix the position and orientation easily. It is also a good dataset showing a case where the hypothesis of a planar motion of the scanner does not hold.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5040,Robotics,,,,
Mountain Plain — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/plain_01-Sep-2011-16_39_18/csv_global/global_frame.tar.gz,0.055,v1,.tar.gz,proprietary,,,,"This dataset was recorded at 1920 m above the sea level nearby a small lake, Lake Cadagno, which is located in a small concave basin in an alpine plain. Recordings were made while the scanner was moving down a small slope towards the lake. The majority of the outer part of the basin is 3-4 meter higher than the middle section. There is no major vertical structure in the environment and the main elements that can be found on the ground are dry vegetation (around 50 cm height) (see Contextual Photographs). The motivation for this dataset is to evaluate robustness of registration algorithms against low-constrained environment. The opposite of a low-constrained environment would be an apartment where ceilings and walls are large enough to fix the position and orientation easily. It is also a good dataset showing a case where the hypothesis of a planar motion of the scanner does not hold.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5040,Robotics,,,,
Mountain Plain — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/plain_01-Sep-2011-16_39_18/vtk/vtk_global.tar.gz,0.056,v1,.tar.gz,proprietary,,,,"This dataset was recorded at 1920 m above the sea level nearby a small lake, Lake Cadagno, which is located in a small concave basin in an alpine plain. Recordings were made while the scanner was moving down a small slope towards the lake. The majority of the outer part of the basin is 3-4 meter higher than the middle section. There is no major vertical structure in the environment and the main elements that can be found on the ground are dry vegetation (around 50 cm height) (see Contextual Photographs). The motivation for this dataset is to evaluate robustness of registration algorithms against low-constrained environment. The opposite of a low-constrained environment would be an apartment where ceilings and walls are large enough to fix the position and orientation easily. It is also a good dataset showing a case where the hypothesis of a planar motion of the scanner does not hold.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5040,Robotics,,,,
Mountain Plain — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/plain_01-Sep-2011-16_39_18/rosbags/rosbags.tar.gz,0.01,v1,.tar.gz,proprietary,,,,"This dataset was recorded at 1920 m above the sea level nearby a small lake, Lake Cadagno, which is located in a small concave basin in an alpine plain. Recordings were made while the scanner was moving down a small slope towards the lake. The majority of the outer part of the basin is 3-4 meter higher than the middle section. There is no major vertical structure in the environment and the main elements that can be found on the ground are dry vegetation (around 50 cm height) (see Contextual Photographs). The motivation for this dataset is to evaluate robustness of registration algorithms against low-constrained environment. The opposite of a low-constrained environment would be an apartment where ceilings and walls are large enough to fix the position and orientation easily. It is also a good dataset showing a case where the hypothesis of a planar motion of the scanner does not hold.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, repetitive elements, straight line, 2D plane","image, text",P5040,Robotics,,,,
Stairs — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/stairs_26-Aug-2011-14_26_14/csv_local/local_frame.tar.gz,0.124,v1,.tar.gz,proprietary,,,,"The recording position starts in an elongated corridor expending on each side of the scanner. The scanner passed through a doorway and crossed a small staircase, composed of 5 steep stairs that lead towards another doorway. This second doorway leads outside the building where bicycles and motorbikes are parked (seeContextual Photographs). The motivation behind this dataset is to test the robustness of registration algorithms against rapid variations in scanned volumes. It is also a good dataset showing that the hypothesis of a planar motion of the scanner does not hold.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Magnetic North, Gravity","image, text",P5041,Robotics,,,,
Stairs — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/stairs_26-Aug-2011-14_26_14/csv_global/global_frame.tar.gz,0.1,v1,.tar.gz,proprietary,,,,"The recording position starts in an elongated corridor expending on each side of the scanner. The scanner passed through a doorway and crossed a small staircase, composed of 5 steep stairs that lead towards another doorway. This second doorway leads outside the building where bicycles and motorbikes are parked (seeContextual Photographs). The motivation behind this dataset is to test the robustness of registration algorithms against rapid variations in scanned volumes. It is also a good dataset showing that the hypothesis of a planar motion of the scanner does not hold.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Magnetic North, Gravity","image, text",P5041,Robotics,,,,
Stairs — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/stairs_26-Aug-2011-14_26_14/vtk/vtk_global.tar.gz,0.096,v1,.tar.gz,proprietary,,,,"The recording position starts in an elongated corridor expending on each side of the scanner. The scanner passed through a doorway and crossed a small staircase, composed of 5 steep stairs that lead towards another doorway. This second doorway leads outside the building where bicycles and motorbikes are parked (seeContextual Photographs). The motivation behind this dataset is to test the robustness of registration algorithms against rapid variations in scanned volumes. It is also a good dataset showing that the hypothesis of a planar motion of the scanner does not hold.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Magnetic North, Gravity","image, text",P5041,Robotics,,,,
Stairs — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/stairs_26-Aug-2011-14_26_14/rosbags/rosbags.tar.gz,0.0148,v1,.tar.gz,proprietary,,,,"The recording position starts in an elongated corridor expending on each side of the scanner. The scanner passed through a doorway and crossed a small staircase, composed of 5 steep stairs that lead towards another doorway. This second doorway leads outside the building where bicycles and motorbikes are parked (seeContextual Photographs). The motivation behind this dataset is to test the robustness of registration algorithms against rapid variations in scanned volumes. It is also a good dataset showing that the hypothesis of a planar motion of the scanner does not hold.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Point Cloud Views, Scanner Poses, Magnetic North, Gravity","image, text",P5041,Robotics,,,,
Gazebo (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/gazebo_summer_04-Aug-2011-16_13_22/csv_local/local_frame.tar.gz,0.109,v1,.tar.gz,proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo. The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5042,Robotics,,,,
Gazebo (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/gazebo_summer_04-Aug-2011-16_13_22/csv_global/global_frame.tar.gz,0.089,v1,.tar.gz,proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo. The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5042,Robotics,,,,
Gazebo (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/gazebo_summer_04-Aug-2011-16_13_22/vtk/vtk_global.tar.gz,0.094,v1,.tar.gz,proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo. The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5042,Robotics,,,,
Gazebo (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/gazebo_summer_04-Aug-2011-16_13_22/rosbags/rosbags.tar.gz,0.0168,v1,.tar.gz,proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo. The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5042,Robotics,,,,
Gazebo (winter) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/gazebo_winter_18-Jan-2012-16_10_04/csv_local/local_frame.tar.gz,0.095,v1,"csv, tar",proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo (see Contextual Photographs). The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording. The global representation can also be compared with Gazebo in summer to highlight global (seasonal) modification of the environment.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5043,Robotics,,,,
Gazebo (winter) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/gazebo_winter_18-Jan-2012-16_10_04/csv_global/global_frame.tar.gz,0.077,v1,"csv, tar",proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo (see Contextual Photographs). The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording. The global representation can also be compared with Gazebo in summer to highlight global (seasonal) modification of the environment.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5043,Robotics,,,,
Gazebo (winter) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/public_html/gazebo_winter_18-Jan-2012-16_10_04/vtk/vtk_global.tar.gz,0.08,v1,"csv, tar",proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo (see Contextual Photographs). The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording. The global representation can also be compared with Gazebo in summer to highlight global (seasonal) modification of the environment.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5043,Robotics,,,,
Gazebo (winter) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/gazebo_winter_18-Jan-2012-16_10_04/rosbags/rosbags.tar.gz,0.0131,v1,"csv, tar",proprietary,,,,"The environment is located in a park where there is grass, paved small roads and sparse trees. The main construction is a gazebo made of rock walls and a wood ceiling covered with vines trees. People usually walk in the path and take a break under the gazebo (see Contextual Photographs). The main motivation for this dataset was to test registration algorithms against semi-structured environments. Some people were walking in the area while the scanner was recording. The global representation can also be compared with Gazebo in summer to highlight global (seasonal) modification of the environment.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5043,Robotics,,,,
Wood (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_summer_25-Aug-2011-13_00_30/csv_local/local_frame.tar.gz,0.155,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in autumn to highlight the global (seasonal) modification of the environment.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5044,Robotics,,,,
Wood (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_summer_25-Aug-2011-13_00_30/csv_global/global_frame.tar.gz,0.125,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in autumn to highlight the global (seasonal) modification of the environment.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5044,Robotics,,,,
Wood (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_summer_25-Aug-2011-13_00_30/vtk/vtk_global.tar.gz,0.124,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in autumn to highlight the global (seasonal) modification of the environment.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5044,Robotics,,,,
Wood (summer) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_summer_25-Aug-2011-13_00_30/rosbags/rosbags.tar.gz,0.0183,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in autumn to highlight the global (seasonal) modification of the environment.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5044,Robotics,,,,
Wood (autmn) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_autumn_09-Dec-2011-15_44_05/csv_local/local_frame.tar.gz,0.129,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in summer to highlight the global (seasonal) modification of the environment. Notes: The sun sets early and the pictures taken are darker at the end.","Point clouds in base frame: Point clouds of this section have their origin at the scanner center. 2D scans have been transformed using the axis encoder to produce consistent 3D point clouds. The supporting data (Gravity, Magnetic North and GPS) have been recorded while the scanner was rotating. If you do not wish to compute the statistics for the supporting data, you can go to the section Point Clouds in Global Coordinates were single measurements have been selected per pose. The “ground truth” poses can also be downloaded in the section Point Clouds in Global Coordinates.; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5045,Robotics,,,,
Wood (autmn) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_autumn_09-Dec-2011-15_44_05/csv_global/global_frame.tar.gz,0.104,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in summer to highlight the global (seasonal) modification of the environment. Notes: The sun sets early and the pictures taken are darker at the end.","Point Clouds in Global Frame: Point clouds of this section have been moved to a global reference frame where the pose of the first 3D scan is the origin. The supporting data (Gravity, Magnetic North and GPS) have been post-processed to have only one reading per 3D scan; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5045,Robotics,,,,
Wood (autmn) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_autumn_09-Dec-2011-15_44_05/vtk/vtk_global.tar.gz,0.105,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in summer to highlight the global (seasonal) modification of the environment. Notes: The sun sets early and the pictures taken are darker at the end.","Visualization Files: File type chosen for visualization is VTK legacy in ASCII format. We suggest to use Paraview to view the files because it can be freely downloaded for Windows, Mac and Ubuntu user and it's supported by Kitware. All screenshots of this page were realized using this software. You can download it here: http://www.paraview.org/; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5045,Robotics,,,,
Wood (autmn) — ASL Datasets,http://robotics.ethz.ch/~asl-datasets/wood_autumn_09-Dec-2011-15_44_05/rosbags/rosbags.tar.gz,0.0155,v1,"csv, tar",proprietary,,,,"This environment is mainly constituted of vegetation (tree, bushes, etc). The only structured element is a small paved road that crosses the wood. The scanner path starts in the wood and continues for approximately 12 scans before joining the small road (see Contextual Photographs). The main motivation for this dataset is to evaluate robustness of registration algorithm to unstructured environment. Some people were walking on the road while recording. The complete representation can also be compared with Wood in summer to highlight the global (seasonal) modification of the environment. Notes: The sun sets early and the pictures taken are darker at the end.","Raw Data: this type of data can be useful if you want to do some preprocessing on the 2D scans directly. We used ROS (www.ros.org) as middleware so the raw recordings can be downloaded and playback using rosbag. One rosbag has been recored per 3D scan. The ground truth poses are not available in the rosbags. For more information on how to use this format see: www.ros.org/wiki/rosbag; Citation: F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, Challenging data sets for point cloud registration algorithms, International Journal of Robotic Research, vol. 31, no. 14, pp. 1705–1711, Dec. 2012",Electronics,"Environment topography, 2D plane, point cloud, gravity","image, text",P5045,Robotics,,,,
3D Deformable Partial Shape Matching,https://vision.in.tum.de/_media/data/datasets/partial_off.zip,0.3,v1,OFF.zip,proprietary,,,,"The datasets we provide here can be used for tasks of deformable 3D shape matching and retrieval under partiality transformations. This is considered a more challenging setting if compared to the more classical tasks dealing with full shapes. There are two datasets, named cuts (456 partial shapes) and holes (684 partial shapes), exemplifying different kinds of partiality. The shapes span different classes and are based on the TOSCA high-resolution dataset. The cuts dataset consists of shapes undergoing a single cut; an example is given by the human on the left, above. The holes dataset is more challenging, as it contains irregular holes and multiple cuts; see the middle and right examples above. In addition to the partial shapes we provide null shapes for each class, i.e. full shapes in a canonical pose. These can be used to perform part-to-whole matching, as we did in the paper below. The null shapes are the same as the originals from TOSCA, remeshed to 10K vertices for additional challenge. All partial shapes are also remeshed.",,Electronics,"deformable, shapes, 3D, cuts, holes, TOSCA",image,P5046,Robotics,,,,
3D Object in Clutter Recognition and Segmentation,https://vision.in.tum.de/_media/data/datasets/clutter/clutter-scenes.zip,0.149,v1,.zip,proprietary,,,,"This dataset focuses on the recognition of known objects in cluttered and incomplete 3D scans. It is composed of 150 synthetic scenes, captured with a (perspective) virtual camera, and each scene contains 3 to 5 objects. The model set is composed of 20 different objects, taken from different sources and then processed in order to obtain comparably smooth surfaces of almost uniform 100-350k triangles with an average resolution of 1.0. The dataset features some original shapes (faces) specifically designed for the task by Matteo Sala.","Scenes and segmentation; Cite: A Scale Independent Selection Process for 3D Object Recognition in Cluttered Scenes E. Rodola, A. Albarelli, F. Bergamasco, and A. Torsello International Journal of Computer Vision (IJCV), vol. 102, 2013",Electronics,"synthetic scenes, shapes, 3D object",image,P5124,Robotics,,,,
3D Object in Clutter Recognition and Segmentation,https://vision.in.tum.de/_media/data/datasets/clutter/clutter-models.zip,0.0463,v1,.zip,proprietary,,,,"This dataset focuses on the recognition of known objects in cluttered and incomplete 3D scans. It is composed of 150 synthetic scenes, captured with a (perspective) virtual camera, and each scene contains 3 to 5 objects. The model set is composed of 20 different objects, taken from different sources and then processed in order to obtain comparably smooth surfaces of almost uniform 100-350k triangles with an average resolution of 1.0. The dataset features some original shapes (faces) specifically designed for the task by Matteo Sala.","Complete models; Cite: A Scale Independent Selection Process for 3D Object Recognition in Cluttered Scenes E. Rodola, A. Albarelli, F. Bergamasco, and A. Torsello International Journal of Computer Vision (IJCV), vol. 102, 2013",Electronics,"synthetic scenes, shapes, 3D object",image,P5124,Robotics,,,,
3D Object in Clutter Recognition and Segmentation,https://vision.in.tum.de/_media/data/datasets/clutter/clutter-gt.zip,0.064,v1,.zip,proprietary,,,,"This dataset focuses on the recognition of known objects in cluttered and incomplete 3D scans. It is composed of 150 synthetic scenes, captured with a (perspective) virtual camera, and each scene contains 3 to 5 objects. The model set is composed of 20 different objects, taken from different sources and then processed in order to obtain comparably smooth surfaces of almost uniform 100-350k triangles with an average resolution of 1.0. The dataset features some original shapes (faces) specifically designed for the task by Matteo Sala.","Ground-truth rigid motions; Cite: A Scale Independent Selection Process for 3D Object Recognition in Cluttered Scenes E. Rodola, A. Albarelli, F. Bergamasco, and A. Torsello International Journal of Computer Vision (IJCV), vol. 102, 2013",Electronics,"synthetic scenes, shapes, 3D object",image,P5124,Robotics,,,,
Deformable 3D Shape Matching with Topological Noise,https://vision.in.tum.de/_media/data/datasets/topkids/topkids.zip,0.0492,v1,.zip,proprietary,,,,"This dataset consists of a collection of 3D shapes undergoing within-class deformations that include in topological changes. The changes simulate coalescence of spatially close surface regions – a scenario that frequently occurs when dealing with real data under suboptimal acquisition conditions. The dataset is based on the fat kid from the KIDS dataset with additional poses. All shapes are given in OFF format and exist in two different resolutions (~10k and ~60-80k). Ground-truth matches to the null shape are given for all shapes, but due to the changes in topology not all vertices have a ground-truth match. A map indicating which vertices do not have a match is provided. We also provide a ground-truth symmetry correspondence for all shapes.","Cite: SHREC’16: Matching of Deformable Shapes with Topological Noise (Z. Lähner, E. Rodola, M. M. Bronstein, D. Cremers, O. Burghard, L. Cosmo, A. Dieckmann, R. Klein, Y. Sahillioglu), In Proc. of Eurographics Workshop on 3D Object Retrieval (3DOR), 2016. [bib] [pdf]",Electronics,"Ground truth, topological changes, 3D shapes, deformations",image,P5125,Robotics,,,,
Deformable 3D Shape Matching,https://vision.in.tum.de/_media/spezial/bib/kids.zip,0.0331,v1,.zip,proprietary,,,,"This dataset consists of a collection of 3D shapes undergoing nearly-isometric and within-class deformations. In particular, we provide two shape classes (“kid” and “fat kid”) under different poses, where the same poses are applied to both classes. Filenames within each class are ordered by deviation from isometry, i.e. the last shape has the largest deformation with respect to the null pose. All shapes in the dataset are given in OFF format, have around 60k vertices and consistent triangulations. Ground-truth: The shapes have compatible vertex ordering and thus the ground-truth correspondence is the identity. We are also including a text file which provides the symmetric ground-truth correspondence, that is, giving for each vertex its symmetric match on the same shape (self-isometry).","Cite: Dense Non-Rigid Shape Correspondence Using Random Forests E. Rodola, S. Rota Bulo, T. Windheuser, M. Vestner, and D. Cremers In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014",Electronics,"Ground truth, symmetric ground-truth, 3D shapes, isometric, deforamtions",image,P5126,Robotics,,,,
Deformable Shape Tracking Datasets,https://vision.in.tum.de/lib/exe/fetch.php?tok=81c1b1&media=http%3A%2F%2Fvision.in.tum.de%2Fold%2Fdata%2Fcvpr08_train.zip,0.000119,v1,.zip,proprietary,,,,"Shape Priors in Variational Image Segmentation: Convexity, Lipschitz Continuity and Globally Optimal Solutions","Training data; Cite: Shape Priors in Variational Image Segmentation: Convexity, Lipschitz Continuity and Globally Optimal Solutions (D. Cremers, F. R. Schmidt, F. Barthel), In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008. [bib] [pdf]",Electronics,Lightfield-depth benchmark,image,P5127,Robotics,,,,
Deformable Shape Tracking Datasets,https://vision.in.tum.de/lib/exe/fetch.php?tok=478d70&media=http%3A%2F%2Fvision.in.tum.de%2Fold%2Fdata%2Fcvpr08_test.zip,0.026,v1,.zip,proprietary,,,,"Shape Priors in Variational Image Segmentation: Convexity, Lipschitz Continuity and Globally Optimal Solutions","Testing data; Cite: Shape Priors in Variational Image Segmentation: Convexity, Lipschitz Continuity and Globally Optimal Solutions (D. Cremers, F. R. Schmidt, F. Barthel), In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008. [bib] [pdf]",Electronics,Lightfield-depth benchmark,image,P5127,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/lightfield.tar.gz,24.5,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,Lightfield,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/lightfield-mat.tar.gz,23.9,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,Lightfield-mat,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/depthregistered.tar.gz,57.9,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,DepthRegistered,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/rawimage.tar.gz,29.7,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,RawImage,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/lytrocalibpattern.tar.gz,2.3,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,LF CalibPattern,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/B5143904760.tar.gz,1.5,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,LF WhiteImages,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/ddff-dataset-trainval.h5,12.6,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,DDFF Trainval,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
DDFF 12-Scene,https://vision.in.tum.de/webarchive/hazirbas/ddff12scene/ddff-dataset-test.h5,0.761,v1,.tar.gz,Creative Commons Attribution License,,,,DDFF 12-Scene dataset consists of 720 lightfield images and coregistered depth maps.,DDFF Test,Electronics,Lightfield-depth benchmark,image,P5128,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/lion-rgbd.zip,1,v1,.zip,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Lion, rgbd; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/lion-fusion.zip,0.014,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Lion, fusion; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/lion-intrinsic3d.zip,0.0542,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Lion, intrinsic3D; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/gate-rgbd.zip,2,v1,.zip,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Gate, rgbd; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/gate-fusion.zip,0.0135,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Gate, fusion; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/gate-intrinsic3d.zip,0.044,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Gate, instrinsic 3D; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/hieroglyphics-rgbd.zip,1.5,v1,.zip,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Hieroglyphics, rgbd; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/hieroglyphics-fusion.zip,0.0065,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Hieroglyphics, fusion; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/hieroglyphics-intrinsic3d.zip,0.0247,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Hieroglyphics, instrincs 3D; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/tomb-statuary-rgbd.zip,0.808,v1,.zip,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Tomb statuary, rgbd; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/tomb-statuary-fusion.zip,0.0013,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Tomb statuary, fusion; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/tomb-statuary-intrinsic3d.zip,0.0199,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Tomb statuary, intrinsic 3D; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/bricks-rgbd.zip,1.6,v1,.zip,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Bricks, rgbd; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/bricks-fusion.zip,0.014,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Bricks, fusion; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Intrinsic3D Dataset,https://vision.in.tum.de/_media/data/datasets/intrinsic3d/bricks-intrinsic3d.zip,0.0532,v1,.ply,proprietary,,,,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.","Bricks, intrinsic 3D; Cite: Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting",Electronics,"RGB-D, 3D Model, Shape-from-Shading",image,P5133,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_01.zip,1.3,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_01, Frames: 4757, Duration: 95.13s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_02.zip,0.88,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_02, Frames: 3500, Duration: 69.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_03.zip,1.51,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_03, Frames: 5427, Duration: 108.55s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_04.zip,1.83,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_04, Frames: 6921, Duration: 138.47s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_05.zip,2.01,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_05, Frames: 6300, Duration: 125.97s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_06.zip,1.21,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_06, Frames: 4500, Duration: 90.04s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_07.zip,0.94,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_07, Frames: 3556, Duration: 71.10s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_08.zip,1.01,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_08, Frames: 4300, Duration: 86.08s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_09.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_09, Frames: 2300, Duration: 46.00s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_10.zip,0.36,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_10, Frames: 2100, Duration: 41.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_11.zip,0.25,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_11, Frames: 1500, Duration: 59.95s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_12.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_12, Frames: 2250, Duration: 89.99s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_13.zip,0.28,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_13, Frames: 1800, Duration: 71.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_14.zip,0.23,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_14, Frames: 1550, Duration: 61.93s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_15.zip,0.5,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_15, Frames: 2700, Duration: 107.91s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_16.zip,0.3,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_16, Frames: 1850, Duration: 73.93s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_17.zip,0.99,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_17, Frames: 4980, Duration: 124.39s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_18.zip,1.12,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_18, Frames: 6200, Duration: 154.94s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_19.zip,1.65,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_19, Frames: 8380, Duration: 209.43s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_20.zip,1.1,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_20, Frames: 5380, Duration: 134.45s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_21.zip,1.47,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_21, Frames: 5470, Duration: 273.68s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_22.zip,1.79,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_22, Frames: 6340, Duration: 316.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_23.zip,0.89,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_23, Frames: 3740, Duration: 124.64s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_24.zip,0.77,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_24, Frames: 3500, Duration: 116.64s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_25.zip,0.96,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_25, Frames: 4090, Duration: 136.31s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_26.zip,0.36,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_26, Frames: 2760, Duration: 91.95s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_27.zip,0.69,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_27, Frames: 3480, Duration: 115.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_28.zip,0.7,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_28, Frames: 5550, Duration: 185.31s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_29.zip,2.3,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_29, Frames: 8400, Duration: 280.03s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_30.zip,0.45,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_30, Frames: 1800, Duration: 85.30s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_31.zip,0.85,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_31, Frames: 3240, Duration: 153.59s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_32.zip,0.72,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_32, Frames: 2700, Duration: 127.99s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_33.zip,0.68,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_33, Frames: 2760, Duration: 91.92s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_34.zip,1.12,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_34, Frames: 4290, Duration: 203.38s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_35.zip,0.36,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_35, Frames: 2550, Duration: 85.06s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_36.zip,0.32,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_36, Frames: 2350, Duration: 78.29s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_37.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_37, Frames: 2970, Duration: 98.96s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_38.zip,0.37,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_38, Frames: 3330, Duration: 133.17s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_39.zip,0.38,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_39, Frames: 3540, Duration: 141.65s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_40.zip,0.44,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_40, Frames: 4350, Duration: 174.42s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_41.zip,0.42,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_41, Frames: 3100, Duration: 123.98s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_42.zip,1.14,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_42, Frames: 4830, Duration: 224.49s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_43.zip,0.66,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_43, Frames: 2160, Duration: 100.28s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_44.zip,0.45,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_44, Frames: 2100, Duration: 97.50s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_45.zip,0.93,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_45, Frames: 3000, Duration: 99.99s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_46.zip,1.02,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_46, Frames: 4110, Duration: 137.07s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_47.zip,0.99,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_47, Frames: 3260, Duration: 129.84s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_48.zip,0.96,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_48, Frames: 3250, Duration: 129.41s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_49.zip,0.82,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_49, Frames: 3255, Duration: 129.48s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
Monocular Visual Odometry Dataset,http://vision.in.tum.de/mono/dataset/sequence_50.zip,1.12,v1,zip,Creative Commons Attribution License,,,,"We present a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: We provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting). Further, we propose a simple approach to non-parametric vignette and camera response calibration, which require minimal set-up and are easy to reproduce","sequence_50, Frames: 4050, Duration: 161.12s",Electronics,"Visual Odometry, SLAM, vidoe sequence",Video,P5134,Robotics,,,,
data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/dem.tif.gz,0.574,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Digital Elevation Map ,Tagged Image File ,P1087,,,,,
data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/ir-raw.tif.gz,0.0017,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Infrared Raw Color,Tagged Image File ,P1087,,,,,
data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/ir-falsecolor.tif.gz,0.0012,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Infrared False Color ,Tagged Image File ,P1087,,,,,
data selfdriving cars,data.selfracingcars.com/thunderhill/kairos/optical.tif.gz,0.211,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Optical ,Tagged Image File ,P1087,,,,,
data selfdriving cars,data.selfracingcars.com/thunderhill/velodyne/velodyne.tar.gz,1.2,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Manuals and Application Notes,Tagged Image File ,P1087,,,,,
data selfdriving cars,https://docs.google.com/document/d/1tVIFs0YJvQS_EABt1OIdBzJTttza4W-oJ5zoVn8PnPc/edit,,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Swift Navigation Log,google docs ,P1087,,,,,
data selfdriving cars,data.selfracingcars.com/thunderhill/fcs_xsens/fcs_xsens_data.tar.gz,0.356,v1,zip,Creative Commons BY 4.0,Public,,,Thunderhill,,Automotive,Fairchild and Xsens Data Set,google docs ,P1087,,,,,
CVRR-HANDS 3D,cvrr.ucsd.edu/LISA/Datasets/hands.zip,2,01/01/2013,zip,N/A,Public,,,"The CVRR-HANDS 3D dataset was designed in order to study natural human activity under difﬁcult settings of cluttered background, volatile illumination, and frequent occlusion. "," usage for educational, research and non-profit purposes, without fee",Automotive,"Hand localization, hand and objects localization, and 19 hand gestures for occupant-vehicle interaction",Image,P1088,,,,,
2010 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/vedg-c5sb/rows.csv?accessType=DOWNLOAD,0.000045,8/21/2011,csv,N/A,public,,,"This dataset includes employee information, salary, and position specifics. ",,Government,"salary, whitehouse",text,p3146,,,,,
VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_dayTrain.zip,11.6,1/1/2015,zip,N/A,public,,,Day Train Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,
The White House - Nominations & Appointments,https://opendata.socrata.com/api/views/n5m4-mism/rows.csv?accessType=DOWNLOAD,0.000259,12/29/2015,csv,N/A,public,,,"This dataset includes information about candidate name, position, agency, nomination date, and confirmation vote. ",,Government,"vote, nomination",text,p3147,,,,,
2011 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/73t8-rw4g/rows.csv?accessType=DOWNLOAD,0.00004,08/05/2012,csv,N/A,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees.",,Government,"salary, whitehouse",text,p3148,,,,,
Milk RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/pkfj-5jsd/rows.csv?accessType=DOWNLOAD,0.000011,10/23/2011,csv,N/A,public,,,"Dataset provided by the Environmental Protection Agency, contains milk testing information about radiation.",,Life Science,,text,p3149,,,,,
VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_nightTrain.zip,0.8,1/1/2015,zip,N/A,public,,,Night Train Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,
VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_dayTest.zip,3.8,1/1/2015,zip,N/A,public,,,Day Test Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,
VIVA traffic light detection benchmark,cvrr.ucsd.edu/vivachallenge/data/Lights_Detection/LISA_TL_nightTest.zip,2.6,1/1/2015,zip,N/A,public,,,Night Test Set," usage for educational, research and non-profit purposes, without fee",Automotive,trafffic lights ,Image,P1089,,,,,
DAVIS Driving Dataset 2017 (DDD17) ,https://link.resilio.com/#f=DDD17-DavisDrivingDataset2017&sz=48E10&t=1&s=N5ZDFRPULYU55ATIMTC2S23J24UFYWMR&i=COA4ESOQXVA2DPQHD2CYMRAL3T2HUZ76D&v=2.5,447,8/6/2018,resilio sync,N/A,public,,,first public end-to-end training dataset of automotive driving using a DAVIS event+frame camera,,Automotive,"steering, throttle, GPS",,P1090,,,,,
Sorted RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/w9fb-tgv6/rows.csv?accessType=DOWNLOAD,0.0001,7/23/2013,csv,N/A,public,,,Dataset provided by the US Environmental Protection Agency. Includes information about a RadNet labratory analysis. ,,Life Science,"radiation, lab test",text,p3150,,,,,
Precipitation RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/e2xy-undq/rows.csv?accessType=DOWNLOAD,0.00003,08/21/2011,csv,N/A,public,,,Dataset provided by the US Environmental Protection Agency. Includes information about a Precipitation RadNet labratory analysis. ,,Life Science,"radiation, lab test",text,p3151,,,,,
2012 Annual Report to Congress on White House Staff,https://opendata.socrata.com/api/views/jv7a-cjdv/rows.csv?accessType=DOWNLOAD,0.00004,6/29/2012,csv,N/A,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees.",,Government,"salary, whitehouse",text,p3152,,,,,
2013 Salaries: Pennsylvania State System of Higher Education,https://opendata.socrata.com/api/views/26jq-uk2i/rows.csv?accessType=DOWNLOAD,0.0008,3/19/2013,csv,N/A,public,,,Pennsylvania State System of Higher Education salaries for 2013.,,Government,"education, salary",text,p3153,education,,,,
Drinking Water RadNet Laboratory Analysis,https://opendata.socrata.com/api/views/4ig7-9eqd/rows.csv?accessType=DOWNLOAD,0.00003,08/21/2011,csv,N/A,public,,,Dataset provided by the US Environmental Protection Agency. Includes information about a drinking water RadNet labratory analysis. ,,Life Science,"water, lab test, radiation",text,p3154,,,,,
2013 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/44xn-rs2p/rows.csv?accessType=DOWNLOAD,0.00005,6/28/2013,csv,N/A,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees. ",,Government,"salary, whitehouse",text,p3155,,,,,
2009 Report to Congress on White House Staff,https://opendata.socrata.com/api/views/pc5g-zfsx/rows.csv?accessType=DOWNLOAD,0.00005,8/21/2011,csv,N/A,public,,,"Since 1995, the White House has been required to deliver a report to Congress listing the title and salary of every White House Office employee. Consistent with President Obama's commitment to transparency, this report is being publicly disclosed on our website as it is transmitted to Congress. In addition, this report also contains the title and salary details of administration officials who work at the Office of Policy Development, including the Domestic Policy Council and the National Economic Council -- along with White House Office employees. ",,Government,"salary, whitehouse",text,p3156,,,,,
Franchise Failureby Brand 2011,https://opendata.socrata.com/api/views/5qh7-7usu/rows.csv?accessType=DOWNLOAD,0.00003,6/15/2012,csv,N/A,public,,,"SBA Portfolio Performance by Franchise Code - Data as of 09/30/2011 Combined 7(a) & 504 loan performance data based on loans approved between 10/01/2001 and 09/30/2011, which were designated with a franchise code* and subsequently disbursed.",,Consumer Products,,text,p3157,,,,,
Unclaimed bank accounts,https://opendata.socrata.com/api/views/n2rk-fwkj/rows.csv?accessType=DOWNLOAD,0.001,6/26/2014,csv,N/A,public,,,A list of all the abandoned bank accounts at branches in the Edmonton area and or registered to addresses in the Edmonton area.,,Banking,,text,p3158,,,,,
Country List ISO 3166 Codes Latitude Longitude,https://opendata.socrata.com/api/views/mnkm-8ram/rows.csv?accessType=DOWNLOAD,0.000008,8/21/2011,csv,N/A,public,,,"Country code, latitude, longitude, and numeric code.",,Government,location,text,p3159,,,,,
Groups That Have Lost Their Tax-Exempt Status,https://opendata.socrata.com/api/views/j5va-k6fu/rows.csv?accessType=DOWNLOAD,0.08,9/10/2011,csv,N/A,public,,,"Use the searchable the database or browse the list of the 275,000 organizations that the IRS says are no longer tax-exempt because they did not file legally required documents. The IRS announced the revocations on June 8.",,Government,tax,text,p3160,,,,,
South Carolina State Employee Salary Database,https://opendata.socrata.com/api/views/67f6-9d58/rows.csv?accessType=DOWNLOAD,0.002,2/6/2014,csv,N/A,public,,,"All employees of the State of South Carolina that make at least $50,000 per year. Imported from the South Carolina Budget and Control Board website.",,Government,salary,text,p3161,,,,,
Airplane Crashes and Fatalities Since 1908,https://opendata.socrata.com/api/views/q2te-8cvq/rows.csv?accessType=DOWNLOAD,0.002,08/21/2011,csv,,,,,"Full history of airplane crashes throughout the world, from 1908-present.",,Travel and transportation,"airplane, fatalities, crash",text,p3162,,,,,
Alcohol Consumption Per Country,https://opendata.socrata.com/api/views/hj43-2bpj/rows.csv?accessType=DOWNLOAD,0.003,08/21/2011,csv,N/A,public,,,The World Health Organization (WHO)'s breakdown of per capita alcohol consumption among adults over 15.,,Government,alcohol,text,p3162,,,,,
Population Growth,https://datahub.io/core/population-growth-estimates-and-projections/r/population-growth-estimates-and-projections_zip.zip,0.028,1/1/2018,zip,N/A,public,,,"Data comes from United Nations’ Population Division datasets. Total population (both sexes combined) by region, subregion and country, annually for 1950-2100 (thousands). Data is cleaned, normalized, “un-pivoted”, and represented in nice machine readable way in CSV format.",,Government,population,text,p3163,,,,,
Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6248/soft/GDS6248_full.soft.gz,0.0149,3/1/2014,zip,N/A,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",full SOFT file,P1091,,,,,
City Population Annual Timeseries (UN Statistics Division),https://datahub.io/core/population-city/r/population-city_zip.zip,0.002,1/1/2018,zip,N/A,public,,,"UNSD Demographic Statistics: City population by sex, city and city type. Data Source: UNData. UNSD Demographic Statistics.",,Government,"population, statistics",text,p3164,,,,,
IMF World Economic Outlook Database,https://datahub.io/core/imf-weo/r/imf-weo_zip.zip,0.005,3/1/2018,zip,N/A,public,,,IMF World Economic Outlook (WEO) database. The [IMF World Economic Outlook][weo] is a twice-yearly survey by IMF staff that presents IMF staff economists' analyses of global economic developments during the near and medium term.,,Government,economics,text,p3165,,,,,
European Union Emissions Trading System,https://datahub.io/core/eu-emissions-trading-system/r/eu-emissions-trading-system_zip.zip,0.0009,3/1/2018,zip,N/A,public,,,Data about the EU emission trading system (ETS). The EU emission trading system (ETS) is one of the main measures introduced by the EU to achieve cost-efficient reductions of greenhouse gas emissions and reach its targets under the Kyoto Protocol and other commitments.,,Government,"environment, emissions",text,p3166,,,,,
Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6248/soft/GDS6248.soft.gz,0.0073,3/1/2014,zip,N/A,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",DataSet full SOFT file,P1091,,,,,
Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39549/soft/GSE39549_family.soft.gz,0.041,3/1/2014,zip,N/A,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",DataSet SOFT file,P1091,,,,,
Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39549/miniml/GSE39549_family.xml.tgz,0.041,3/1/2014,zip,N/A,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",Series family SOFT file,P1091,,,,,
Diet-induced obesity model: liver,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6887/annot/GPL6887.annot.gz,0.007,3/1/2014,zip,N/A,public,,,Analysis of livers of C57BL/6J mice fed a high fat diet for up to 24 weeks. Significant body weight gain was observed after 4 weeks. Results provide insight into the effect of high fat diets on metabolism in the liver.,,Health,"mice, liver, Mus musculus",Series family MINiML file,P1091,,,,,
UN-LOCODE Codelist,https://datahub.io/core/un-locode/r/un-locode_zip.zip,0.006,3/1/2018,zip,N/A,public,,,"The United Nations Code for Trade and Transport Locations is a code list mantained by UNECE, United Nations agency, to facilitate trade. Data Data comes from the UNECE page, released at least once a year.",,Government,,text,p3167,,,,,
Natural Earth Admin1 Polygons as GeoJSON,https://datahub.io/core/geo-ne-admin1/r/geo-ne-admin1_zip.zip,0.009,6/1/2018,zip,N/A,public,,,Geodata data package providing geojson polygons for the largest administrative subdivisions in every countries. ,,Government,"geo, country",text,p3168,,,,,
Gross capital formation (% of GDP),https://datahub.io/world-bank/ne.gdi.totl.zs/r/ne_gdi_totl_zs_zip.zip,0.0004,,zip,N/A,public,,,"Gross capital formation (formerly gross domestic investment) consists of outlays on additions to the fixed assets of the economy plus net changes in the level of inventories. Fixed assets include land improvements (fences, ditches, drains, and so on); plant, machinery, and equipment purchases.",,Government,GDP,text,p3169,,,,,
Diet-induced obesity model: white adipose tissue,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6247/soft/GDS6247_full.soft.gz,0.0013,3/1/2014,zip,N/A,public,,,Analysis of epididymal and mesenteric white adipose tissues (WAT) of mice fed a high fat diet for up to 24 weeks. Excessive fat accumulation was evident in visceral WAT depots after 4 weeks. Results provide insight into the molecular events that occur during the development of diet-induced obesity.,,Health,"mice, liver, Mus musculus",DataSet full SOFT file,P1092,,,,,
Diet-induced obesity model: white adipose tissue,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6247/soft/GDS6247.soft.gz,0.006,3/1/2014,zip,N/A,public,,,Analysis of epididymal and mesenteric white adipose tissues (WAT) of mice fed a high fat diet for up to 24 weeks. Excessive fat accumulation was evident in visceral WAT depots after 4 weeks. Results provide insight into the molecular events that occur during the development of diet-induced obesity.,,Health,"mice, liver, Mus musculus",DataSet SOFT file,P1092,,,,,
Diet-induced obesity model: white adipose tissue,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE39nnn/GSE39549/soft/GSE39549_family.soft.gz,0.041,3/1/2014,zip,N/A,public,,,Analysis of epididymal and mesenteric white adipose tissues (WAT) of mice fed a high fat diet for up to 24 weeks. Excessive fat accumulation was evident in visceral WAT depots after 4 weeks. Results provide insight into the molecular events that occur during the development of diet-induced obesity.,,Health,"mice, liver, Mus musculus",Series family SOFT file,P1092,,,,,
Diet-induced obesity model: white adipose tissue,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6247/soft/GDS6247.soft.gz,0.041,3/1/2014,zip,N/A,public,,,Analysis of epididymal and mesenteric white adipose tissues (WAT) of mice fed a high fat diet for up to 24 weeks. Excessive fat accumulation was evident in visceral WAT depots after 4 weeks. Results provide insight into the molecular events that occur during the development of diet-induced obesity.,,Health,"mice, liver, Mus musculus",Series family MINiML file,P1092,,,,,
Diet-induced obesity model: white adipose tissue,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6887/annot/GPL6887.annot.gz,0.006,3/1/2014,zip,N/A,public,,,Analysis of epididymal and mesenteric white adipose tissues (WAT) of mice fed a high fat diet for up to 24 weeks. Excessive fat accumulation was evident in visceral WAT depots after 4 weeks. Results provide insight into the molecular events that occur during the development of diet-induced obesity.,,Health,"mice, liver, Mus musculus",Annotation SOFT file,P1092,,,,,
"Labor force, female (% of total labor force)",https://datahub.io/world-bank/sl.tlf.totl.fe.zs/r/sl_tlf_totl_fe_zs_zip.zip,0.000237,6/1/2018,zip,N/A,public,,,Female labor force as a percentage of the total show the extent to which women are active in the labor force. Labor force comprises people ages 15 and older who supply labor for the production of goods and services during a specified period.,,Government,"salary, labor, women",text,p3170,,,,,
 Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6177/soft/GDS6177_full.soft.gz,0.013,7/25/2013,zip,N/A,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1093,,,,,
 Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6177/soft/GDS6177.soft.gz,0.0048,7/25/2013,zip,N/A,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1093,,,,,
 Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE20nnn/GSE20489/soft/GSE20489_family.soft.gz,0.043,7/25/2013,zip,N/A,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1093,,,,,
 Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE20nnn/GSE20489/miniml/GSE20489_family.xml.tgz,0.043,7/25/2013,zip,N/A,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1093,,,,,
 Title: 	Acute alcohol consumption effect on whole blood (control group): time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPLnnn/GPL570/annot/GPL570.annot.gz,0.0081,7/25/2013,zip,N/A,public,,,"	Analysis of blood from subjects administered orange juice w/o alcohol. Blood collected at time points corresponding to collection times for the alcohol group in GDS4938. These results, together with those from GDS4938, provide insight into molecular response of blood during acute ethanol exposure.",,Health,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1093,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,https://www.ncbi.nlm.nih.gov/sites/GDSbrowser/,0.0081,9/15/2013,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,full SOFT file,P1093,,,,,
Energy intensity level of primary energy,https://datahub.io/world-bank/eg.egy.prim.pp.kd/r/eg_egy_prim_pp_kd_zip.zip,0.00023,6/1/2018,zip,N/A,public,,,Energy intensity level of primary energy is the ratio between energy supply and gross domestic product measured at purchasing power parity. Energy intensity is an indication of how much energy is used to produce one unit of economic output.,,Energy and Utilities,"energy, intensity",text,p3171,,,,,
Arable land (% of land area),https://datahub.io/world-bank/ag.lnd.arbl.zs/r/ag_lnd_arbl_zs_zip.zip,0.0004,6/1/2018,zip,N/A,public,,,"Arable land includes land defined by the FAO as land under temporary crops (double-cropped areas are counted once), temporary meadows for mowing or for pasture, land under market or kitchen gardens, and land temporarily fallow. Land abandoned as a result of shifting cultivation is excluded.",,Consumer Products,"land, agriculture",text,p3172,,,,,
Alternative and nuclear energy (% of total energy use),https://datahub.io/world-bank/eg.use.comm.cl.zs/r/eg_use_comm_cl_zs_zip.zip,0.0003,6/1/2018,zip,N/A,public,,,"Clean energy is noncarbohydrate energy that does not produce carbon dioxide when generated. It includes hydropower and nuclear, geothermal, and solar power, among others.",,Energy and Utilities,clean energy,text,p3173,,,,,
"Population, female (% of total)",https://datahub.io/world-bank/sp.pop.totl.fe.zs/r/sp_pop_totl_fe_zs_zip.zip,0.0004,6/1/2018,zip,N/A,public,,,"Female population is the percentage of the population that is female. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship.",,Government,"population, female",text,p3174,,,,,
Domestic credit to private sector (% of GDP),https://datahub.io/world-bank/fs.ast.prvt.gd.zs/r/fs_ast_prvt_gd_zs_zip.zip,0.000358,6/1/2018,zip,N/A,,,,"Domestic credit to private sector refers to financial resources provided to the private sector by financial corporations, such as through loans, purchases of nonequity securities, and trade credits and other accounts receivable, that establish a claim for repayment.",,Banking,"credit, private sector",text,p3175,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6176/soft/GDS6176.soft.gz,0.002,9/15/2013,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,DataSet full SOFT file,P1093,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE32nnn/GSE32515/soft/GSE32515_family.soft.gz,0.0115,9/15/2013,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,DataSet SOFT file,P1093,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE32nnn/GSE32515/miniml/GSE32515_family.xml.tgz,0.0114,9/15/2013,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,Series family SOFT file,P1093,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE32nnn/GSE32515/miniml/GSE32515_family.xml.tgz,0.0056,9/15/2013,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health, Mus musculus,Series family MINiML file,P1093,,,,,
Population density,https://datahub.io/world-bank/en.pop.dnst/r/en_pop_dnst_zip.zip,0.0005,6/1/2018,zip,N/A,public,,,"Population density is midyear population divided by land area in square kilometers. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship–except for refugees not permanently settled in the country of asylum, who are generally considered part of the population of their country of origin.",,Government,"population, land",text,p3176,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6100/soft/GDS6100_full.soft.gz,0.0093,4/21/2015,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1094,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6100/soft/GDS6100.soft.gz,0.002,4/21/2015,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1094,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE57nnn/GSE57820/soft/GSE57820_family.soft.gz,0.021,4/21/2015,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1094,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE57nnn/GSE57820/miniml/GSE57820_family.xml.tgz,0.021,4/21/2015,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1094,,,,,
Caspase-1 deficiency effect on lipid-loaded intestines,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL10nnn/GPL10558/annot/GPL10558.annot.gz,0.0071,4/21/2015,zip,N/A,public,,,"  	Analysis of the epithelial layer of intestine from Casp1-null, C57BL/6 males 2 hours after an oral lipid bolus. Caspase-1 activates the proinflammatory cytokines IL-1β and IL-18. These results, together with those from GDS4922, provide insight into the role of caspase-1 in lipid metabolism.",,Health,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1094,,,,,
"Employment in agriculture, female",https://datahub.io/world-bank/sl.agr.empl.fe.zs/r/sl_agr_empl_fe_zs_zip.zip,0.0002,6/1/2018,zip,N/A,public,,,"Employment is defined as persons of working age who were engaged in any activity to produce goods or provide services for pay or profit, whether at work during the reference period or not at work due to temporary absence from a job, or to working-time arrangement.",,Government,"agriculture, employment",text,p3178,,,,,
Prevalence of anemia among children,https://datahub.io/world-bank/sh.anm.chld.zs/r/sh_anm_chld_zs_zip.zip,0.000127,6/1/2018,zip,N/A,,,,"Prevalence of anemia, children under age 5, is the percentage of children under age 5 whose hemoglobin level is less than 110 grams per liter at sea level.",,Health,"anemia, children",text,p3179,,,,,
Renewable electricity output,https://datahub.io/world-bank/eg.elc.rnew.zs/r/eg_elc_rnew_zs_zip.zip,0.000204,6/1/2018,zip,N/A,public,,,Renewable electricity is the share of electrity generated by renewable power plants in total electricity generated by all types of plants.,,Energy and Utilities,"energy, electricity, renewable energy",text,p3180,,,,,
"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6083/soft/GDS6083_full.soft.gz,0.011,3/31/2015,zip,N/A,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1095,,,,,
"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6083/soft/GDS6083.soft.gz,0.0025,3/31/2015,zip,N/A,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1095,,,,,
"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62533/soft/GSE62533_family.soft.gz,0.016,3/31/2015,zip,N/A,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1095,,,,,
"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62533/miniml/GSE62533_family.xml.tgz,0.016,3/31/2015,zip,N/A,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1095,,,,,
"	
Chronic lymphocytic leukemia cells response to the neutralization of inhibitor of apoptosis proteins",ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPLnnn/GPL570/annot/GPL570.annot.gz,0.016,3/31/2015,zip,N/A,public,,,"Analysis of chronic lymphocytic leukemia (CLL) cells treated with BV6, a Smac mimetic. CLL is characterized by B-lymphocyte accumulation, which is attributed to defective cell death. Inhibitor of apoptosis (IAP) proteins are highly expressed in CLL cells. Smac binds to and inhibits IAP proteins.",,,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1095,,,,,
Urban Population,https://datahub.io/world-bank/sp.urb.totl.in.zs/r/sp_urb_totl_in_zs_zip.zip,0.000329,6/1/2018,zip,N/A,public,,,Urban population refers to people living in urban areas as defined by national statistical offices. The data are collected and smoothed by United Nations Population Division.,,Government,"population, urban",text,p3181,,,,,
"Mortality rate, infant",https://datahub.io/world-bank/sp.dyn.imrt.in/r/sp_dyn_imrt_in_zip.zip,0.000195,6/1/2018,zip,N/A,public,,,"Infant mortality rate is the number of infants dying before reaching one year of age, per 1,000 live births in a given year.",,Health,"mortality, infant",text,p3182,,,,,
Comic Characters,https://datahub.io/five-thirty-eight/comic-characters/r/comic-characters_zip.zip,0.002,6/1/2018,zip,N/A,public,,,"This folder contains data behind the story Comic Books Are Still Made By Men, For Men And About Men. The data comes from Marvel Wikia and DC Wikia.",,Media and Entertainment,,text,p3183,,,,,
Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6082/soft/GDS6082_full.soft.gz,0.0084,3/25/2015,zip,N/A,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1096,,,,,
Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6082/soft/GDS6082.soft.gz,0.0014,3/25/2015,zip,N/A,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1096,,,,,
Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67198/soft/GSE67198_family.soft.gz,0.0014,3/25/2015,zip,N/A,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1096,,,,,
Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/GSE67198/miniml/GSE67198_family.xml.tgz,0.0175,3/25/2015,zip,N/A,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1096,,,,,
Sendai virus infection effect on monocytic cell line: dose response,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL10nnn/GPL10558/annot/GPL10558.annot.gz,0.007,3/25/2015,zip,N/A,public,,,Analysis of U937 monocytic cells infected at multiplicities of infection of 0.0002 and 0.02 with the Sendai virus. Results provide insight into the differences between the innate immune response of cells infected with a low and high concentration of virus.,,,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1096,,,,,
Congress Age,https://datahub.io/five-thirty-eight/congress-age/r/congress-age_zip.zip,0.001,6/1/2018,zip,N/A,public,,,This folder contains the data behind the story Both Republicans And Democrats Have an Age Problem congress-terms.csv has an entry for every member of congress who served at any point during a particular congress between January 1947 and Februrary 2014.,,Government,"congress, whitehouse",text,p3184,,,,,
Mayweather Mcgregor,https://datahub.io/five-thirty-eight/mayweather-mcgregor/r/mayweather-mcgregor_zip.zip,0.002,6/1/2018,zip,N/A,public,,,"This folder contains 12,118 tweets that contain one or more emojis and match one or more of the following hashtags: #MayMac, #MayweatherMcGregor, #MayweatherVMcGregor, #MayweatherVsMcGregor, #McGregor and #Mayweather.",,Media and Entertainment,"fight, mayweather",text,p3185,,,,,
Soccer Spi,https://datahub.io/five-thirty-eight/soccer-spi/r/soccer-spi_zip.zip,0.001,6/1/2018,zip,N/A,public,,,This file contains links to the data behind our Club Soccer Predictions and Global Club Soccer Rankings.,,Media and Entertainment,"sports, soccer",text,p3186,,,,,
Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6064/soft/GDS6064_full.soft.gz,0.0092,3/1/2015,zip,N/A,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1096,,,,,
Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6064/soft/GDS6064.soft.gz,0.002,3/1/2015,zip,N/A,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1096,,,,,
Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61140/soft/GSE61140_family.soft.gz,0.0138,3/1/2015,zip,N/A,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1096,,,,,
Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61140/miniml/GSE61140_family.xml.tgz,0.0137,3/1/2015,zip,N/A,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1096,,,,,
Arthritic tarsal joints induced by collagen: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.0137,3/1/2015,zip,N/A,public,,,Analysis of tarsal joints for up to 4 weeks following treatment with collagen to induce arthritis. Collagen induced arthritis is a model for rheumatoid arthritis. Results provide insight into the molecular events underlying the inflammation and bone remodeling that occurs in inflammatory arthritis.,,,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1096,,,,,
Real Estate Across The United States Building Inventory,https://datahub.io/JohnSnowLabs/real-estate-across-the-united-states-building-inventory/r/datapackage_zip.zip,0.001,1/1/2018,zip,N/A,public,,,Real estate inventory (United States). ,,Consumer Products,"real estate, housing",text,p3187,Real Estate,,,,
Employee Salaries Montgomery County MD 2014,https://datahub.io/JohnSnowLabs/employee-salaries-montgomery-county-md-2014/r/datapackage_zip.zip,0.001,1/1/2018,zip,N/A,public,,,Employee salaries in Montgomery county. ,,Government,salary,text,p3188,,,,,
State of the City Corpus,https://datahub.io/etachov/state-of-the-city/r/datapackage_zip.zip,0.001,1/1/2018,zip,N/A,public,,,A corpus of State of the City speeches made by mayors across America. Right now I'm focusing three years of speeches from the fifteen largest cities and am tracking my progress here.,,Government,"speeches, mayor",text,p3189,,,,,
Estimates Emissions of CO2,https://datahub.io/JohnSnowLabs/estimates-emissions-of-co2-at-country-and-global-level-starting-1751/r/estimates-emissions-of-co2-at-country-and-global-level-starting-1751_zip.zip,0.001,1/1/2018,zip,N/A,public,,,Estimates of CO2 emissions at at country and global level (starting in 1751).,,Government,emissions,text,p3190,,,,,
GDP by Industry and Country,https://datahub.io/JohnSnowLabs/gdp-by-industry-and-country/r/gdp-by-industry-and-country_zip.zip,0.003,1/1/2018,zip,N/A,public,,,"GDP by industry and country - includes information about currency, indicator, and yearly estimates.",,Government,GDP,text,p3191,,,,,
Iswc2018 dataset,https://datahub.io/sunzequn/iswc2018-dataset/r/iswc2018-dataset_zip.zip,2,5/1/2018,zip,Apache License 2.0,public,,,"Datasets for Embedding-based Entity Alignment We considered the following four aspects to build our datasets: source KG, dataset language, entity size and difference of degree distributions between the extracted datasets and original KGs.",,Electronics,embedding,text,p3192,,,,,
"Charges for the use of intellectual property, receipts",https://datahub.io/test/bx.gsr.royl.cd/r/bx_gsr_royl_cd_zip.zip,0.000196,6/1/2018,zip,CC-BY-4.0,public,,,"Charges for the use of intellectual property are payments and receipts between residents and nonresidents for the authorized use of proprietary rights (such as patents, trademarks, copyrights, industrial processes and designs including trade secrets, and franchises) and for the use, through licensing agreements, of produced originals or prototypes (such as copyrights on books and manuscripts, computer software, cinematographic works, and sound recordings) and related rights (such as for live performances and television, cable, or satellite broadcast). Data are in current U.S. dollars.",,Consumer Products,"trade, property",text,p3193,,,,,
"Life expectancy at birth, male (years)",https://datahub.io/world-bank/sp.dyn.le00.ma.in/r/sp_dyn_le00_ma_in_zip.zip,0.0003,6/1/2018,zip,CC-BY-4.0,public,,,Life expectancy at birth indicates the number of years a newborn infant would live if prevailing patterns of mortality at the time of its birth were to stay the same throughout its life.,,Health,"life expectancy, male",text,p3194,,,,,
	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6063/soft/GDS6063_full.soft.gz,0.0091,2/1/2016,zip,N/A,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",DataSet full SOFT file,P1097,,,,,
	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6063/soft/GDS6063.soft.gz,0.0019,2/1/2016,zip,N/A,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",DataSet SOFT file,P1097,,,,,
	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE68nnn/GSE68849/soft/GSE68849_family.soft.gz,0.021,2/1/2016,zip,N/A,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",Series family SOFT file,P1097,,,,,
	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE68nnn/GSE68849/miniml/GSE68849_family.xml.tgz,0.00216,2/1/2016,zip,N/A,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",Series family MINiML file,P1097,,,,,
	Influenza A effect on plasmacytoid dendritic cells,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL10nnn/GPL10558/annot/GPL10558.annot.gz,0.007,2/1/2016,zip,N/A,public,,,"Analysis of primary plasmacytoid dendritic cells (pDC) exposed to influenza A for 8 hours ex vivo. pDCs are vital to antiviral defense, directing immune responses via secretion of interferon-alpha. Results provide insight into the regulation of the response of pDC to viral pathogens.",,,"homo sapiens, organism, alcohol effect",Annotation SOFT file,P1097,,,,,
"Primary completion rate, total (% of relevant age group)",https://datahub.io/world-bank/se.prm.cmpt.zs/r/se_prm_cmpt_zs_zip.zip,0.00023,6/1/2018,zip,CC-BY-4.0,public,,,"Primary completion rate, or gross intake ratio to the last grade of primary education, is the number of new entrants (enrollments minus repeaters) in the last grade of primary education, regardless of age, divided by the population at the entrance age for the last grade of primary education. ",,Education,"completion, education, rate",text,p3195,,,,,
"Total debt service (% of exports of goods, services and primary income)",https://datahub.io/world-bank/dt.tds.dect.ex.zs/r/dt_tds_dect_ex_zs_zip.zip,0.00017,6/1/2018,zip,CC-BY-4.0,public,,,"Total debt service to exports of goods, services and primary income. Total debt service is the sum of principal repayments and interest actually paid in currency, goods, or services on long-term debt, interest paid on short-term debt, and repayments (repurchases and charges) to the IMF.",,Banking,debt,text,p3196,,,,,
"Gross intake ratio in first grade of primary education, female",https://datahub.io/world-bank/se.prm.gint.fe.zs/r/se_prm_gint_fe_zs_zip.zip,0.000218,6/1/2018,zip,CC-BY-4.0,public,,,"Gross intake ratio in first grade of primary education is the number of new entrants in the first grade of primary education regardless of age, expressed as a percentage of the population of the official primary entrance age.",,Education,primary,text,p3196,Primary,,,,
"Life expectancy at birth, female (years)",https://datahub.io/world-bank/sp.dyn.le00.fe.in/r/sp_dyn_le00_fe_in_zip.zip,0.0003,6/1/2018,zip,CC-BY-4.0,public,,,Life expectancy at birth indicates the number of years a newborn infant would live if prevailing patterns of mortality at the time of its birth were to stay the same throughout its life.,,Health,"life expectancy, female",text,p3197,,,,,
High-technology exports (current US$),https://datahub.io/world-bank/tx.val.tech.cd/r/tx_val_tech_cd_zip.zip,0.00012,6/1/2018,zip,CC-BY-4.0,public,,,"High-technology exports are products with high R&D intensity, such as in aerospace, computers, pharmaceuticals, scientific instruments, and electrical machinery. Data are in current U.S. dollars.",,Consumer Products,"exports, technology",text,p3198,,,,,
Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6016/soft/GDS6016_full.soft.gz,0.0091,12/23/2013,zip,N/A,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,DataSet full SOFT file,P1098,,,,,
Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6016/soft/GDS6016.soft.gz,0.000729,12/23/2013,zip,N/A,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,DataSet SOFT file,P1098,,,,,
Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE51nnn/GSE51612/soft/GSE51612_family.soft.gz,0.0089,12/23/2013,zip,N/A,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,Series family SOFT file,P1098,,,,,
Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE51nnn/GSE51612/miniml/GSE51612_family.xml.tgz,0.009,12/23/2013,zip,N/A,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,Series family MINiML file,P1098,,,,,
Transcription factor engrailed-2 loss-of-function model of autism spectrum disorder: hippocampus,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL7nnn/GPL7202/annot/GPL7202.annot.gz,0.0084,12/23/2013,zip,N/A,public,,,Analysis of hippocampal samples from engrailed-2 (En2) mutant adults. The En2-/- animals are a model for autism spectrum disorder (ASD). The hippocampus is a brain area profoundly affected in ASD patients. Results provide insight into molecular mechanisms underlying ASD-related neuropathology.,,,	Mus musculus,Annotation SOFT file,P1098,,,,,
Merchandise exports (current US$),https://datahub.io/world-bank/tx.val.mrch.cd.wt/r/tx_val_mrch_cd_wt_zip.zip,0.000306,6/15/2018,zip,CC-BY-4.0,public,,,Merchandise exports show the f.o.b. value of goods provided to the rest of the world valued in current U.S. dollars.,,Consumer Products,"exports, merchandise",text,p3199,,,,,
Food production index,https://datahub.io/world-bank/ag.prd.food.xd/r/ag_prd_food_xd_zip.zip,0.00027,6/15/2018,zip,CC-BY-4.0,public,,,"Food production index covers food crops that are considered edible and that contain nutrients. Coffee and tea are excluded because, although edible, they have no nutritive value.",,Consumer Products,"food, health, production",text,p3200,,,,,
"Net flows on external debt, total",https://datahub.io/world-bank/dt.nfl.dect.cd/r/dt_nfl_dect_cd_zip.zip,0.000137,6/15/2018,zip,CC-BY-4.0,public,,,"Net flows on external debt are disbursements on long-term external debt and IMF purchases minus principal repayments on long-term external debt and IMF repurchases up to 1984. Beginning in 1985 this line includes the change in stock of short-term debt (including interest arrears for long-term debt). Thus, if the change in stock is positive, a disbursement is assumed to have taken place; if negative, a repayment is assumed to have taken place. Long-term external debt is defined as debt that has an original or extended maturity of more than one year and that is owed to nonresidents by residents of an economy and repayable in currency, goods, or services.",,Banking,"debt, NFL",text,p3201,,,,,
Exports of goods and services (% of GDP),https://datahub.io/world-bank/ne.exp.gnfs.zs/r/ne_exp_gnfs_zs_zip.zip,0.0004,6/15/2018,zip,CC-BY-4.0,public,,,"Exports of goods and services represent the value of all goods and other market services provided to the rest of the world. They include the value of merchandise, freight, insurance, transport, travel, royalties, license fees, and other services, such as communication, construction, financial, information, business, personal, and government services. ",,Consumer Products,exports,text,p3202,,,,,
"School enrollment, secondary (% net)",https://datahub.io/world-bank/se.sec.nenr/r/se_sec_nenr_zip.zip,0.000143,6/15/2018,zip,CC-BY-4.0,public,,,"Net enrollment rate is the ratio of children of official school age who are enrolled in school to the population of the corresponding official school age. Secondary education completes the provision of basic education that began at the primary level, and aims at laying the foundations for lifelong learning and human development, by offering more subject- or skill-oriented instruction using more specialized teachers.",,Education,enrollment,text,p3203,,,,,
Energy use (kg of oil equivalent per capita),https://datahub.io/world-bank/eg.use.pcap.kg.oe/r/eg_use_pcap_kg_oe_zip.zip,0.000302,6/15/2018,zip,CC-BY-4.0,public,,,"Energy use refers to use of primary energy before transformation to other end-use fuels, which is equal to indigenous production plus imports and stock changes, minus exports and fuels supplied to ships and aircraft engaged in international transport.",,Energy and Utilities,energy,text,p3204,,,,,
	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6010/soft/GDS6010_full.soft.gz,0.0091,1/4/2016,zip,N/A,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,DataSet full SOFT file,P1099,,,,,
	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6010/soft/GDS6010.soft.gz,0.0027,1/4/2016,zip,N/A,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,DataSet SOFT file,P1099,,,,,
	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE66nnn/GSE66597/soft/GSE66597_family.soft.gz,0.007,1/4/2016,zip,N/A,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,Series family SOFT file,P1099,,,,,
	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE66nnn/GSE66597/miniml/GSE66597_family.xml.tgz,0.0127,1/4/2016,zip,N/A,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,Series family MINiML file,P1099,,,,,
	Influenza virus H5N1 infection of U251 astrocyte cell line: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6480/annot/GPL6480.annot.gz,0.007,1/4/2016,zip,N/A,public,,,Analysis of U251 astrocyte cells infected with the influenza H5N1 virus for up to 24 hours. Results provide insight into the immune response of astrocytes to H5N1 infection.,,,Homo Sapiens,Annotation SOFT file,P1099,,,,,
"GNI, Atlas method (current US$)",https://datahub.io/world-bank/ny.gnp.atls.cd/r/ny_gnp_atls_cd_zip.zip,0.000391,6/15/2018,zip,CC-BY-4.0,public,,,"GNI (formerly GNP) is the sum of value added by all resident producers plus any product taxes (less subsidies) not included in the valuation of output plus net receipts of primary income (compensation of employees and property income) from abroad. Data are in current U.S. dollars. GNI, calculated in national currency, is usually converted to U.S. dollars at official exchange rates for comparisons across economies, although an alternative rate is used when the official exchange rate is judged to diverge by an exceptionally large margin from the rate actually applied in international transactions. ",,Government,"tax, GNI",text,p3205,,,,,
High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6000/soft/GDS6000_full.soft.gz,0.012,1/8/2015,zip,N/A,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,DataSet full SOFT file,P2000,,,,,
High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS6nnn/GDS6000/soft/GDS6000.soft.gz,0.05,1/8/2015,zip,N/A,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,DataSet SOFT file,P2000,,,,,
High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE64nnn/GSE64718/soft/GSE64718_family.soft.gz,0.25,1/8/2015,zip,N/A,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,Series family SOFT file,P2000,,,,,
High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE64nnn/GSE64718/miniml/GSE64718_family.xml.tgz,0.25,1/8/2015,zip,N/A,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,Series family MINiML file,P2000,,,,,
High-fat diet effect on brown adipose tissue development,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6887/annot/GPL6887.annot.gz,0.066,1/8/2015,zip,N/A,public,,,Analysis of interscapular brown adipose tissues from mice fed a high fat diet for up to 24 weeks. Results compared to those from epididymal white adipose tissues (GDS6247) in order to provide insight into the effect of high-fat diets on the development of brown and white adipose tissues,,,	Mus musculus,Annotation SOFT file,P2000,,,,,
Refugee population by country or territory of asylum,https://datahub.io/world-bank/sm.pop.refg/r/sm_pop_refg_zip.zip,0.000108,6/15/2018,zip,CC-BY-4.0,public,,,"Refugees are people who are recognized as refugees under the 1951 Convention Relating to the Status of Refugees or its 1967 Protocol, the 1969 Organization of African Unity Convention Governing the Specific Aspects of Refugee Problems in Africa, people recognized as refugees in accordance with the UNHCR statute, people granted refugee-like humanitarian status, and people provided temporary protection. Asylum seekers–people who have applied for asylum or refugee status and who have not yet received a decision or who are registered as asylum seekers–are excluded. Palestinian refugees are people (and their descendants) whose residence was Palestine between June 1946 and May 1948 and who lost their homes and means of livelihood as a result of the 1948 Arab-Israeli conflict. Country of asylum is the country where an asylum claim was filed and granted.",,Government,"refugees, humanitarian",text,p3206,,,,,
Agricultural land (% of land area),https://datahub.io/world-bank/ag.lnd.agri.zs/r/ag_lnd_agri_zs_zip.zip,0.000373,6/15/2018,zip,CC-BY-4.0,public,,,"Agricultural land refers to the share of land area that is arable, under permanent crops, and under permanent pastures. Arable land includes land defined by the FAO as land under temporary crops (double-cropped areas are counted once), temporary meadows for mowing or for pasture, land under market or kitchen gardens, and land temporarily fallow.",,Government,"arable, agriculture",text,p3207,,,,,
Rural population,https://datahub.io/world-bank/sp.rur.totl/r/sp_rur_totl_zip.zip,336,6/15/2018,zip,CC-BY-4.0,public,,,Rural population refers to people living in rural areas as defined by national statistical offices. It is calculated as the difference between total population and urban population. Aggregation of urban and rural population may not add up to total population because of different country coverages.,,Government,"population, rural",text,p3208,,,,,
Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5948/soft/GDS5948_full.soft.gz,0.0077,5/15/2015,zip,N/A,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,DataSet full SOFT file,P2001,,,,,
Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5948/soft/GDS5948.soft.gz,0.0009,5/15/2015,zip,N/A,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,DataSet SOFT file,P2001,,,,,
Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE56nnn/GSE56819/soft/GSE56819_family.soft.gz,0.013,5/15/2015,zip,N/A,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,Series family SOFT file,P2001,,,,,
Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE56nnn/GSE56819/miniml/GSE56819_family.xml.tgz,0.013,5/15/2015,zip,N/A,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,Series family MINiML file,P2001,,,,,
Zipper-interacting protein kinase deficiency effect on coronary artery smooth muscle cells in vitro,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6244/annot/GPL6244.annot.gz,0.0069,5/15/2015,zip,N/A,public,,,"Analysis of cultured vascular smooth muscle cells following knockdown of zipper-interacting protein kinase (ZIPK). ZIPK is phosphorylated and activated by Rho-associated kinase 1 (ROCK1). These results, together with those from GDS5659, provide further insight into ROCK1 and ZIPK functions.",,,Homo Sapiens,Annotation SOFT file,P2001,,,,,
Net ODA received,https://datahub.io/world-bank/dt.oda.odat.gn.zs/r/dt_oda_odat_gn_zs_zip.zip,0.000327,6/15/2018,zip,CC-BY-4.0,public,,,"Net official development assistance (ODA) consists of disbursements of loans made on concessional terms (net of repayments of principal) and grants by official agencies of the members of the Development Assistance Committee (DAC), by multilateral institutions, and by non-DAC countries to promote economic development and welfare in countries and territories in the DAC list of ODA recipients.",,Government,"loans, banking, welfare",text,p3209,,,,,
"Birth rate, crude (per 1,000 people)",https://datahub.io/world-bank/sp.dyn.cbrt.in/r/sp_dyn_cbrt_in_zip.zip,0.0003,6/15/2018,zip,CC-BY-4.0,public,,,"Crude birth rate indicates the number of live births occurring during the year, per 1,000 population estimated at midyear.",,Government,birth rate,text,p3210,,,,,
"Gross intake ratio in first grade of primary education, male",https://datahub.io/world-bank/se.prm.gint.ma.zs/r/se_prm_gint_ma_zs_zip.zip,0.000217,6/15/2018,zip,CC-BY-4.0,public,,,"Gross intake ratio in first grade of primary education is the number of new entrants in the first grade of primary education regardless of age, expressed as a percentage of the population of the official primary entrance age.",,Education,primary,text,p3211,Primary,,,,
Population ages 15-64 (% of total),https://datahub.io/world-bank/sp.pop.1564.to.zs/r/sp_pop_1564_to_zs_zip.zip,0.0005,6/15/2018,zip,CC-BY-4.0,public,,,"Total population between the ages 15 to 64 as a percentage of the total population. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship.",,Government,population,text,p3212,,,,,
High-technology exports (% of manufactured exports),https://datahub.io/world-bank/tx.val.tech.mf.zs/r/tx_val_tech_mf_zs_zip.zip,0.000179,6/15/2018,zip,CC-BY-4.0,public,,,"High-technology exports are products with high R&D intensity, such as in aerospace, computers, pharmaceuticals, scientific instruments, and electrical machinery. Data are in current U.S. dollars.",,Consumer Products,"technology, exports",text,p3213,,,,,
"Current account balance (BoP, current US$)",https://datahub.io/world-bank/bn.cab.xoka.cd/r/bn_cab_xoka_cd_zip.zip,0.000216,6/15/2018,zip,CC-BY-4.0,public,,,"Current account balance is the sum of net exports of goods and services, net primary income, and net secondary income. Data are in current U.S. dollars.",,Banking,account balance,text,p3214,,,,,
Fossil fuel energy consumption (% of total),https://datahub.io/world-bank/eg.use.comm.fo.zs/r/eg_use_comm_fo_zs_zip.zip,0.000292,6/15/2018,zip,CC-BY-4.0,public,,,"Fossil fuel comprises coal, oil, petroleum, and natural gas products.",,Energy and Utilities,,text,p3215,,,,,
Military expenditure (% of GDP),https://datahub.io/world-bank/ms.mil.xpnd.gd.zs/r/ms_mil_xpnd_gd_zs_zip.zip,0.000317,6/15/2018,zip,CC-BY-4.0,public,,,"Military expenditures data from SIPRI are derived from the NATO definition, which includes all current and capital expenditures on the armed forces, including peacekeeping forces; defense ministries and other government agencies engaged in defense projects; paramilitary forces, if these are judged to be trained and equipped for military operations; and military space activities. Such expenditures include military and civil personnel, including retirement pensions of military personnel and social services for personnel; operation and maintenance; procurement; military research and development; and military aid (in the military expenditures of the donor country).",,Government,"expenditures, military, banking",text,p3216,Military,,,,
Imports of goods and services (% of GDP),https://datahub.io/world-bank/ne.imp.gnfs.zs/r/ne_imp_gnfs_zs_zip.zip,0.0004,6/15/2018,zip,CC-BY-4.0,public,,,"Imports of goods and services represent the value of all goods and other market services received from the rest of the world. They include the value of merchandise, freight, insurance, transport, travel, royalties, license fees, and other services, such as communication, construction, financial, information, business, personal, and government services. They exclude compensation of employees and investment income (formerly called factor services) and transfer payments.",,Consumer Products,"imports, services, goods",text,p3217,,,,,
"Contributing family workers, male",https://datahub.io/world-bank/sl.fam.work.ma.zs/r/sl_fam_work_ma_zs_zip.zip,0.00019,6/15/2018,zip,CC-BY-4.0,public,,,Contributing family workers are those workers who hold “self-employment jobs” as own-account workers in a market-oriented establishment operated by a related person living in the same household.,,Government,household,text,p3218,,,,,
"Immunization, measles (% of children ages 12-23 months)",https://datahub.io/world-bank/sh.imm.meas/r/sh_imm_meas_zip.zip,0.000142,6/15/2018,zip,CC-BY-4.0,public,,,"Child immunization, measles, measures the percentage of children ages 12-23 months who received the measles vaccination before 12 months or at any time before the survey. A child is considered adequately immunized against measles after receiving one dose of vaccine.",,Health,"immunization, measles",text,p3219,,,,,
"Unemployment, total (% of total labor force)",https://datahub.io/world-bank/sl.uem.totl.zs/r/sl_uem_totl_zs_zip.zip,0.000171,6/15/2018,zip,CC-BY-4.0,public,,,Unemployment refers to the share of the labor force that is without work but available for and seeking employment.,,Government,unemployment,text,p3220,,,,,
"Employment in industry, male",https://datahub.io/world-bank/sl.ind.empl.ma.zs/r/sl_ind_empl_ma_zs_zip.zip,0.000179,6/15/2018,zip,CC-BY-4.0,public,,,"Employment is defined as persons of working age who were engaged in any activity to produce goods or provide services for pay or profit, whether at work during the reference period or not at work due to temporary absence from a job, or to working-time arrangement.",,Government,employment,text,p3221,,,,,
"Foreign direct investment, net inflows",https://datahub.io/world-bank/bx.klt.dinv.cd.wd/r/bx_klt_dinv_cd_wd_zip.zip,0.000313,6/15/2018,zip,CC-BY-4.0,public,,,"Foreign direct investment refers to direct investment equity flows in the reporting economy. It is the sum of equity capital, reinvestment of earnings, and other capital. Direct investment is a category of cross-border investment associated with a resident in one economy having control or a significant degree of influence on the management of an enterprise that is resident in another economy. ",,Banking,"investment, foreign",text,p3222,,,,,
"Wage and salaried workers, male",https://datahub.io/world-bank/sl.emp.work.ma.zs/r/sl_emp_work_ma_zs_zip.zip,0.000179,6/15/2018,zip,CC-BY-4.0,public,,,"Wage and salaried workers (employees) are those workers who hold the type of jobs defined as “paid employment jobs,” where the incumbents hold explicit (written or oral) or implicit employment contracts that give them a basic remuneration that is not directly dependent upon the revenue of the unit for which they work.",,Government,salary,text,p3223,,,,,
"Immunization, DPT (% of children ages 12-23 months)",https://datahub.io/world-bank/sh.imm.idpt/r/sh_imm_idpt_zip.zip,0.000144,6/15/2018,zip,CC-BY-4.0,public,,,"Child immunization, DPT, measures the percentage of children ages 12-23 months who received DPT vaccinations before 12 months or at any time before the survey.",,Health,immunization,text,p3224,,,,,
Technical cooperation grants ,https://datahub.io/world-bank/bx.grt.tech.cd.wd/r/bx_grt_tech_cd_wd_zip.zip,0.000147,6/15/2018,zip,CC-BY-4.0,public,,,"Technical cooperation grants include free-standing technical cooperation grants, which are intended to finance the transfer of technical and managerial skills or of technology for the purpose of building up general national capacity without reference to any specific investment projects; and investment-related technical cooperation grants, which are provided to strengthen the capacity to execute specific investment projects. Data are in current U.S. dollars.",,Banking,"grants, technical",text,p3225,,,,,
Goose,https://datahub.io/five-thirty-eight/goose/r/goose_zip.zip,0.002,6/15/2018,zip,N/A,public,,,"This folder contains data behind the stories: The Save Ruined Relief Pitching, The Goose Egg Can Fix It; Kenley Jansen Is The Model Of A Modern Reliever.",,Media and Entertainment,"news, stories",text,p3226,,,,,
Congress Trump Score,https://datahub.io/five-thirty-eight/congress-trump-score/r/congress-trump-score_zip.zip,0.002,6/15/2018,zip,N/A,public,,,This file contains links to the data behind Tracking Congress In The Age Of Trump. This dataset was scraped from FiveThirtyEight - congress-trump-score,,Media and Entertainment,"Trump, score",text,p3227,,,,,
Reluctant Trump,https://datahub.io/five-thirty-eight/reluctant-trump/r/reluctant-trump_zip.zip,0.001,6/15/2018,zip,N/A,public,,,Files in this directory contain data from a SurveyMonkey News Survey conducted in partnership with FiveThirtyEight and include crosstabs of Trump voters who were excited to vote for Trump versus those who were not.,,Media and Entertainment,"Trump, score",text,p3228,,,,,
Diagnosed Diabetes Prevalence 2004-2013,https://datahub.io/JohnSnowLabs/diagnosed-diabetes-prevalence-2004-2013/r/datapackage_zip.zip,0.002,1/1/2018,zip,N/A,public,,,Dataset includes information about diabetes diagnoses between 2004 and 2013.,,Health,diabetes,text,p3229,,,,,
City Population Annual Timeseries,https://datahub.io/JohnSnowLabs/city-population-annual-timeseries/r/city-population-annual-timeseries_zip.zip,0.002,1/1/2018,zip,proprietary,public,,,Timeseries of cities' populations. ,,Government,population,text,p3230,,,,,
Speed Dating,https://datahub.io/machine-learning/speed-dating/r/speed-dating_zip.zip,0.001,6/1/2018,zip,N/A,public,,,"This data was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four-minute “first date” with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again.",,Social Sciences,dating,text,p3231,,,,,
GDP per capita (current US$),https://datahub.io/world-bank/ny.gdp.pcap.cd/r/ny_gdp_pcap_cd_zip.zip,0.00043,6/15/2018,zip,CC-BY-4.0,public,,,GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.,,Government,GDP,text,p3232,,,,,
"Inflation, GDP deflator (annual %)",https://datahub.io/world-bank/ny.gdp.defl.kd.zg/r/ny_gdp_defl_kd_zg_zip.zip,0.00041,6/15/2018,zip,CC-BY-4.0,public,,,Inflation as measured by the annual growth rate of the GDP implicit deflator shows the rate of price change in the economy as a whole. The GDP implicit deflator is the ratio of GDP in current local currency to GDP in constant local currency.,,Government,"inflation, GDP",text,p3233,,,,,
Nba Elo,https://datahub.io/five-thirty-eight/nba-elo/r/nba-elo_zip.zip,0.015,6/25/2018,zip,N/A,public,periodically,,This directory contains the data behind the Complete History Of The NBA interactive.,,Media and Entertainment,NBA,text,p3234,,,,,
Nba Carmelo,https://datahub.io/five-thirty-eight/nba-carmelo/r/nba-carmelo_zip.zip,0.01,6/25/2018,zip,N/A,public,,,This file contains links to the data behind The Complete History Of The NBA and our NBA Predictions.,,Media and Entertainment,NBA,text,p3235,,,,,
Health Insurance Plans Unified Rate Review,https://datahub.io/JohnSnowLabs/health-insurance-plans-unified-rate-review-puf-2015/r/datapackage_zip.zip,0.06,1/1/2018,zip,N/A,public,,,"Review of health insurance plans, 2015. ",,Health,insurance,text,p3236,,,,,
Brooklyn Public Library Catalog,https://datahub.io/JohnSnowLabs/brooklyn-public-library-catalog/r/datapackage_zip.zip,0.157,1/1/2018,zip,N/A,public,,,Public library catalogue (2018),,Consumer Products,library,text,p3237,,,,,
Euro 3 Cars Emissions Traded On UK Market 2000-2007,https://datahub.io/JohnSnowLabs/euro-3-cars-emissions-traded-on-uk-market-2000-2007/r/euro-3-cars-emissions-traded-on-uk-market-2000-2007_zip.zip,0.001,1/1/2018,zip,N/A,public,,,UK traded car emissions,,Consumer Products,trade,text,p3238,,,,,
Mammography Data from Breast Cancer Surveillance Consortium,https://datahub.io/JohnSnowLabs/mammography-data-from-breast-cancer-surveillance-consortium/r/mammography-data-from-breast-cancer-surveillance-consortium_zip.zip,0.001,1/1/2018,zip,N/A,public,,,Mammography Data,,Health,,text,p3239,,,,,
Multiveiw Datasets,https://vision.in.tum.de/old/data/bird_data.tar.gz,0.0248,v1,.tar.gz,Proprietary,,,,"Bird Data: We provide multiple datasets capturing objects from various vantage points. Each entry contains an image sequence, corresponding silhouettes and full calibration parameters. We are happy to share our data with other researchers. Please refer to the respective publication when using this data.","Images are of birds; cite: Continuous Global Optimization in Multiview 3D Reconstruction (K. Kolev, M. Klodt, T. Brox, D. Cremers), In International Journal of Computer Vision, volume 84, 2009; Multiview Stereo and Silhouette Consistency via Convex Functionals over Convex Domains (D. Cremers, K. Kolev), In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 33, 2011.",Electronics,"Multiview, stereo, silhouette, 3D reconstruction",Image,P5135,Robotics,,,,
Multiveiw Datasets,https://vision.in.tum.de/old/data/beethoven_data.tar.gz,0.0393,v1,.tar.gz,Proprietary,,,,"Beethoven Data: We provide multiple datasets capturing objects from various vantage points. Each entry contains an image sequence, corresponding silhouettes and full calibration parameters. We are happy to share our data with other researchers. Please refer to the respective publication when using this data.","Images are of Beethoven bust; Continuous Global Optimization in Multiview 3D Reconstruction (K. Kolev, M. Klodt, T. Brox, D. Cremers), In International Journal of Computer Vision, volume 84, 2009; Multiview Stereo and Silhouette Consistency via Convex Functionals over Convex Domains (D. Cremers, K. Kolev), In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 33, 2011.",Electronics,"Multiview, stereo, silhouette, 3D reconstruction",Image,P5135,Robotics,,,,
Multiveiw Datasets,https://vision.in.tum.de/old/data/bunny_data.tar.gz,0.0435,v1,.tar.gz,Proprietary,,,,"Bunny data: We provide multiple datasets capturing objects from various vantage points. Each entry contains an image sequence, corresponding silhouettes and full calibration parameters. We are happy to share our data with other researchers. Please refer to the respective publication when using this data.","Images are of bunnie; Continuous Global Optimization in Multiview 3D Reconstruction (K. Kolev, M. Klodt, T. Brox, D. Cremers), In International Journal of Computer Vision, volume 84, 2009; Multiview Stereo and Silhouette Consistency via Convex Functionals over Convex Domains (D. Cremers, K. Kolev), In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 33, 2011.",Electronics,"Multiview, stereo, silhouette, 3D reconstruction",Image,P5135,Robotics,,,,
Multiveiw Datasets,https://vision.in.tum.de/old/data/head_data.tar.gz,0.0383,v1,.tar.gz,Proprietary,,,,"Head data: We provide multiple datasets capturing objects from various vantage points. Each entry contains an image sequence, corresponding silhouettes and full calibration parameters. We are happy to share our data with other researchers. Please refer to the respective publication when using this data.","Images are of heads; Continuous Global Optimization in Multiview 3D Reconstruction (K. Kolev, M. Klodt, T. Brox, D. Cremers), In International Journal of Computer Vision, volume 84, 2009; Multiview Stereo and Silhouette Consistency via Convex Functionals over Convex Domains (D. Cremers, K. Kolev), In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 33, 2011.",Electronics,"Multiview, stereo, silhouette, 3D reconstruction",Image,P5135,Robotics,,,,
Multiveiw Datasets,https://vision.in.tum.de/old/data/pig_data.tar.gz,0.0297,v1,.tar.gz,Proprietary,,,,"Pig data: We provide multiple datasets capturing objects from various vantage points. Each entry contains an image sequence, corresponding silhouettes and full calibration parameters. We are happy to share our data with other researchers. Please refer to the respective publication when using this data.","Images are of pigs; Continuous Global Optimization in Multiview 3D Reconstruction (K. Kolev, M. Klodt, T. Brox, D. Cremers), In International Journal of Computer Vision, volume 84, 2009; Multiview Stereo and Silhouette Consistency via Convex Functionals over Convex Domains (D. Cremers, K. Kolev), In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 33, 2011.",Electronics,"Multiview, stereo, silhouette, 3D reconstruction",Image,P5135,Robotics,,,,
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_xyz.tgz,0.47,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/xyz; Duration: 30.09s, Length: 7.112m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_rpy.tgz,0.42,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/rpy; Duration: 27.67s, Length: 1.664m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_xyz.tgz,2.39,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/xyz; Duration: 122.74s, Length: 7.029m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_rpy.tgz,2.13,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Testing and debugging datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/rpy; Duration: 109.97s, Length: 1.506m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_360.tgz,0.45,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/360; Duration: 28.69s, Length: 5.818m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_floor.tgz,0.82,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/floor; Duration: 49.87s, Length: 12.569m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk.tgz,0.36,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/desk; Duration: 23.40s, Length: 9.263m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk2.tgz,0.37,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/desk2; Duration: 24.86s, Length: 10.161m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room.tgz,0.83,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/room; Duration: 48.90s, Length: 15.989m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_hemisphere.tgz,1.5,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/360_hemisphere; Duration: 91.48s, Length: 14.773m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_kidnap.tgz,0.74,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/360_kidnap; Duration: 48.04s, Length: 14.286m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk.tgz,2.01,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/desk; Duration: 99.36s, Length: 18.880m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_large_no_loop.tgz,1.92,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/large_no_loop; Duration: 112.37s, Length: 26.086m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_large_with_loop.tgz,2.83,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/large_with_loop; Duration: 173.19s, Length: 39.111m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_long_office_household.tgz,1.58,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Handheld SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/long_office_household; Duration: 87.09s, Length: 21.455m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_360.tgz,0.73,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_360; Duration: 72.75s, Length: 16.118m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_slam.tgz,1.82,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_slam; Duration: 155.72s, Length: 40.380m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_slam2.tgz,1.33,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_slam2; Duration: 115.63s, Length: 21.735m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_slam3.tgz,1.6,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Robot SLAM datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/pioneer_slam3; Duration: 111.91s, Length: 18.135m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_far.tgz,0.21,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_notexture_far; Duration: 15.79s, Length: 2.897m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_near_withloop.tgz,0.53,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_notexture_near_withloop; Duration: 37.74s, Length: 11.739m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_far.tgz,0.2,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_texture_far; Duration: 15.53s, Length: 4.343m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_near_withloop.tgz,0.79,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/nostructure_texture_near_withloop; Duration: 56.48s, Length: 13.456m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_far.tgz,0.31,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_notexture_far; Duration: 27.28s, Length: 4.353m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_near.tgz,0.43,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_notexture_near; Duration: 36.44s, Length: 3.872m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_far.tgz,0.55,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_texture_far; Duration: 31.55s, Length: 5.884m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_near.tgz,0.6,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Structure vs. Texture datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/structure_texture_near; Duration: 36.91s, Length: 5.050m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk_with_person.tgz,2.71,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/desk_with_person; Duration: 142.08s, Length: 17.044m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_static.tgz,0.44,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_static; Duration: 23.63s, Length: 0.259m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_xyz.tgz,0.79,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_xyz; Duration: 42.50s, Length: 5.496m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_halfsphere.tgz,0.66,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_halfsphere; Duration: 37.15s, Length: 6.503m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_rpy.tgz,0.49,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/sitting_rpy; Duration: 27.48s, Length: 1.110m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_static.tgz,0.48,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_static; Duration: 24.83s, Length: 0.282m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_xyz.tgz,0.54,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_xyz; Duration: 28.83s, Length: 5.791m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_halfsphere.tgz,0.65,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_halfsphere; Duration: 35.81s, Length: 7.686m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_rpy.tgz,0.54,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Dynamic Objects datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/walking_rpy; Duration: 30.61s, Length: 2.698m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_plant.tgz,0.74,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/plant; Duration: 41.53s, Length: 14.795m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_teddy.tgz,0.93,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/teddy; Duration: 50.82s, Length: 15.709m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_coke.tgz,1.43,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/coke; Duration: 84.55s, Length: 11.681m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_dishes.tgz,1.58,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/dishes; Duration: 100.55s, Length: 15.009m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_flowerbouquet.tgz,1.77,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/flowerbouquet; Duration: 99.40s, Length: 10.758m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_flowerbouquet_brownbackground.tgz,1.25,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/flowerbouquet_brownbackground; Duration: 76.89s, Length: 11.924m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_metallic_sphere.tgz,1.29,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/metallic_sphere; Duration: 75.60s, Length: 11.040m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_metallic_sphere2.tgz,1.03,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/metallic_sphere2; Duration: 62.33s, Length: 11.813m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_cabinet.tgz,0.52,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/cabinet; Duration: 38.58s, Length: 8.111m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_large_cabinet.tgz,0.48,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/large_cabinet; Duration: 33.98s, Length: 11.954m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_teddy.tgz,1.3,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"3D Object Reconstruction datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/teddy; Duration: 80.79s, Length: 19.807m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_xyz_validation.tgz,0.57,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_rpy_validation.tgz,0.56,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk_validation.tgz,0.66,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/desk_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk2_validation.tgz,0.4,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/desk2_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_360_validation.tgz,0.53,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/360_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room_validation.tgz,1.4,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/room_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_plant_validation.tgz,0.8,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/plant_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_xyz_validation.tgz,2.4,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_rpy_validation.tgz,2.3,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_hemisphere_validation.tgz,1.14,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/360_hemisphere_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_360_kidnap_validation.tgz,0.89,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/360_kidnap_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk_validation.tgz,2.26,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/desk_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk_with_person_validation.tgz,2.6,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/desk_with_person_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_pioneer_360_validation.tgz,1.25,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/pioneer_360_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_cabinet_validation.tgz,0.44,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/cabinet_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_large_cabinet_validation.tgz,0.5,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/large_cabinet_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_long_office_household_validation.tgz,1.64,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/long_office_household_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_far_validation.tgz,0.17,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_notexture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_notexture_near_withloop_validation.tgz,0.5,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_notexture_near_withloop_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_far_validation.tgz,0.21,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_texture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_nostructure_texture_near_withloop_validation.tgz,0.88,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/nostructure_texture_near_withloop_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_far_validation.tgz,0.36,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_notexture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_notexture_near_validation.tgz,0.46,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_notexture_near_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_far_validation.tgz,0.46,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_texture_far_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_structure_texture_near_validation.tgz,0.58,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/structure_texture_near_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_static_validation.tgz,0.43,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_static_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_xyz_validation.tgz,0.54,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_halfsphere_validation.tgz,0.59,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_halfsphere_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_sitting_rpy_validation.tgz,0.47,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/sitting_rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_static_validation.tgz,0.51,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_static_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_xyz_validation.tgz,0.57,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_xyz_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_halfsphere_validation.tgz,0.74,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_halfsphere_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_rpy_validation.tgz,0.5,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Validation Files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/walking_rpy_validation; Validation files without public ground truth,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_rgb_calibration.tgz,0.98,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/rgb_calibration; Duration: 50.27s, Length: 0.003m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_ir_calibration.tgz,2.02,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr1/ir_calibration,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_large_checkerboard_calibration.tgz,0.99,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr1/large_checkerboard_calibration; Duration: , Length:",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_rgb_calibration.tgz,1.15,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/rgb_calibration; Duration: 65.77s, Length: 0.017m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_ir_calibration.tgz,2.16,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr2/ir_calibration,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_large_checkerboard_calibration.tgz,1.55,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr2/large_checkerboard_calibration; Duration: 65.02s, Length: 20.052m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_calibration_rgb_depth.tgz,0.64,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/calibration_rgb_depth; Duration: 44.45s, Length: 0.005m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_calibration_ir.tgz,2.39,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.",fr3/calibration_ir,Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
RGB-D SLAM Dataset and Benchmark,https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_checkerboard_large.tgz,0.82,v1,.tgz,Creative Commons 4.0 Attribution License,,,,"Calibration files datasets; we provide a large dataset containing RGB-D data and ground-truth data with the goal to establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems. Our dataset contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640×480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz). Further, we provide the accelerometer data from the Kinect. Finally, we propose an evaluation criterion for measuring the quality of the estimated camera trajectory of visual SLAM systems.","fr3/checkerboard_large; Duration: 53.41s, Length: 19.745m",Electronics,"RGB-D data, ground-truth, SLAM, Microsoft Kinect sensor",Video,P5136,Robotics,,,,source code is licensed under a BSD-2-Clause License
Drinking Water Quality Distribution Monitoring Data,https://datahub.io/JohnSnowLabs/drinking-water-quality-distribution-monitoring-data/r/drinking-water-quality-distribution-monitoring-data_zip.zip,0.003,1/1/2018,zip,proprietary,,,,Drinking water analysis,,Life Sciences,water,text,p3240,,,,,
JunIBIS Data Package,https://datahub.io/andrejjh/junibis_data/r/junibis_data_zip.zip,0.000773,6/15/2018,zip,Creative Commons 4.0 Attribution License,public,,,"The Campaign of Belgium in 1815 marked the end of the Napoleonic wars. This dataset lists the armies and units in presence, their commanders and their hourly positions during the six days of campaign between June 15 and June 20. ",,Government,campaign,text,p3241,,,,,
"Primary completion rate, female",https://datahub.io/world-bank/se.prm.cmpt.fe.zs/r/se_prm_cmpt_fe_zs_zip.zip,0.000208,6/15/2018,zip,CC-BY-4.0,,,,"Primary completion rate, or gross intake ratio to the last grade of primary education, is the number of new entrants (enrollments minus repeaters) in the last grade of primary education, regardless of age, divided by the population at the entrance age for the last grade of primary education. ",,Education,"primary, completion",text,p3242,,,,,
"School enrollment, preprimary",https://datahub.io/world-bank/se.pre.enrr/r/se_pre_enrr_zip.zip,0.000247,6/15/2018,zip,CC-BY-4.0,,,,"Gross enrollment ratio is the ratio of total enrollment, regardless of age, to the population of the age group that officially corresponds to the level of education shown. Preprimary education refers to programs at the initial stage of organized instruction, designed primarily to introduce very young children to a school-type environment and to provide a bridge between home and school.",,Education,preprimary,text,p3243,,,,,
Age dependency ratio,https://datahub.io/world-bank/sp.pop.dpnd/r/sp_pop_dpnd_zip.zip,0.0005,6/15/2018,zip,CC-BY-4.0,,,,Age dependency ratio is the ratio of dependents–people younger than 15 or older than 64–to the working-age population–those ages 15-64. Data are shown as the proportion of dependents per 100 working-age population.,,Government,"research, age",text,p3244,,,,,
Total natural resources rents (% of GDP),https://datahub.io/world-bank/ny.gdp.totl.rt.zs/r/ny_gdp_totl_rt_zs_zip.zip,0.000368,6/15/2018,zip,CC-BY-4.0,,,,"Total natural resources rents are the sum of oil rents, natural gas rents, coal rents (hard and soft), mineral rents, and forest rents.",,Oil and Gas,resources,text,p3245,,,,,
Police Deaths,https://datahub.io/five-thirty-eight/police-deaths/r/police-deaths_zip.zip,0.004,6/25/2018,zip,N/A,,,,This directory contains the data and code behind the story The Dallas Shooting Was Among The Deadliest For Police In U.S. History.,,Government,"death, police",text,p3246,,,,,
NYS Math Test Results By Grade 2006-2011,https://datahub.io/JohnSnowLabs/nys-math-test-results-by-grade-2006-2011/r/datapackage_zip.zip,0.003,1/1/2018,zip,N/A,,,,Math test results by grade. ,,Education,"math, test",text,p3247,,,,,
School Attendance 4PM Report,https://datahub.io/JohnSnowLabs/school-attendance-4pm-report/r/school-attendance-4pm-report_zip.zip,0.01,1/1/2018,zip,proprietary,,,,School attendance report,,Education,attendance,text,p3248,,,,,
YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5914/soft/GDS5914_full.soft.gz,0.0084,10/17/2014,zip,N/A,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,DataSet full SOFT file,P2002,,,,,
YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5914/soft/GDS5914.soft.gz,0.0014,10/17/2014,zip,N/A,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,DataSet SOFT file,P2002,,,,,
YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61989/soft/GSE61989_family.soft.gz,0.0137,10/17/2014,zip,N/A,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,Series family SOFT file,P2002,,,,,
YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE61nnn/GSE61989/miniml/GSE61989_family.xml.tgz,0.0137,10/17/2014,zip,N/A,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,Series family MINiML file,P2002,,,,,
YAP transcriptional regulator depletion effect on endothelial cells,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6244/annot/GPL6244.annot.gz,0.0069,10/17/2014,zip,N/A,public,,,Analysis of umbilical vein endothelial HUVEC cells depleted for the transcriptional regulator YAP. YAP binds to the SH3 domain of the transcriptional activator Yes. Results provide insight into the role of YAP in endothelial cell proliferation.,,Health,Homo Sapiens,Annotation SOFT file,P2002,,,,,
SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5913/soft/GDS5913_full.soft.gz,0.0096,6/17/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,DataSet full SOFT file,P2003,,,,,
SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5913/soft/GDS5913.soft.gz,0.0015,6/17/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,DataSet SOFT file,P2003,,,,,
SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62947/soft/GSE62947_family.soft.gz,0.0134,6/17/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,Series family SOFT file,P2003,,,,,
SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE62nnn/GSE62947/miniml/GSE62947_family.xml.tgz,0.0134,6/17/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,Series family MINiML file,P2003,,,,,
SRPIN803 small molecule inhibitor of SRPK1 effect on retinal pigment epithelial cell line,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPLnnn/GPL570/annot/GPL570.annot.gz,0.0081,6/17/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Homo Sapiens,Annotation SOFT file,P2003,,,,,
Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5881/soft/GDS5881_full.soft.gz,0.0081,6/25/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,DataSet full SOFT file,P2004,,,,,
Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5881/soft/GDS5881.soft.gz,0.0018,6/25/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,DataSet SOFT file,P2004,,,,,
Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70213/soft/GSE70213_family.soft.gz,0.0164,6/25/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,Series family SOFT file,P2004,,,,,
Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70213/miniml/GSE70213_family.xml.tgz,0.0164,6/25/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,Series family MINiML file,P2004,,,,,
Nebulin deficiency effect on the soleus,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.007,6/25/2015,zip,N/A,public,,,"Analysis of retinal pigment epithelial ARPE-19 cells treated with SRPIN803, a small molecule inhibitor of serine-arginine protein kinase 1 SRPK1. SRPK1 inhibition prevents pathological angiogenesis. Results provide insight into the molecular mechanisms underlying the inhibitory activity of SRPIN803.",,Health,Mus Musculus,Annotation SOFT file,P2004,,,,,
Nebulin deficiency effect on the quadriceps,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5880/soft/GDS5880_full.soft.gz,0.0089,6/25/2015,zip,N/A,public,,,Analysis of quadriceps muscles from 6 week old males with a conditional knockout of nebulin in their striated muscles. Nebulin is a giant filamentous protein that is coextensive with actin filaments of the skeletal muscle sarcomere. Results provide insight into the role of nebulin in adult muscles.,,Health,Mus Musculus,DataSet full SOFT file,P2005,,,,,
Nebulin deficiency effect on the quadriceps,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5880/soft/GDS5880.soft.gz,0.0018,6/25/2015,zip,N/A,public,,,Analysis of quadriceps muscles from 6 week old males with a conditional knockout of nebulin in their striated muscles. Nebulin is a giant filamentous protein that is coextensive with actin filaments of the skeletal muscle sarcomere. Results provide insight into the role of nebulin in adult muscles.,,Health,Mus Musculus,DataSet SOFT file,P2005,,,,,
Nebulin deficiency effect on the quadriceps,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70213/soft/GSE70213_family.soft.gz,0.0164,6/25/2015,zip,N/A,public,,,Analysis of quadriceps muscles from 6 week old males with a conditional knockout of nebulin in their striated muscles. Nebulin is a giant filamentous protein that is coextensive with actin filaments of the skeletal muscle sarcomere. Results provide insight into the role of nebulin in adult muscles.,,Health,Mus Musculus,Series family SOFT file,P2005,,,,,
Nebulin deficiency effect on the quadriceps,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70213/miniml/GSE70213_family.xml.tgz,0.0164,6/25/2015,zip,N/A,public,,,Analysis of quadriceps muscles from 6 week old males with a conditional knockout of nebulin in their striated muscles. Nebulin is a giant filamentous protein that is coextensive with actin filaments of the skeletal muscle sarcomere. Results provide insight into the role of nebulin in adult muscles.,,Health,Mus Musculus,Series family MINiML file,P2005,,,,,
Nebulin deficiency effect on the quadriceps,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.007,6/25/2015,zip,N/A,public,,,Analysis of quadriceps muscles from 6 week old males with a conditional knockout of nebulin in their striated muscles. Nebulin is a giant filamentous protein that is coextensive with actin filaments of the skeletal muscle sarcomere. Results provide insight into the role of nebulin in adult muscles.,,Health,Mus Musculus,Annotation SOFT file,P2005,,,,,
Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5879/soft/GDS5879_full.soft.gz,0.0059,8/11/2015,zip,N/A,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,DataSet full SOFT file,P2006,,,,,
Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5879/soft/GDS5879.soft.gz,0.000857,8/11/2015,zip,N/A,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,DataSet SOFT file,P2006,,,,,
Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71868/soft/GSE71868_family.soft.gz,0.0067,8/11/2015,zip,N/A,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,Series family SOFT file,P2006,,,,,
Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE71nnn/GSE71868/miniml/GSE71868_family.xml.tgz,0.0064,8/11/2015,zip,N/A,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,Series family MINiML file,P2006,,,,,
Pulmonary CDC11c+ cells from young and middle-age animals,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6885/annot/GPL6885.annot.gz,0.0047,8/11/2015,zip,N/A,public,,,Analysis of pulmonary CDC11c+ cells from 6-8 week and 10-13 month old C57BL/6 animals. CDC11c+ cells are key modulators of the immune response in the lung. Results provide insight into molecular mechanisms underlying the decline in immune function associated with aging.,,Health,Mus Musculus,Annotation SOFT file,P2006,,,,,
	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5826/soft/GDS5826_full.soft.gz,0.0113,7/9/2015,zip,N/A,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,DataSet full SOFT file,P2007,,,,,
	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5826/soft/GDS5826.soft.gz,0.0029,7/9/2015,zip,N/A,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,DataSet SOFT file,P2007,,,,,
	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE69nnn/GSE69078/soft/GSE69078_family.soft.gz,0.016,7/9/2015,zip,N/A,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,Series family SOFT file,P2007,,,,,
	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE69nnn/GSE69078/miniml/GSE69078_family.xml.tgz,0.016,7/9/2015,zip,N/A,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,Series family MINiML file,P2007,,,,,
	Multiple myeloma cell lines with acquired resistance to chemotherapeutic agent carfilzomib,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPLnnn/GPL570/annot/GPL570.annot.gz,0.0081,7/9/2015,zip,N/A,public,,,"Analysis of proteasome inhibitor carfilzomib-resistant multiple myeloma (MM) cell lines KMS-11/Cfz and KMS-34/Cfz, after 1 week of growth in the absence of carfilzomib. Results provide insight into the molecular mechanisms underlying the acquisition of proteasome inhibitor resistance in MM.",,Health,Homo Sapiens,Annotation SOFT file,P2007,,,,,
Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5825/soft/GDS5825_full.soft.gz,0.0087,6/27/2015,zip,N/A,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,DataSet full SOFT file,P2008,,,,,
Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5825/soft/GDS5825.soft.gz,0.0016,6/27/2015,zip,N/A,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,DataSet SOFT file,P2008,,,,,
Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70302/soft/GSE70302_family.soft.gz,0.0133,6/27/2015,zip,N/A,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,Series family SOFT file,P2008,,,,,
Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70302/miniml/GSE70302_family.xml.tgz,0.0133,6/27/2015,zip,N/A,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,Series family MINiML file,P2008,,,,,
Interleukin-1α deficiency effect on injured spinal cord,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.007,6/27/2015,zip,N/A,public,,,Analysis of spinal cord segment from interleukin (IL)-1α -/- and IL-1β -/- C57BL/6 adults 24 hrs after spinal cord injury (SCI). IL-1α and IL-1β belong to a family of cytokines playing a key role in neurodegeneration. Results provide insight into the role of IL-1α in the neuropathology of SCI.,,Health,Mus Musculus,Annotation SOFT file,P2008,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5824/soft/GDS5824_full.soft.gz,0.0084,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,DataSet full SOFT file,P2009,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5824/soft/GDS5824.soft.gz,0.0025,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,DataSet SOFT file,P2009,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65557/soft/GSE65557_family.soft.gz,0.0156,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Series family SOFT file,P2009,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65557/miniml/GSE65557_family.xml.tgz,0.0156,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Series family MINiML file,P2009,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.007,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Annotation SOFT file,P2009,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.007,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,DataSet full SOFT file,P2010,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5824/soft/GDS5824.soft.gz,0.0025,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,DataSet SOFT file,P2010,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65557/soft/GSE65557_family.soft.gz,0.015,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Series family SOFT file,P2010,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65557/miniml/GSE65557_family.xml.tgz,0.0159,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Series family MINiML file,P2010,,,,,
Short-term high fat diet effect on epididymal adipose fractions: time course,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6246/annot/GPL6246.annot.gz,0.007,2/4/2015,zip,N/A,public,,,Analysis of primary adipocytes from epididymis of C57BL/6J males fed a 60% high fat diet (HFD) for up to 7 days to induce obesity. Results provide insight into the molecular mechanisms underlying insulin resistance in adipose tissues during early obesity.,,Health,Mus Musculus,Annotation SOFT file,P2010,,,,,
	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5822/soft/GDS5822_full.soft.gz,0.007,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,DataSet full SOFT file,P2011,,,,,
	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5822/soft/GDS5822.soft.gz,0.0012,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,DataSet SOFT file,P2011,,,,,
	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/soft/GSE52812_family.soft.gz,0.0163,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,Series family SOFT file,P2011,,,,,
	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/miniml/GSE52812_family.xml.tgz,0.0163,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,Series family MINiML file,P2011,,,,,
	Anti-CTLA-4 immunotherapy effect on αSMA+ myofibroblast late-depleted pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6887/annot/GPL6887.annot.gz,0.0066,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to deplete αSMA+ myofibroblasts and treated with anti-CTLA4. Results provide insight into molecular basis of anti-CTLA4 therapy in PDAC.,,Health,Mus Musculus,Annotation SOFT file,P2011,,,,,
αSMA+ myofibroblast late depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5821/soft/GDS5821_full.soft.gz,0.0074,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in late-stage PDAC.,,Health,Mus Musculus,DataSet full SOFT file,P2012,,,,,
αSMA+ myofibroblast late depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5821/soft/GDS5821.soft.gz,0.000924,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in late-stage PDAC.,,Health,Mus Musculus,DataSet SOFT file,P2012,,,,,
αSMA+ myofibroblast late depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/soft/GSE52812_family.soft.gz,0.0163,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in late-stage PDAC.,,Health,Mus Musculus,Series family SOFT file,P2012,,,,,
αSMA+ myofibroblast late depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/miniml/GSE52812_family.xml.tgz,0.0163,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in late-stage PDAC.,,Health,Mus Musculus,Series family MINiML file,P2012,,,,,
αSMA+ myofibroblast late depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6887/annot/GPL6887.annot.gz,0.0163,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 6 weeks of age for 10 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in late-stage PDAC.,,Health,Mus Musculus,Annotation SOFT file,P2012,,,,,
αSMA+ myofibroblast early depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5820/soft/GDS5820_full.soft.gz,0.0074,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 4 to 4.5 weeks of age for 14 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in early-stage PDAC.,,Health,Mus Musculus,DataSet full SOFT file,P2013,,,,,
αSMA+ myofibroblast early depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5820/soft/GDS5820.soft.gz,0.00093,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 4 to 4.5 weeks of age for 14 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in early-stage PDAC.,,Health,Mus Musculus,DataSet SOFT file,P2013,,,,,
αSMA+ myofibroblast early depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/soft/GSE52812_family.soft.gz,0.0164,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 4 to 4.5 weeks of age for 14 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in early-stage PDAC.,,Health,Mus Musculus,Series family SOFT file,P2013,,,,,
αSMA+ myofibroblast early depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52812/miniml/GSE52812_family.xml.tgz,0.0164,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 4 to 4.5 weeks of age for 14 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in early-stage PDAC.,,Health,Mus Musculus,Series family MINiML file,P2013,,,,,
αSMA+ myofibroblast early depletion effect on pancreatic ductal adenocarcinoma model,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL6nnn/GPL6887/annot/GPL6887.annot.gz,0.0064,1/20/2015,zip,N/A,public,,,Analysis of pancreatic ductal adenocarcinoma (PDAC) tumors from PKT; αSMA-tk+ transgenics injected daily with ganciclovir at 4 to 4.5 weeks of age for 14 days or less to selectively deplete αSMA+ myofibroblasts. Results provide insight into the role of αSMA+ myofibroblasts in early-stage PDAC.,,Health,Mus Musculus,Annotation SOFT file,P2013,,,,,
Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5819/soft/GDS5819_full.soft.gz,0.0086,2/3/2015,zip,N/A,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,DataSet full SOFT file,P2014,,,,,
Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS5nnn/GDS5819/soft/GDS5819.soft.gz,0.0015,2/3/2015,zip,N/A,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,DataSet SOFT file,P2014,,,,,
Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65517/soft/GSE65517_family.soft.gz,0.0176,2/3/2015,zip,N/A,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,Series family SOFT file,P2014,,,,,
Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE65nnn/GSE65517/miniml/GSE65517_family.xml.tgz,0.0179,2/3/2015,zip,N/A,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,Series family MINiML file,P2014,,,,,
Metastatic breast cancer and sepsis: monocytes,ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/GPL10nnn/GPL10558/annot/GPL10558.annot.gz,0.007,2/3/2015,zip,N/A,public,,,"Analysis of the total population of monocytes from patients with metastatic breast cancer (MBC), sepsis, or tuberculosis. Results provide insight into molecular similarities between monocytes from MBC patients and reprogrammed immunosuppressive monocytes from sepsis patients.",,Health,Homo Sapiens,Annotation SOFT file,P2014,,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T1/T1_rectified.tar.gz,0.809,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T1; 2:40 minutes @ 540 fps; PNG: Pinhole Rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T1/T1_orig.tar.gz,1.4,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T1; 2:40 minutes @ 540 fps; PNG: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T1/T1_rectified.bag,1.7,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T1; 2:40 minutes @ 540 fps; RosBag: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T1/T1_orig.bag,2.5,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T1; 2:40 minutes @ 540 fps; RosBag: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T2/T2_rectified.tar.gz,1,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T2; 1:19 minutes @ 50fps; PNG: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T2/T2_orig.tar.gz,0.711,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T2; 1:19 minutes @ 50fps; PNG: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T2/T2_rectified.bag,0.878,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T2; 1:19 minutes @ 50fps; RosBag: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T2/T2_orig.bag,1.2,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T2; 1:19 minutes @ 50fps; Rosbag: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T3/T3_rectified.tar.gz,0.69,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T3; 2:17 minutes @ 50fps; PNG: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T3/T3_orig.tar.gz,1.2,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T3; 2:17 minutes @ 50fps; PNG: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T3/T3_rectified.bag,1.5,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T3; 2:17 minutes @ 50fps; RosBag: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T3/T3_orig.bag,2.1,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T3; 2:17 minutes @ 50fps; Rosbag: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T4/T4_rectified.tar.gz,0.69,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T4; 2:08 minutes @ 50fps; PNG: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T4/T4_orig.tar.gz,1.1,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T4; 2:08 minutes @ 50fps; PNG: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T4/T4_rectified.bag,1.4,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T4; 2:08 minutes @ 50fps; RosBag: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T4/T4_orig.bag,2,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T4; 2:08 minutes @ 50fps; Rosbag: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T5/T5_rectified.tar.gz,0.794,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T5; 2:45 minutes @ 50fps; PNG: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T5/T5_orig.tar.gz,1.4,v1,.tar.gz,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T5; 2:45 minutes @ 50fps; PNG: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T5/T5_rectified.bag,1.8,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T5; 2:45 minutes @ 50fps; RosBag: Pinhole rectified,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Large-Scale Direct SLAM for Omnidirectional Cameras,http://vmcremers8.informatik.tu-muenchen.de/lsd/OmniDataset/T5/T5_orig.bag,2.5,v1,.bag,"Unless stated otherwise, all data in the SLAM for Omnidirectional Cameras Dataset is licensed under a Creative Commons 4.0 Attribution License (CC BY 4.0).",,,,"We propose a real-time, direct monocular SLAM method for omnidirectional or wide field-of-view fisheye cam- eras. Both tracking (direct image alignment) and mapping (pixel-wise distance filtering) are directly formulated for the unified omnidirectional model, which can model central imaging devices with a field of view well above 150° . This is in stark contrast to existing direct mono-SLAM approaches like DTAM or LSD-SLAM, which operate on rectified images, limiting the field of view to well below 180° . Not only does this allow to observe – and reconstruct – a larger portion of the surrounding environment, but it also makes the system more robust to degenerate (rotation-only) movement. The two main contribution are (1) the formulation of direct image alignment for the unified omnidirectional model, and (2) a fast yet accurate approach to incremental stereo directly on distorted images. We evaluated our framework on real-world sequences taken with a 185◦ fish-eye lens, and compare it to a rectified and a piecewise rectified approach.",T5; 2:45 minutes @ 50fps; Rosbag: Omnidirectional,Electronics,"SLAM, omnidirectional, wide field, fish-eye",Video,P5137,Robotics,,,,
Unlabeled images Coco,images.cocodataset.org/zips/unlabeled2017.zip,17.4,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,unlabeled images for object detection,,AI,"object detection, unlabeled images ",Image,P2015,,,,,
Unlabeled images Coco,images.cocodataset.org/annotations/image_info_unlabeled2017.zip,0.004,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,unlabeled images information,,AI,"object detection, unlabeled images ",Image,P2015,,,,,
Test images Coco ,images.cocodataset.org/zips/test2017.zip,6.2,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,test images for object detection,,AI,"object detection, unlabeled images ",Image,P2016,,,,,
Test images Coco ,images.cocodataset.org/annotations/image_info_test2017.zip,0.0011,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,test images information,,AI,"object detection, unlabeled images ",Image,P2016,,,,,
Val images ,images.cocodataset.org/zips/val2017.zip,0.778,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,val images for object detection  ,,AI,"object detection, unlabeled images ",Image,P2017,,,,,
Panoptic Train/Val Annotations,images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip,0.821,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,panoptic train/val annotations ,,AI,"object detection, unlabeled images ",Image,P2017,,,,,
Train images ,images.cocodataset.org/zips/train2017.zip,18,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2017,,,,,
Stuff train/val annotations ,images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip,1.1,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,stuff train/val annotations ,,AI,"object detection, unlabeled images ",Image,P2017,,,,,
Train/val annotations ,images.cocodataset.org/annotations/annotations_trainval2017.zip,0.241,01/01/2017,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2017,,,,,
Test images Coco ,images.cocodataset.org/zips/test2015.zip,12,01/01/2015,.zip,Creative Commons Attribution 4.0 License,,,,test images for object detection,,AI,"object detection, unlabeled images ",Image,P2018,,,,,
Testing image info,images.cocodataset.org/annotations/image_info_test2015.zip,0.002,01/01/2015,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2018,,,,,
Test images Coco ,images.cocodataset.org/annotations/image_info_test2015.zip,6,1/1/2014,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2019,,,,,
,,,1/1/2014,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2019,,,,,
,,,1/1/2014,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2019,,,,,
,,,1/1/2014,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2019,,,,,
,,,1/1/2014,.zip,Creative Commons Attribution 4.0 License,,,,train images for object detection,,AI,"object detection, unlabeled images ",Image,P2019,,,,,
Net ODA received per capita (current US$),https://datahub.io/world-bank/dt.oda.odat.pc.zs/r/dt_oda_odat_pc_zs_zip.zip,0.000402,6/20/2018,zip,CC-BY-4.0,,,,"Net official development assistance (ODA) per capita consists of disbursements of loans made on concessional terms (net of repayments of principal) and grants by official agencies of the members of the Development Assistance Committee (DAC), by multilateral institutions, and by non-DAC countries to promote economic development and welfare in countries and territories in the DAC list of ODA recipients; and is calculated by dividing net ODA received by the midyear population estimate.",,Government,Loans,text,p3149,,,,,
GDP growth (annual %),https://datahub.io/world-bank/ny.gdp.mktp.kd.zg/r/ny_gdp_mktp_kd_zg_zip.zip,0.000418,6/20/2018,zip,CC-BY-4.0,,,,Annual percentage growth rate of GDP at market prices based on constant local currency. Aggregates are based on constant 2010 U.S. dollars. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products.,,Government,GDP,text,p3150,,,,,
Inconvenient Sequel,https://datahub.io/five-thirty-eight/inconvenient-sequel/r/inconvenient-sequel_zip.zip,0.002,6/20/2018,zip,N/A,,,,This folder contains data behind the story Al Gore’s New Movie Exposes The Big Flaw In Online Movie Ratings.,,Media and Entertainment,"movie, rating",text,p3151,,,,,
MNST Training Images,http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz,0.0099,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples.",,AI,digits,image,p3152,,,,,
MNST Training Labels,http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz,0.000028,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples.",,AI,digits,text,p3152,,,,,
MNST Test Images,http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz,0.00165,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a test set of 10,000 examples.",,AI,digits,image,p3152,,,,,
MNST Test Labels,http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz,0.0000045,,gzip,,,,,"The MNIST database of handwritten digits, available from this page, has a test set of 10,000 examples.",,AI,digits,text,p3152,,,,,
CIFAR-100 ,https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz,0.161,,gzip,,,,,"This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs).",,AI,"photo, label",image,p3153,Computer Vision,,,,
Flickr Audio Corpus,https://groups.csail.mit.edu/sls/downloads/flickraudio/downloads/flickr_audio.tar.gz,4.2,,gzip,,,,,"The Flickr 8k Audio Caption Corpus contains 40,000 spoken captions of 8,000 natural images. It was collected in 2015 to investigate multimodal learning schemes for unsupervised speech pattern discovery.",,AI,audio caption,audio,p3154,,,,,
Golos,http://webstructor.net/data/golos_texts_all.zip,14.34,,zip,,,,,Webstructor text data,,AI,large file,text,p3155,,,,,
Steemit,http://webstructor.net/data/steemit_texts_all.zip,2.63,,zip,,,,,Webstructor text data,,AI,text data ,text,p3156,,,,,
,,,,,,,,,,,,,,,,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam1_1024_16.bag,1.7,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam1_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam2_1024_16.bag,1.1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam2_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam3_1024_16.bag,1.3,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam3_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam4_1024_16.bag,1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam4_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam5_1024_16.bag,1.5,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam5_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam6_1024_16.bag,1.4,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam6_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam7_1024_16.bag,0.603,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam7_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam8_1024_16.bag,0.812,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-cam8_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu1_1024_16.bag,4.1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu1_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu2_1024_16.bag,4.1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu2_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu3_1024_16.bag,3.8,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu3_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu4_1024_16.bag,3.4,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-imu4_1024_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-vignette2_1024_16.bag,8.7,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-vignette2_1024_16.bag.md5,7.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-vignette3_1024_16.bag,6.7,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-calib-vignette3_1024_16.bag.md5,7.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor1_1024_16.bag,23,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor1_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor2_1024_16.bag,26,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor2_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor3_1024_16.bag,23,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor3_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor4_1024_16.bag,7.5,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor4_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor5_1024_16.bag,23,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-corridor5_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale1_1024_16.bag,60,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale1_1024_16.bag.md5,7.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale2_1024_16.bag,42,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale2_1024_16.bag.md5,6.60E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale3_1024_16.bag,38,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale3_1024_16.bag.md5,6.60E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale4_1024_16.bag,49,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale4_1024_16.bag.md5,6.60E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale5_1024_16.bag,35,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale5_1024_16.bag.md5,6.60E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale6_1024_16.bag,45,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-magistrale6_1024_16.bag.md5,6.60E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors1_1024_16.bag,100,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors1_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors2_1024_16.bag,72,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors2_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors3_1024_16.bag,66,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors3_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors4_1024_16.bag,55,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors4_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors5_1024_16.bag,69,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors5_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors6_1024_16.bag,115,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors6_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors7_1024_16.bag,88,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors7_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors8_1024_16.bag,63,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-outdoors8_1024_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room1_1024_16.bag,11,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room1_1024_16.bag.md5,6.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room2_1024_16.bag,11,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room2_1024_16.bag.md5,6.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room3_1024_16.bag,11,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room3_1024_16.bag.md5,6.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room4_1024_16.bag,8.7,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room4_1024_16.bag.md5,6.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room5_1024_16.bag,11,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room5_1024_16.bag.md5,6.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room6_1024_16.bag,10,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-room6_1024_16.bag.md5,6.00E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-slides1_1024_16.bag,22,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-slides1_1024_16.bag.md5,6.20E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-slides2_1024_16.bag,19,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-slides2_1024_16.bag.md5,6.20E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-slides3_1024_16.bag,28,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/1024_16/dataset-slides3_1024_16.bag.md5,6.20E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 1024×1024:,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam1_512_16.bag,0.446,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam1_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam2_512_16.bag,0.287,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam2_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam3_512_16.bag,0.344,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam3_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam4_512_16.bag,0.269,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam4_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam5_512_16.bag,0.384,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam5_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam6_512_16.bag,0.362,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam6_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam7_512_16.bag,0.153,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam7_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam8_512_16.bag,0.206,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-cam8_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu1_512_16.bag,1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu1_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu2_512_16.bag,1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu2_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu3_512_16.bag,1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu3_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu4_512_16.bag,883,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-imu4_512_16.bag.md5,6.40E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-vignette2_512_16.bag,2.2,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-vignette2_512_16.bag.md5,6.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-vignette3_512_16.bag,1.7,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-calib-vignette3_512_16.bag.md5,6.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor1_512_16.bag,5.9,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor1_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor2_512_16.bag,6.6,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor2_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor3_512_16.bag,5.7,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor3_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor4_512_16.bag,1.9,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor4_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor5_512_16.bag,5.8,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-corridor5_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale1_512_16.bag,15,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale1_512_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale2_512_16.bag,11,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale2_512_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale3_512_16.bag,9.5,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale3_512_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale4_512_16.bag,12,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale4_512_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale5_512_16.bag,8.7,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale5_512_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale6_512_16.bag,11,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-magistrale6_512_16.bag.md5,6.50E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors1_512_16.bag,25,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors1_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors2_512_16.bag,18,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors2_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors3_512_16.bag,17,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors3_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors4_512_16.bag,14,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors4_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors5_512_16.bag,17,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors5_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors6_512_16.bag,29,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors6_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors7_512_16.bag,22,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors7_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors8_512_16.bag,16,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-outdoors8_512_16.bag.md5,6.30E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room1_512_16.bag,2.8,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room1_512_16.bag.md5,5.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room2_512_16.bag,2.8,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room2_512_16.bag.md5,5.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room3_512_16.bag,2.8,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room3_512_16.bag.md5,5.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room4_512_16.bag,2.2,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room4_512_16.bag.md5,5.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room5_512_16.bag,2.8,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room5_512_16.bag.md5,5.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room6_512_16.bag,2.6,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-room6_512_16.bag.md5,5.90E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-slides1_512_16.bag,5.5,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-slides1_512_16.bag.md5,6.10E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-slides2_512_16.bag,4.8,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-slides2_512_16.bag.md5,6.10E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-slides3_512_16.bag,7.1,v1,.bag,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/calibrated/512_16/dataset-slides3_512_16.bag.md5,6.10E-08,v1,.bag.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Bag 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam1_1024_16.tar,0.854,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam1_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam2_1024_16.tar,0.553,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam2_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam3_1024_16.tar,0.658,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam3_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam4_1024_16.tar,0.508,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam4_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam5_1024_16.tar,0.737,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam5_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam6_1024_16.tar,0.681,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam6_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam7_1024_16.tar,0.29,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam7_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam8_1024_16.tar,0.393,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-cam8_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu1_1024_16.tar,1.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu1_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu2_1024_16.tar,1.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu2_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu3_1024_16.tar,1.6,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu3_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu4_1024_16.tar,1.4,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-imu4_1024_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-vignette2_1024_16.tar,3.9,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-vignette2_1024_16.tar.md5,7.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-vignette3_1024_16.tar,2.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-calib-vignette3_1024_16.tar.md5,7.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor1_1024_16.tar,9.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor1_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor2_1024_16.tar,11,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor2_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor3_1024_16.tar,9.4,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor3_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor4_1024_16.tar,3.1,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor4_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor5_1024_16.tar,9.6,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-corridor5_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale1_1024_16.tar,28,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale1_1024_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale2_1024_16.tar,20,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale2_1024_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale3_1024_16.tar,18,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale3_1024_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale4_1024_16.tar,21,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale4_1024_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale5_1024_16.tar,16,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale5_1024_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale6_1024_16.tar,19,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-magistrale6_1024_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors1_1024_16.tar,49,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors1_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors2_1024_16.tar,33,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors2_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors3_1024_16.tar,29,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors3_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors4_1024_16.tar,25,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors4_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors5_1024_16.tar,30,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors5_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors6_1024_16.tar,57,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors6_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors7_1024_16.tar,39,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors7_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors8_1024_16.tar,28,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-outdoors8_1024_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room1_1024_16.tar,4.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room1_1024_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room2_1024_16.tar,4.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room2_1024_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room3_1024_16.tar,4.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room3_1024_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room4_1024_16.tar,3.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room4_1024_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room5_1024_16.tar,4.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room5_1024_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room6_1024_16.tar,4.4,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-room6_1024_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides1_1024_16.tar,9.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides1_1024_16.tar.md5,6.20E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides2_1024_16.tar,8.5,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides2_1024_16.tar.md5,6.20E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides3_1024_16.tar,12,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/1024_16/dataset-slides3_1024_16.tar.md5,6.20E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 1024x1024,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam1_512_16.tar,0.854,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x512,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam1_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x513,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam2_512_16.tar,0.553,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x514,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam2_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x515,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam3_512_16.tar,0.658,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x516,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam3_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x517,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam4_512_16.tar,0.508,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x518,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam4_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x519,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam5_512_16.tar,0.737,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x520,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam5_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x521,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam6_512_16.tar,0.681,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x522,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam6_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x523,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam7_512_16.tar,0.29,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x524,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam7_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x525,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam8_512_16.tar,0.393,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x526,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-cam8_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x527,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu1_512_16.tar,1.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x528,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu1_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x529,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu2_512_16.tar,1.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x530,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu2_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x531,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu3_512_16.tar,1.6,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x532,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu3_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x533,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu4_512_16.tar,1.4,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x534,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-imu4_512_16.tar.md5,6.50E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x535,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-vignette2_512_16.tar,3.9,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x536,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-vignette2_512_16.tar.md5,7.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x537,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-vignette3_512_16.tar,2.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x538,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-calib-vignette3_512_16.tar.md5,7.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x539,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor1_512_16.tar,9.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x540,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor1_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x541,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor2_512_16.tar,11,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x542,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor2_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x543,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor3_512_16.tar,9.4,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x544,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor3_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x545,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor4_512_16.tar,3.1,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x546,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor4_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x547,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor5_512_16.tar,9.6,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x548,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-corridor5_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x549,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale1_512_16.tar,28,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x550,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale1_512_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x551,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale2_512_16.tar,20,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x552,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale2_512_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x553,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale3_512_16.tar,18,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x554,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale3_512_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x555,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale4_512_16.tar,21,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x556,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale4_512_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x557,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale5_512_16.tar,16,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x558,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale5_512_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x559,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale6_512_16.tar,19,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x560,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-magistrale6_512_16.tar.md5,6.60E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x561,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors1_512_16.tar,49,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x562,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors1_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x563,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors2_512_16.tar,33,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x564,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors2_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x565,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors3_512_16.tar,29,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x566,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors3_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x567,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors4_512_16.tar,25,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x568,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors4_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x569,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors5_512_16.tar,30,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x570,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors5_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x571,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors6_512_16.tar,57,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x572,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors6_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x573,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors7_512_16.tar,39,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x574,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors7_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x575,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors8_512_16.tar,28,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x576,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-outdoors8_512_16.tar.md5,6.40E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x577,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room1_512_16.tar,4.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x578,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room1_512_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x579,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room2_512_16.tar,4.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x580,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room2_512_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x581,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room3_512_16.tar,4.8,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x582,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room3_512_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x583,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room4_512_16.tar,3.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x584,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room4_512_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x585,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room5_512_16.tar,4.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x586,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room5_512_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x587,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room6_512_16.tar,4.4,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x588,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-room6_512_16.tar.md5,6.00E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x589,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides1_512_16.tar,9.7,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x590,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides1_512_16.tar.md5,6.20E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x591,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides2_512_16.tar,8.5,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x592,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides2_512_16.tar.md5,6.20E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x593,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides3_512_16.tar,12,v1,.tar,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x594,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
Visual-Inertial Dataset,https://vision.in.tum.de/tumvi/exported/euroc/512_16/dataset-slides3_512_16.tar.md5,6.20E-08,v1,.tar.md5,Public,,,,"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data will be made publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",Euroc/DSO 512x595,Electronics,"SLAM, visual odometry, visual-interial, VI, TUM VI",Video,P5138,Robotics,,,,
,,,,,,,,,,,,,,,,,,,
,,7641.065252,,,,,,,,,,,,,,,,,