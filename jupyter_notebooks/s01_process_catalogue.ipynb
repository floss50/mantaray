{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import osmosis_aws_driver.data_S3_plugin as ocean_s3\n",
    "# General imports\n",
    "import sys\n",
    "import os\n",
    "#import glob\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# %% Logging\n",
    "import logging\n",
    "\n",
    "loggers_dict = logging.Logger.manager.loggerDict\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "\n",
    "# Set level\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# FORMAT = \"%(asctime)s - %(levelno)s - %(module)-15s - %(funcName)-15s - %(message)s\"\n",
    "FORMAT = \"%(asctime)s L%(levelno)s: %(module)-15s %(message)s\"\n",
    "\n",
    "DATE_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "\n",
    "# Create handler and assign\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "logger.critical(\"Logging started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The working directory is the repo root\n",
    "logging.debug(\"Current working directory: {}\".format(os.getcwd()))\n",
    "\n",
    "# The source catalog\n",
    "FNAME_SOURCE_CATALOG = \"Original/OceanDataSets_master catalog clean.csv\"\n",
    "# The current catalog stores the updated state\n",
    "FNAME_CURRENT_CATALOG = r\"Master catalog current.csv\"\n",
    "PATH_SOURCE_CATALOGUE = os.path.join(os.getcwd(),'catalog', FNAME_SOURCE_CATALOG)\n",
    "PATH_CURRENT_CATALOGUE = os.path.join(os.getcwd(),'catalog', FNAME_CURRENT_CATALOG)\n",
    "assert os.path.exists(PATH_SOURCE_CATALOGUE), \"{}\".format(PATH_SOURCE_CATALOGUE)\n",
    "assert os.path.exists(PATH_CURRENT_CATALOGUE), \"{}\".format(PATH_CURRENT_CATALOGUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_CURRENT_CATALOGUE)\n",
    "\n",
    "total_GB = sum(df.loc[:,'SizeGB'])\n",
    "logging.debug(\"Loaded data catalogue with {} records representing {:0.0f} GB\".format(len(df),total_GB))\n",
    "logging.debug(\"{} files have been flagged as already uploaded to S3.\".format(sum(df['uploaded'])))\n",
    "errors = df[df['error'] != 'No error']['error'].value_counts()\n",
    "logging.debug(\"{} files have been flagged with an upload error.\".format(sum(errors)))\n",
    "\n",
    "print(\"Error summary:\")\n",
    "for err in errors.iteritems():\n",
    "    print('\\t',*err)\n",
    "\n",
    "res = df.head()\n",
    "df = df[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the connection via the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The `osmosis-aws-driver`, imported here as `ocean_s3` is a wrapper for Boto3.\n",
    "\n",
    "# config = dict()\n",
    "# config['region'] = 'eu-central-1'\n",
    "config = None\n",
    "ocn_s3 = ocean_s3.S3_Plugin(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,b in enumerate(ocn_s3.list_buckets()):\n",
    "    print(i,b['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bucketname =\"data-catalogue-r00\"\n",
    "#bucket = ocn_s3.s3_client.head_bucket(Bucket=bucketname)\n",
    "bucket = ocn_s3.s3_resource.Bucket(bucketname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3files = {obj.key:obj for obj in  bucket.objects.all()}\n",
    "\n",
    "# Select a subset of files\n",
    "these_keys = list(s3files.keys())[:2]\n",
    "for f in these_keys:\n",
    "    meta_data = s3files[f].Object().metadata\n",
    "    print(f, meta_data)\n",
    "\n",
    "total_GB=sum([s3files[f].size for f in s3files])/1000/1000/1000\n",
    "\n",
    "logging.debug(\"{} files on {}, {:0.2f} GB\".format(len(s3files),bucketname,total_GB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in df.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['uploaded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the dataset onto blockchain"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
