{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After confirming the connection to S3, use this script to manage datasets.\n",
    "\n",
    "A data catalogue .csv is maintained in the /catalog directory.\n",
    "\n",
    "The catalog is loaded in a Pandas Dataframe object.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import osmosis_aws_driver.data_S3_plugin as ocean_s3\n",
    "# General imports\n",
    "import sys\n",
    "import os\n",
    "#import glob\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "#%% Constants\n",
    "S3_BUCKET_NAME = \"\"\n",
    "\n",
    "#%% Logging\n",
    "import logging\n",
    "\n",
    "loggers_dict = logging.Logger.manager.loggerDict\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "\n",
    "# Set level\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# FORMAT = \"%(asctime)s - %(levelno)s - %(module)-15s - %(funcName)-15s - %(message)s\"\n",
    "FORMAT = \"%(asctime)s L%(levelno)s: %(module)-15s %(message)s\"\n",
    "\n",
    "DATE_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "\n",
    "# Create handler and assign\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "logger.critical(\"Logging started\")\n",
    "\n",
    "\n",
    "#%%\n",
    "# The working directory is the repo root\n",
    "logging.debug(\"Current working directory: {}\".format(os.getcwd()))\n",
    "\n",
    "# The source catalog\n",
    "FNAME_SOURCE_CATALOG = \"Original/OceanDataSets_master catalog clean.csv\"\n",
    "# The current catalog stores the updated state\n",
    "FNAME_CURRENT_CATALOG = r\"Master catalog current.csv\"\n",
    "PATH_SOURCE_CATALOGUE = os.path.join(os.getcwd(),'catalog', FNAME_SOURCE_CATALOG)\n",
    "PATH_CURRENT_CATALOGUE = os.path.join(os.getcwd(),'catalog', FNAME_CURRENT_CATALOG)\n",
    "assert os.path.exists(PATH_SOURCE_CATALOGUE), \"{}\".format(PATH_SOURCE_CATALOGUE)\n",
    "assert os.path.exists(PATH_CURRENT_CATALOGUE), \"{}\".format(PATH_CURRENT_CATALOGUE)\n",
    "\n",
    "#%% Load the data catalogue\n",
    "df = pd.read_csv(PATH_CURRENT_CATALOGUE)\n",
    "\n",
    "total_GB = sum(df.loc[:,'SizeGB'])\n",
    "logging.debug(\"Loaded data catalogue with {} records representing {:0.0f} GB\".format(len(df),total_GB))\n",
    "logging.debug(\"{} files have been flagged as already uploaded to S3.\".format(sum(df['uploaded'])))\n",
    "errors = df[df['error'] != 'No error']['error'].value_counts()\n",
    "logging.debug(\"{} files have been flagged with an upload error.\".format(sum(errors)))\n",
    "\n",
    "print(\"Error summary:\")\n",
    "for err in errors.iteritems():\n",
    "    print('\\t',*err)\n",
    "\n",
    "res = df.head()\n",
    "df = df[0:5]\n",
    "\n",
    "\n",
    "#%% ## Create the connection via the wrapper\n",
    "\n",
    "# The `osmosis-aws-driver`, imported here as `ocean_s3` is a wrapper for Boto3.\n",
    "# config = dict()\n",
    "# config['region'] = 'eu-central-1'\n",
    "\n",
    "config = None # No configuration needed\n",
    "ocn_s3 = ocean_s3.S3_Plugin(config)\n",
    "\n",
    "#%% List buckets\n",
    "for i,b in enumerate(ocn_s3.list_buckets()):\n",
    "    print(i,b['Name'])\n",
    "\n",
    "#%% Delete a bucket (WARNING!)\n",
    "# bucketname=\"ocean-test-osmosis-data-plugin-1537444458\"\n",
    "# ocn_s3.delete_bucket(bucketname)\n",
    "\n",
    "#%% Get the bucket\n",
    "bucket = ocn_s3.s3_resource.Bucket(S3_BUCKET_NAME)\n",
    "\n",
    "#%% Get the files\n",
    "s3files = {obj.key:obj for obj in  bucket.objects.all()}\n",
    "total_GB=sum([s3files[f].size for f in s3files])/1000/1000/1000\n",
    "logging.debug(\"{} files on {}, {:0.2f} GB\".format(len(s3files),bucketname,total_GB))\n",
    "\n",
    "#%%\n",
    "# Select a subset of files\n",
    "these_keys = list(s3files.keys())[:2]\n",
    "for f in these_keys:\n",
    "    meta_data = s3files[f].Object().metadata\n",
    "    print(f, meta_data)\n",
    "#%%\n",
    "for row in df.iterrows():\n",
    "    print(row)\n",
    "\n",
    "#%%\n",
    "df['uploaded']\n",
    "\n",
    "#%% Register the dataset onto blockchain"
   ]
  }
 ],
 "metadata": {
  "jupytext_formats": "py:percent",
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
